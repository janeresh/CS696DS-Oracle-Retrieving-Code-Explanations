{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-31 14:42:51,245\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 14:42:51 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-31 14:42:52 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-31 14:43:01 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-31 14:43:01 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f', speculative_config=None, tokenizer='/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-31 14:43:02 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 03-31 14:43:03 model_runner.py:1110] Starting to load model /datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.39s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.57it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 14:43:05 model_runner.py:1115] Loading model weights took 4.7199 GB\n",
      "INFO 03-31 14:43:06 worker.py:267] Memory profiling takes 0.51 seconds\n",
      "INFO 03-31 14:43:06 worker.py:267] the current vLLM instance can use total_gpu_memory (44.40GiB) x gpu_memory_utilization (0.90) = 39.96GiB\n",
      "INFO 03-31 14:43:06 worker.py:267] model weights take 4.72GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 34.69GiB.\n",
      "INFO 03-31 14:43:06 executor_base.py:111] # cuda blocks: 28419, # CPU blocks: 3276\n",
      "INFO 03-31 14:43:06 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 111.01x\n",
      "INFO 03-31 14:43:08 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-31 14:43:20 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.31 GiB\n",
      "INFO 03-31 14:43:20 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 14.87 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "model_name = \"/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f\"\n",
    "# model_name = \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\"\n",
    "\n",
    "llm = LLM(\n",
    "            model=model_name,\n",
    "            dtype=\"half\"\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:02<00:00,  2.60s/it, est. speed input: 16.56 toks/s, output: 110.90 toks/s]\n"
     ]
    }
   ],
   "source": [
    "query = \"sort a list in python\"\n",
    "prompts = [f\"Expand the following search query with additional relevant terms and make 20 expanded queries. Seperate each expanded query with '\\n' seperator. \\nQuery: {query}\\nExpanded Queries:\"]\n",
    "\n",
    "results = llm.generate(prompts, sampling_params)\n",
    "explanations = [result.outputs[0].text for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n1. sort a list in python using built-in sort function\\n2. sort a list in python using sorted() function\\n3. sort a list in python using lambda function\\n4. sort a list in python using custom sort function\\n5. sort a list in python using key function\\n6. sort a list in python using reverse order\\n7. sort a list in python using multiple keys\\n8. sort a list in python using custom key function\\n9. sort a list in python using custom comparison function\\n10. sort a list in python using custom sorting algorithm\\n11. sort a list in python using heapq module\\n12. sort a list in python using bisect module\\n13. sort a list in python using heapq.nsmallest() function\\n14. sort a list in python using heapq.nlargest() function\\n15. sort a list in python using sorted() function with key and reverse\\n16. sort a list in python using list.sort() method with key and reverse\\n17. sort a list in python using sorted() function with custom comparison function\\n18. sort a list in python using list.sort() method with custom comparison function\\n19. sort a list in python using heapq.heapify() function\\n20. sort a list in python using heapq.heappush() function']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Scores for candidate explanations: [1.18711262 1.047546   1.15146349 1.18711262 1.15146349 1.15146349\n",
      " 1.15146349 1.09704935 1.09704935 1.09704935 1.15146349 1.15146349\n",
      " 1.047546   1.047546   0.88737757 0.88737757 0.88737757 0.88737757\n",
      " 1.047546   1.047546  ]\n",
      "\n",
      "Original Query: sort a list in python\n",
      "Best Explanation: \n",
      "1. sort a list in python using built-in sort function\n",
      "2. sort a list in python using sorted() function\n",
      "3. sort a list in python using lambda function\n",
      "4. sort a list in python using custom sort function\n",
      "5. sort a list in python using key function\n",
      "6. sort a list in python using reverse order\n",
      "7. sort a list in python using multiple keys\n",
      "8. sort a list in python using custom key function\n",
      "9. sort a list in python using custom comparison function\n",
      "10. sort a list in python using custom sorting algorithm\n",
      "11. sort a list in python using heapq module\n",
      "12. sort a list in python using bisect module\n",
      "13. sort a list in python using heapq.nsmallest() function\n",
      "14. sort a list in python using heapq.nlargest() function\n",
      "15. sort a list in python using sorted() function with key and reverse\n",
      "16. sort a list in python using list.sort() method with key and reverse\n",
      "17. sort a list in python using sorted() function with custom comparison function\n",
      "18. sort a list in python using list.sort() method with custom comparison function\n",
      "19. sort a list in python using heapq.heapify() function\n",
      "20. sort a list in python using heapq.heappush() function\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/anamikaghosh_umass_edu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Ensure you have installed the required packages:\n",
    "# pip install rank-bm25 nltk\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "splitted_explanations = [line.strip() for line in explanations[0].split('\\n') if line.strip()]\n",
    "\n",
    "splitted_explanations = [re.sub(r'^\\d+\\.\\s*', '', exp) for exp in splitted_explanations]\n",
    "\n",
    "original_query = \"sort a list in python\"\n",
    "\n",
    "tokenized_explanations = [word_tokenize(exp.lower()) for exp in splitted_explanations]\n",
    "tokenized_query = word_tokenize(original_query.lower())\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_explanations)\n",
    "\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "print(\"BM25 Scores for candidate explanations:\", scores)\n",
    "\n",
    "best_index = np.argmax(scores)\n",
    "best_explanation = explanations[best_index]\n",
    "\n",
    "print(\"\\nOriginal Query:\", original_query)\n",
    "print(\"Best Explanation:\", best_explanation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
