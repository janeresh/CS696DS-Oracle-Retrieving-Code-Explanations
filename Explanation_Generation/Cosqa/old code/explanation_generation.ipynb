{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['corpus', 'queries'])\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"CoIR-Retrieval/cosqa-queries-corpus\")\n",
    "print(dataset.keys())\n",
    "# Convert the 'train' split to a pandas DataFrame\n",
    "# df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "# # Save the DataFrame to a CSV file (without the index)\n",
    "# df.to_csv(\"cosqa_queries_corpus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset=pd.DataFrame(dataset['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['_id', 'partition', 'text', 'title', 'language', 'meta_information'],\n",
       "    num_rows: 20604\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['corpus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = new_dataset.rename(columns={'_id':'corpus_id', 'text':'code'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>code</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "      <th>meta_information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d1</td>\n",
       "      <td>train</td>\n",
       "      <td>def writeBoolean(self, n):\\n        \"\"\"\\n     ...</td>\n",
       "      <td></td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d2</td>\n",
       "      <td>train</td>\n",
       "      <td>def paste(xsel=False):\\n    \"\"\"Returns system ...</td>\n",
       "      <td></td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d3</td>\n",
       "      <td>train</td>\n",
       "      <td>def _format_json(data, theme):\\n    \"\"\"Pretty ...</td>\n",
       "      <td></td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d4</td>\n",
       "      <td>train</td>\n",
       "      <td>def create_path(path):\\n    \"\"\"Creates a absol...</td>\n",
       "      <td></td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d5</td>\n",
       "      <td>train</td>\n",
       "      <td>def _vector_or_scalar(x, type='row'):\\n    \"\"\"...</td>\n",
       "      <td></td>\n",
       "      <td>PYTHON</td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  corpus_id partition                                               code  \\\n",
       "0        d1     train  def writeBoolean(self, n):\\n        \"\"\"\\n     ...   \n",
       "1        d2     train  def paste(xsel=False):\\n    \"\"\"Returns system ...   \n",
       "2        d3     train  def _format_json(data, theme):\\n    \"\"\"Pretty ...   \n",
       "3        d4     train  def create_path(path):\\n    \"\"\"Creates a absol...   \n",
       "4        d5     train  def _vector_or_scalar(x, type='row'):\\n    \"\"\"...   \n",
       "\n",
       "  title language     meta_information  \n",
       "0         PYTHON  {'dummy_field': ''}  \n",
       "1         PYTHON  {'dummy_field': ''}  \n",
       "2         PYTHON  {'dummy_field': ''}  \n",
       "3         PYTHON  {'dummy_field': ''}  \n",
       "4         PYTHON  {'dummy_field': ''}  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          _id partition                                             text  \\\n",
      "0          q1     train                python code to write bool value 1   \n",
      "1          q2     train             \"python how to manipulate clipboard\"   \n",
      "2          q3     train                    python colored output to html   \n",
      "3          q4     train  python \"create directory\" using \"relative path\"   \n",
      "4          q5     train                        python column of an array   \n",
      "...       ...       ...                                              ...   \n",
      "20599  q20600      test              python how to select first 100 rows   \n",
      "20600  q20601      test          removing columnsns in data frame python   \n",
      "20601  q20602      test                     python array to torch tensor   \n",
      "20602  q20603      test             how to turn a list into a csv python   \n",
      "20603  q20604      test                    how do i unzip file in python   \n",
      "\n",
      "      title language     meta_information  \n",
      "0                     {'dummy_field': ''}  \n",
      "1                     {'dummy_field': ''}  \n",
      "2                     {'dummy_field': ''}  \n",
      "3                     {'dummy_field': ''}  \n",
      "4                     {'dummy_field': ''}  \n",
      "...     ...      ...                  ...  \n",
      "20599                 {'dummy_field': ''}  \n",
      "20600                 {'dummy_field': ''}  \n",
      "20601                 {'dummy_field': ''}  \n",
      "20602                 {'dummy_field': ''}  \n",
      "20603                 {'dummy_field': ''}  \n",
      "\n",
      "[20604 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "new_dataset_queries=pd.DataFrame(dataset['queries'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_queries = new_dataset_queries.rename(columns={'_id':'query_id', 'text':'doc'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>partition</th>\n",
       "      <th>doc</th>\n",
       "      <th>title</th>\n",
       "      <th>language</th>\n",
       "      <th>meta_information</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>train</td>\n",
       "      <td>python code to write bool value 1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>train</td>\n",
       "      <td>\"python how to manipulate clipboard\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>train</td>\n",
       "      <td>python colored output to html</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q4</td>\n",
       "      <td>train</td>\n",
       "      <td>python \"create directory\" using \"relative path\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q5</td>\n",
       "      <td>train</td>\n",
       "      <td>python column of an array</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>{'dummy_field': ''}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id partition                                              doc title  \\\n",
       "0       q1     train                python code to write bool value 1         \n",
       "1       q2     train             \"python how to manipulate clipboard\"         \n",
       "2       q3     train                    python colored output to html         \n",
       "3       q4     train  python \"create directory\" using \"relative path\"         \n",
       "4       q5     train                        python column of an array         \n",
       "\n",
       "  language     meta_information  \n",
       "0           {'dummy_field': ''}  \n",
       "1           {'dummy_field': ''}  \n",
       "2           {'dummy_field': ''}  \n",
       "3           {'dummy_field': ''}  \n",
       "4           {'dummy_field': ''}  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset_queries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_code = pd.concat([new_dataset_queries[['query_id','doc']], new_dataset[['corpus_id','code']]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>python code to write bool value 1</td>\n",
       "      <td>d1</td>\n",
       "      <td>def writeBoolean(self, n):\\n        \"\"\"\\n     ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>\"python how to manipulate clipboard\"</td>\n",
       "      <td>d2</td>\n",
       "      <td>def paste(xsel=False):\\n    \"\"\"Returns system ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>python colored output to html</td>\n",
       "      <td>d3</td>\n",
       "      <td>def _format_json(data, theme):\\n    \"\"\"Pretty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q4</td>\n",
       "      <td>python \"create directory\" using \"relative path\"</td>\n",
       "      <td>d4</td>\n",
       "      <td>def create_path(path):\\n    \"\"\"Creates a absol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q5</td>\n",
       "      <td>python column of an array</td>\n",
       "      <td>d5</td>\n",
       "      <td>def _vector_or_scalar(x, type='row'):\\n    \"\"\"...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                              doc corpus_id  \\\n",
       "0       q1                python code to write bool value 1        d1   \n",
       "1       q2             \"python how to manipulate clipboard\"        d2   \n",
       "2       q3                    python colored output to html        d3   \n",
       "3       q4  python \"create directory\" using \"relative path\"        d4   \n",
       "4       q5                        python column of an array        d5   \n",
       "\n",
       "                                                code  \n",
       "0  def writeBoolean(self, n):\\n        \"\"\"\\n     ...  \n",
       "1  def paste(xsel=False):\\n    \"\"\"Returns system ...  \n",
       "2  def _format_json(data, theme):\\n    \"\"\"Pretty ...  \n",
       "3  def create_path(path):\\n    \"\"\"Creates a absol...  \n",
       "4  def _vector_or_scalar(x, type='row'):\\n    \"\"\"...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_doc_code.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_code.to_csv(\"cosqa_queries_code_corpus.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to help with memory fragmentation.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# def clean_output(text):\n",
    "#     if \"<think>\" in text:\n",
    "#         cleaned = text.rsplit(\"<think>\", 1)[-1]\n",
    "#     else:\n",
    "#         cleaned = text\n",
    "#     return cleaned.strip()\n",
    "\n",
    "def clean_output(text):\n",
    "    cleaned = text.split(\"Answer:\")[-1]\n",
    "    return cleaned\n",
    "\n",
    "# def clean_output(text):\n",
    "#     # Split from the right using 'Answer:' and return the portion after the last occurrence.\n",
    "#     if \"Answer:\" in text:\n",
    "#         return text.rsplit(\"Answer:\", 1)[-1].strip()\n",
    "#     return text.strip()\n",
    "\n",
    "\n",
    "class ExplanationGeneratorLama:\n",
    "    def __init__(self, model_name):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.tokenizer.padding_side = \"left\"  # Set left-padding for decoder-only models. only for Llama3\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16,    # Load in half precision if supported.\n",
    "            low_cpu_mem_usage=True\n",
    "        )\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(self.device)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def generate_explanations_batch(self, entries, max_new_tokens=500):\n",
    "        # Create prompts by combining each entry with each prompt template.\n",
    "        \n",
    "        prompts = []\n",
    "        \n",
    "        for entry in entries:\n",
    "            prompt_templates = [\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Provide a concise explanation of what the above doc and code mean. \"\n",
    "                \"Generate strictly less than 100 words in total. Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "                , \n",
    "\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Provide a detailed line-by-line explanation of this code snippet, describing the purpose and functionality of each statement, function, and control structure. \"\n",
    "                \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "                ,\n",
    "\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Summarize what this code snippet does in simple, non-technical language, focusing on its overall purpose and key operations for someone with little programming experience. \"\n",
    "                \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "                ,\n",
    "\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Generate an explanation of the code snippet in such a way that it can regenerate the code based on this explanation. \"\n",
    "                \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "            ]\n",
    "            \n",
    "            for template in prompt_templates:\n",
    "                prompt = (\n",
    "                    f\"Doc string: {entry['doc']}\\n\"\n",
    "                    f\"Code snippet: {entry['code']}\\n\"\n",
    "                    f\"{template}\\n\"\n",
    "                    \"Answer: \\n\"\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "                \n",
    "        # Tokenize all prompts at once.\n",
    "        inputs = self.tokenizer(\n",
    "            prompts, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=2000,\n",
    "            padding=True\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generate outputs for all prompts.\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1,  # One output per prompt variation.\n",
    "            )\n",
    "            \n",
    "        # Decode the outputs.\n",
    "        explanations = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        \n",
    "        # Regroup explanations by entry.\n",
    "        n_prompts = len(prompt_templates)\n",
    "        grouped_explanations = []\n",
    "        for i in range(0, len(explanations), n_prompts):\n",
    "            grouped_explanations.append(explanations[i:i+n_prompts])\n",
    "            \n",
    "        return grouped_explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example code snippet\n",
    "# doc= \"Python : Add Two Numbers\"\n",
    "# code_snippet = \"def add(a, b): return a + b\"\n",
    "\n",
    "# # Create a prompt instructing the model to explain the code\n",
    "# prompt = f\"Explain what the following Python code does:\\n\\n{code_snippet}\\n\\nExplanation:\"\n",
    "\n",
    "# # Generate 5 different explanations\n",
    "# # explanations = generator(prompt, max_length=150, num_return_sequences=5, do_sample=True, temperature=0.7)\n",
    "# # explanations=generate_explanation(doc,code_snippet,1)\n",
    "\n",
    "# # Print out the explanations\n",
    "\n",
    "# print(explanations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/work/pi_wenlongzhao_umass_edu/27/vaishnavisha/datasets/CoSQA_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19604, 13)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model llama3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:09<00:09,  9.35s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.18s/it]\u001b[A\n",
      "\n",
      "Processing batches:   0%|          | 0/79 [00:00<?, ?it/s]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   1%|▏         | 1/79 [00:05<07:02,  5.42s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   3%|▎         | 2/79 [00:10<06:50,  5.33s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   4%|▍         | 3/79 [00:13<05:33,  4.39s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   5%|▌         | 4/79 [00:19<06:14,  4.99s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   6%|▋         | 5/79 [00:22<05:08,  4.17s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   8%|▊         | 6/79 [00:24<04:19,  3.56s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:   9%|▉         | 7/79 [00:28<04:19,  3.61s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  10%|█         | 8/79 [00:31<04:01,  3.40s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  11%|█▏        | 9/79 [00:35<04:02,  3.46s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  13%|█▎        | 10/79 [00:40<04:33,  3.96s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  14%|█▍        | 11/79 [00:46<05:11,  4.58s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  15%|█▌        | 12/79 [00:49<04:46,  4.27s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  16%|█▋        | 13/79 [00:55<05:10,  4.70s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  18%|█▊        | 14/79 [01:00<05:04,  4.68s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  19%|█▉        | 15/79 [01:03<04:39,  4.37s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  20%|██        | 16/79 [01:09<05:02,  4.80s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  22%|██▏       | 17/79 [01:14<05:06,  4.94s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  23%|██▎       | 18/79 [01:20<05:15,  5.17s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  24%|██▍       | 19/79 [01:24<04:38,  4.65s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  25%|██▌       | 20/79 [01:29<04:42,  4.78s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  27%|██▋       | 21/79 [01:32<04:18,  4.46s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  28%|██▊       | 22/79 [01:38<04:42,  4.95s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  29%|██▉       | 23/79 [01:45<04:58,  5.33s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  30%|███       | 24/79 [01:50<04:53,  5.33s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  32%|███▏      | 25/79 [01:56<04:56,  5.50s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  33%|███▎      | 26/79 [02:02<04:57,  5.62s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  34%|███▍      | 27/79 [02:07<04:45,  5.49s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  35%|███▌      | 28/79 [02:10<04:01,  4.74s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  37%|███▋      | 29/79 [02:13<03:35,  4.30s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  38%|███▊      | 30/79 [02:19<03:45,  4.60s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  39%|███▉      | 31/79 [02:23<03:43,  4.67s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  41%|████      | 32/79 [02:28<03:33,  4.54s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  42%|████▏     | 33/79 [02:32<03:22,  4.39s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  43%|████▎     | 34/79 [02:35<03:00,  4.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  44%|████▍     | 35/79 [02:40<03:14,  4.42s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  46%|████▌     | 36/79 [02:46<03:31,  4.92s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  47%|████▋     | 37/79 [02:50<03:07,  4.46s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  48%|████▊     | 38/79 [02:54<03:08,  4.59s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  49%|████▉     | 39/79 [02:59<02:57,  4.45s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  51%|█████     | 40/79 [03:05<03:14,  4.99s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  52%|█████▏    | 41/79 [03:10<03:12,  5.06s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  53%|█████▎    | 42/79 [03:16<03:15,  5.29s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  54%|█████▍    | 43/79 [03:22<03:14,  5.41s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  56%|█████▌    | 44/79 [03:25<02:43,  4.68s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  57%|█████▋    | 45/79 [03:28<02:26,  4.31s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  58%|█████▊    | 46/79 [03:29<01:53,  3.45s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  59%|█████▉    | 47/79 [03:33<01:52,  3.51s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  61%|██████    | 48/79 [03:37<01:48,  3.51s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  62%|██████▏   | 49/79 [03:39<01:32,  3.09s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  63%|██████▎   | 50/79 [03:45<01:53,  3.92s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  65%|██████▍   | 51/79 [03:50<02:01,  4.35s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  66%|██████▌   | 52/79 [03:53<01:50,  4.08s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  67%|██████▋   | 53/79 [03:59<01:55,  4.46s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  68%|██████▊   | 54/79 [04:05<02:03,  4.94s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  70%|██████▉   | 55/79 [04:07<01:41,  4.24s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  71%|███████   | 56/79 [04:14<01:52,  4.88s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  72%|███████▏  | 57/79 [04:16<01:28,  4.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  73%|███████▎  | 58/79 [04:21<01:29,  4.25s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  75%|███████▍  | 59/79 [04:26<01:29,  4.46s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  76%|███████▌  | 60/79 [04:31<01:28,  4.68s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  77%|███████▋  | 61/79 [04:36<01:29,  4.97s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  78%|███████▊  | 62/79 [04:41<01:24,  4.98s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  80%|███████▉  | 63/79 [04:47<01:21,  5.07s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  81%|████████  | 64/79 [04:52<01:18,  5.24s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  82%|████████▏ | 65/79 [04:56<01:07,  4.82s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  84%|████████▎ | 66/79 [05:00<00:58,  4.49s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  85%|████████▍ | 67/79 [05:06<00:59,  4.99s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  86%|████████▌ | 68/79 [05:11<00:55,  5.01s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  87%|████████▋ | 69/79 [05:14<00:42,  4.28s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  89%|████████▊ | 70/79 [05:16<00:32,  3.66s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  90%|████████▉ | 71/79 [05:22<00:34,  4.29s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  91%|█████████ | 72/79 [05:28<00:33,  4.80s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  92%|█████████▏| 73/79 [05:33<00:30,  5.10s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  94%|█████████▎| 74/79 [05:37<00:22,  4.51s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  95%|█████████▍| 75/79 [05:41<00:18,  4.65s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  96%|█████████▌| 76/79 [05:47<00:14,  4.99s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  97%|█████████▋| 77/79 [05:50<00:08,  4.25s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches:  99%|█████████▊| 78/79 [05:53<00:03,  3.90s/it]\u001b[ASetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "\n",
      "Processing batches: 100%|██████████| 79/79 [05:55<00:00,  4.50s/it]\u001b[A\n",
      "Processing models: 100%|██████████| 1/1 [06:10<00:00, 370.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Explanations from all models have been saved to /work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_llama3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CodeXGLUE/Text-Code/NL-code-search-WebQuery/CoSQA/cosqa-dev.json\"\n",
    "    output_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_llama3.json\"\n",
    "    \n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    print('Data loaded')\n",
    "    \n",
    "    filtered_entries = [\n",
    "        {\"doc\": entry.get(\"doc\", \"\"), \"code\": entry.get(\"code\", \"\"), \"label\": entry.get(\"label\")}\n",
    "        for entry in data if entry.get(\"label\") == 1\n",
    "    ]\n",
    "    \n",
    "    for entry in filtered_entries:\n",
    "        entry[\"explanations\"] = {}\n",
    "    \n",
    "    models_dict = {\n",
    "        # 'llama3-instruct': \"/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6\"\n",
    "        # 'llama2': \"/datasets/ai/llama2/hub/llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590\",\n",
    "        # \"deepseek\": \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "        # \"gemma\": \"/datasets/ai/gemma/hub/models--google--gemma-7b-it/snapshots/8adab6a35fdbcdae0ae41ab1f711b1bc8d05727e\"\n",
    "        # \"llama3\": \"/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-3B/snapshots/13afe5124825b4f3751f836b40dafda64c1ed062\"\n",
    "    }\n",
    "    \n",
    "    batch_size = 20\n",
    "    \n",
    "    for model_key, model_path in tqdm(models_dict.items(), desc=\"Processing models\"):\n",
    "        print(f\"\\nProcessing model {model_key}...\")\n",
    "        generator = ExplanationGeneratorLama(model_path)\n",
    "        for i in tqdm(range(0, len(filtered_entries), batch_size), desc=\"Processing batches\"):\n",
    "            batch_entries = filtered_entries[i:i+batch_size]\n",
    "            batch_explanations = generator.generate_explanations_batch(batch_entries)\n",
    "            for j, entry in enumerate(batch_entries):\n",
    "                raw_text = batch_explanations[j].strip()\n",
    "                parts = raw_text.split(\"Answer: \")\n",
    "                if len(parts) > 1:\n",
    "                    explanation_text = parts[1].strip()\n",
    "                else:\n",
    "                    explanation_text = raw_text\n",
    "                entry[\"explanations\"][model_key] = explanation_text\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        del generator\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    with open(output_json_path, \"w\") as f:\n",
    "        json.dump(filtered_entries, f, indent=4)\n",
    "    \n",
    "    print(f\"\\nExplanations from all models have been saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model deepseek...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Processing batches:   0%|          | 0/2 [06:13<?, ?it/s]\n",
      "Processing models:   0%|          | 0/1 [09:43<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Wrap inference in a no_grad context to prevent gradient computations.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 45\u001b[0m     batch_explanations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_explanations_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_entries\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, explanation_variants \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_explanations):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(batch_explanations))\n",
      "Cell \u001b[0;32mIn[21], line 92\u001b[0m, in \u001b[0;36mExplanationGeneratorLama.generate_explanations_batch\u001b[0;34m(self, entries, max_new_tokens)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# Generate outputs for all prompts.\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 92\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# One output per prompt variation.\u001b[39;49;00m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Decode the outputs.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m explanations \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mdecode(output, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/generation/utils.py:2223\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2215\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2216\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2217\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2218\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2219\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2220\u001b[0m     )\n\u001b[1;32m   2222\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2223\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2224\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2233\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2234\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2236\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2237\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2242\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2243\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/generation/utils.py:3211\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3208\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3211\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3212\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:856\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    853\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    855\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    871\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:579\u001b[0m, in \u001b[0;36mQwen2Model.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    567\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    568\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    569\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    576\u001b[0m         position_embeddings,\n\u001b[1;32m    577\u001b[0m     )\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:276\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    275\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 276\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    279\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/transformers/models/qwen2/modeling_qwen2.py:57\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 57\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch, gc\n",
    "from tqdm import tqdm\n",
    "# Import or define your ExplanationGeneratorLama class here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa_queries_code_corpus.csv\"  # change this to your CSV input path\n",
    "    output_csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CoSQA_granite_explanations_tmp1.csv\"\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path).head(5)\n",
    "    print(\"Data loaded from CSV\")\n",
    "    \n",
    "    # If your CSV has a \"label\" column and you only want to process rows where label == 1, uncomment:\n",
    "    # if \"label\" in df.columns:\n",
    "    #     df = df[df[\"label\"] == 1].reset_index(drop=True)\n",
    "    \n",
    "    # Define your model(s) in a dictionary.\n",
    "    models_dict = {\n",
    "        \"deepseek\": \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "        # 'granite': \"/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f\"\n",
    "\n",
    "    }\n",
    "    \n",
    "    # Create empty columns for each model's explanation in the DataFrame\n",
    "    for model_key in models_dict.keys():\n",
    "        df[f'explanation_{model_key}'] = \"\"\n",
    "    \n",
    "    batch_size = 4  # If memory issues persist, try reducing this value further.\n",
    "    \n",
    "    for model_key, model_path in tqdm(models_dict.items(), desc=\"Processing models\"):\n",
    "        print(f\"\\nProcessing model {model_key}...\")\n",
    "        generator = ExplanationGeneratorLama(model_path)\n",
    "        # Set the model to evaluation mode to disable dropout and other training-specific layers.\n",
    "        if hasattr(generator, 'model'):\n",
    "            generator.model.eval()\n",
    "        \n",
    "        # Process the DataFrame in batches\n",
    "        for i in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "            # Create a batch of entries (each is a dict with \"doc\" and \"code\")\n",
    "            batch_entries = df.iloc[i:i+batch_size][[\"corpus_id\", \"query_id\", \"doc\", \"code\"]].to_dict(\"records\")\n",
    "            \n",
    "            # Wrap inference in a no_grad context to prevent gradient computations.\n",
    "            with torch.no_grad():\n",
    "                batch_explanations = generator.generate_explanations_batch(batch_entries)\n",
    "            \n",
    "            for j, explanation_variants in enumerate(batch_explanations):\n",
    "                print(type(batch_explanations))\n",
    "                for idx, raw_text in enumerate(explanation_variants):\n",
    "                    # print(raw_text)\n",
    "                    # Clean the output using the clean_output function.\n",
    "                    cleaned_text = raw_text.split(\"Answer:\\n\")[-1]\n",
    "                    # cleaned_text = clean_output(raw_text.strip())\n",
    "                    # Optionally remove any \"Answer: \" prefix.\n",
    "                    # parts = cleaned_text.split(\"Answer:\\n\\n\")\n",
    "                    # if len(parts) > 1:\n",
    "                    #     explanation_text = parts[1].strip()\n",
    "                    # else:\n",
    "                    #     explanation_text = cleaned_text\n",
    "                    # print(cleaned_text)\n",
    "                    # Save each explanation variant into its designated column.\n",
    "                    df.loc[i+j, f'explanation_{model_key}_{idx+1}'] = raw_text\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        del generator\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save the DataFrame with the new explanation columns to a CSV file\n",
    "    # df.to_csv(output_csv_path, index=False)\n",
    "    # print(f\"\\nExplanations from all models have been saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_exp_deepseek'] = df['explanation_deepseek'].apply(clean_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n\\nThe doc and code are related to writing a boolean value (1) to a stream.',\n",
       "       '\\n\\nThe code pastes system clipboard contents into memory, allowing selection from either primary or clipboard.',\n",
       "       '\\n\\nThe code snippet is a function `_format_json` that takes `data` and `theme` as arguments. It converts `data` into JSON format using `json.dumps` with `indent=2` and `sort_keys=True`. If `pygments` is enabled and `sys.stdout.isatty()` returns `True`, it uses the `Terminal256Formatter` from `pygments` to highlight the JSON output with a terminal style. Otherwise, it returns the raw JSON output.',\n",
       "       '\\n\\nThe doc explains creating an absolute path from a relative path, and the code snippet shows that the function checks if the path exists and creates it if not.',\n",
       "       \"\\n\\nThe doc and code are about converting an array or list into a scalar, row vector, or column vector. The code checks the type of `x` and converts it accordingly. If `x` is a list or tuple, it's converted to a numpy array. If it's already a numpy array, it asserts it's 1D. If the type is 'column', it adds a new column using `x[:, None]`.\",\n",
       "       \"\\n\\nThe doc and code describe a function that retrieves a property from an experiment object. The function retrieves the property using `getattr`, returns a success response with the property's value.\",\n",
       "       \"\\n\\nThe function reads a .wav file, extracts the first channel's data, processes it into frames, and returns the first channel's data along with the sample rate and bit depth.\",\n",
       "       '\\n\\nTo use a dictionary in the `source_range` function, you need to specify the number of variables in the dictionary. This helps the function know how to map the source numbers to the corresponding values in the dictionary. The function then returns a dictionary of tuples, each containing the index and value for each source variable.',\n",
       "       '\\n\\nThe code snippet calculates the time difference in milliseconds between two timestamps. It creates a `timespan` object by subtracting `start_time` from `datetime.datetime.now()`, converts this duration to milliseconds, and returns the result.',\n",
       "       '\\n\\nThe code converts array-like objects into a NumPy array. It first checks if the input is bytes and uses `frombuffer` with the given dtype, otherwise uses `asarray` with the same dtype. This handles both byte arrays and other array-like objects.',\n",
       "       \"\\n\\nThe function `get_uniques` takes a list `l` and returns a list of unique elements from `l`. It iterates over each element in `l`, adding it to the result list only if it hasn't been added before. The result is a list containing each element of `l` exactly once.\",\n",
       "       '\\n\\nThe code snippet is a wrapper for 1D interpolation using the interpolate_1d function, using deprecated methods. It takes x, xp, and other keyword arguments and calls the interpolation function.',\n",
       "       '\\n\\nThe code serializes a numpy array into a base64 string using the save() method.',\n",
       "       '\\n\\nThe docstring explains that the function calculates the 25th and 75th percentiles of a list. The code sorts the list and calculates the index for the desired percentile, then returns the value at that index.',\n",
       "       \"\\n\\nThe function computes a hash of a string using a specific algorithm, returning a hash value. It initializes a variable `h` to 5381 and updates it for each character in the string, multiplying by 33 and adding the character's Unicode value. The result is the hash.\\n\\nThe code snippet initializes `h` to 5381 and iterates through each character, updating `h` with `h * 33 + ord(c)`. The result is the hash.\\n\\nThe function computes a hash of a string using a specific algorithm, returning a hash value. It initializes a variable `h` to 5381 and updates it for each character in the string, multiplying by 33 and adding the character's Unicode value. The result is the hash.\",\n",
       "       '\\n\\nThe code is creating a transformation matrix from a rotation matrix R and a translation vector t. It reshapes R to be 3x3 and t to be 3x1. Then it combines R and t into a larger matrix and appends [0,0,0,1] to it. The resulting matrix is a 4x4 matrix used for transformations.',\n",
       "       '\\n\\nThe doc and code are about encoding a boolean value into bytes. The function returns a byte string starting with 0x08, followed by the name, and then a byte for the boolean value (1 for True, 0 for False).\\n\\nExplanation: The function encodes a boolean into bytes by appending a byte (1 for True, 0 for False) to a fixed prefix (0x08). The name is included as is.',\n",
       "       '\\n\\nThe code performs a 3D rotation of 2D points around the z-axis, using a rotation matrix and adds an offset to shift points out of the plane.',\n",
       "       '\\n\\nThe function `_not` returns the opposite of the given condition. If the condition is None, it returns True. Otherwise, it returns the negation of the condition. The function takes an optional parameter **kwargs. The result is a boolean.',\n",
       "       '\\n\\nThe docstring explains that ` HttpResponse403` is a Python function returning a 403 status response. The code snippet shows it takes request, template, content, and content_type parameters, returning an `AccessFailedResponse` instance.',\n",
       "       \"\\n\\nThe code snippet retrieves key-value pairs from a section in a configuration file using the configparser class. It filters out the '__name__' key and returns a list of tuples.\",\n",
       "       \"\\n\\nTo compute the magnitude of each vector in the array, the function checks if the first element is a NumPy array. If it is, it returns an array of magnitudes. Otherwise, it applies NumPy's `linalg.norm` to the entire array. This function efficiently computes the magnitude of each vector.\",\n",
       "       '\\n\\nThe code converts a ConfigParser to a dictionary by iterating over sections and options, storing them in a nested dictionary structure. Each section maps to a dictionary of options, retrieved from the ConfigParser.',\n",
       "       \"\\n\\nThe doc and code explain how adding two numbers works in Python. The __add__ method handles the '+' operator by combining the values of self and other.\",\n",
       "       '\\n\\nThe doc and code are attempting to connect to MySQL using provided credentials, with retries if passwords fail.',\n",
       "       \"\\n\\nThe function retrieves a specific column from a matrix, whether it's a NumPy array or a pandas DataFrame. If the matrix is a DataFrame, it accesses the column using `X[column].values`. If it's a NumPy array, it accesses the column using `X[:, column]`.\",\n",
       "       \"\\n\\nThe code connects to an API using Bitbucket's API library, specifying the URL, username, and password. It returns a session object and logs the connection. The explanation involves connecting to an API with username and password, returning a session, and logging the connection.\",\n",
       "       '\\n\\nThe doc and code add an empty row with only an index value to the data frame in place.',\n",
       "       \"\\n\\nThe container is being stopped when it exists, and the code removes it if it's created. If the container is created, it halts. The loop pops and closes the clients, and upon creation, it halts.\",\n",
       "       '\\n\\nThe docstring and code are explaining a function that adds indentation to a string. The function `indented` takes a string and adds indentation to each line. The code snippet shows that this function is used to create a string with curly brackets around the indented text.',\n",
       "       '\\n\\nThe context manager ensures the action runs within its context, preventing it from finishing abruptly after exiting.',\n",
       "       '\\n\\nThe docstring and code snippet define a function `pformat` that formats Python objects into a pretty-printed representation. The function accepts parameters `indent`, `width`, and `depth`, where `depth` determines the level of recursion for formatting.',\n",
       "       '\\n\\nThe doc and code demonstrate a context manager that temporarily sets `sys.argv`, which holds the current argument list, and restores it afterward. The code snippet shows this process: \\n\\n1. **Temporarily set environment**: The context manager replaces `sys.argv` with new arguments.\\n2. **Restores environment**: After the block, restores `sys.argv` to its original state.\\n\\nThe code snippet shows that the context manager does this in a try-finally block.',\n",
       "       '\\n\\nThe doc and code are about serializing an object into a dictionary-like structure. The serializer handles lists by recursively serializing each element, and other objects are serialized using the GenericSerializer.',\n",
       "       '\\n\\nThe method `advance_one_line` changes the token to advance to the next line.\\n\\nThe docstring and code explain that the method `advance_one_line` changes the token to advance to the next line.\\n\\nThe code snippet shows that the method uses a while loop to advance the token until it is on the next line.\\n\\nThe code snippet and docstring both demonstrate that the method advances the token to the next line.\\n\\nThe code snippet and docstring both explain that the method changes the token to advance to the next line.\\n\\nThe code snippet and docstring both show that the method changes the token to advance to the next line.\\n\\nThe code snippet and docstring both demonstrate that the method advances the token to the next line.\\n\\nThe code snippet and docstring both explain that the method advances the token to the next line.\\n\\nThe code snippet and docstring both show that the method changes the token to advance to the next line.',\n",
       "       '\\n\\nThe code adds Swagger documentation to a Django project by generating HTML from a JSON file. It uses a template file and renders it with the root directory and JSON URL.',\n",
       "       '\\n\\nThe doc and code mean that `do_next` skips the next command.',\n",
       "       '\\n\\nThe docstring and code explain that adding two matrices with the same shape (one dimension) involves concatenating them along the first axis using the `+` operator, returning a new `LabeledMatrix` with the combined data and the same labels.',\n",
       "       '\\n\\nThe function interpolates flux based on line_wave and wave parameters.',\n",
       "       '\\n\\nThe docstring and code snippet define a function `send` that sends a message to a WebSocket, supporting binary or string data. It returns a WebSocket object.',\n",
       "       '\\n\\nThe code snippet attempts to extract a number from a string, where the extraction is performed using the `string.digits` set, and then returns the extracted number cast into the specified type (default is int). The docstring explains that the function tries to convert a string into a number, either int or float, by removing non-digit characters. The code explanation concisely states that the function processes the string to extract a number and returns it in the specified type.',\n",
       "       '\\n\\nThe code snippet creates a horizontal line using a window in Python. The horizontal line has a width and height of 1 unit and contains a horizontal line character.',\n",
       "       '\\n\\nThe code splits the cookies string into individual records using semicolons as the delimiter. For each record, it extracts the key and value by splitting on the first equals sign. The extracted key-value pairs are stored in a dictionary.',\n",
       "       '\\n\\nThe code converts camelCase names to snake_case by first converting to uppercase and then to lowercase. It uses regular expressions to handle the casing changes.',\n",
       "       \"\\n\\nThe doc and code mean that the function populates attributes of an object with values from a provided dictionary. The code snippet shows that the function `populate_obj` takes an object `obj` and a dictionary `attrs`, iterating over each key-value pair in `attrs`, and sets each value into the corresponding attribute of `obj`. The doc string explains that the function copies the dictionary's attributes to the object's attributes.\",\n",
       "       '\\n\\nThe function `wordfreq` analyzes word frequencies in a given text. It reads the text, splits it into words, converts each word to lowercase, and counts occurrences. It returns a dictionary of words and their counts.',\n",
       "       \"\\n\\nThe `copyFile` function copies a file from `input` to `output`. If the file isn't found or it's a replacement, it uses `shutil.copy2`.\",\n",
       "       '\\n\\nThe push function appends the last element of the stack to the heap.',\n",
       "       '\\n\\nThe code pastes data from the clipboard into the buffer with specified parameters.',\n",
       "       '\\n\\nThe code reads an image, applies a contour filter, and saves the result.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_exp_deepseek'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged JSON saved to /work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_merged_explanations.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Merging all the explanations\n",
    "json_path_model1 = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_deepseek.json\"  # e.g., explanations from llama3\n",
    "json_path_model2 = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_llama2.json\"  # e.g., explanations from deepseek\n",
    "json_path_model3 = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_gemma.json\"  # e.g., explanations from gemma\n",
    "json_path_model4 = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_llama3-instruct.json\"\n",
    "json_path_model5 = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_explanations_llama3.json\"\n",
    "\n",
    "output_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_merged_explanations.json\"\n",
    "\n",
    "with open(json_path_model1, \"r\") as f:\n",
    "    data1 = json.load(f)\n",
    "with open(json_path_model2, \"r\") as f:\n",
    "    data2 = json.load(f)\n",
    "with open(json_path_model3, \"r\") as f:\n",
    "    data3 = json.load(f)\n",
    "with open(json_path_model4, \"r\") as f:\n",
    "    data4 = json.load(f)\n",
    "with open(json_path_model5, \"r\") as f:\n",
    "    data5 = json.load(f)\n",
    "\n",
    "\n",
    "merged_entries = []\n",
    "\n",
    "for entry1, entry2, entry3,entry4,entry5 in zip(data1, data2, data3, data4 ,data5):\n",
    "    merged_entry = {\n",
    "        \"doc\": entry1[\"doc\"],\n",
    "        \"code\": entry1[\"code\"],\n",
    "        \"explanations\": {}\n",
    "    }\n",
    "    merged_entry[\"explanations\"].update(entry1.get(\"explanations\", {}))\n",
    "    merged_entry[\"explanations\"].update(entry2.get(\"explanations\", {}))\n",
    "    merged_entry[\"explanations\"].update(entry3.get(\"explanations\", {}))\n",
    "    merged_entry[\"explanations\"].update(entry4.get(\"explanations\", {}))\n",
    "    merged_entry[\"explanations\"].update(entry4.get(\"explanations\", {}))\n",
    "    merged_entry[\"explanations\"].update(entry5.get(\"explanations\", {}))\n",
    "\n",
    "\n",
    "    \n",
    "    merged_entries.append(merged_entry)\n",
    "\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(merged_entries, f, indent=4)\n",
    "\n",
    "print(\"Merged JSON saved to\", output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Initialize model name (using CodeBERT as an example encoder)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model_name = \"/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# model_name = \"microsoft/codebert-base\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and model.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_name\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initialize model name (using CodeBERT as an example encoder)\n",
    "# model_name = \"/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6\"\n",
    "# model_name = \"microsoft/codebert-base\"\n",
    "\n",
    "\n",
    "# Load the tokenizer and model.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "## Llama2\n",
    "# def get_embeddings(text, max_length=512):\n",
    "#     \"\"\"\n",
    "#     Given a text string, returns a fixed-size embedding by mean-pooling\n",
    "#     the encoder's output tokens.\n",
    "#     \"\"\"\n",
    "#     inputs = tokenizer(\n",
    "#         text,\n",
    "#         return_tensors=\"pt\",\n",
    "#         truncation=True,\n",
    "#         padding=True,\n",
    "#         max_length=max_length\n",
    "#     )\n",
    "#     inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs)  # outputs.last_hidden_state: (batch, seq_length, hidden_size)\n",
    "#     embedding = outputs.last_hidden_state.mean(dim=1)  # shape: (batch, hidden_size)\n",
    "#     return embedding.squeeze().cpu().numpy().tolist()\n",
    "def get_cls_embedding(text, max_length=512):\n",
    "    \"\"\"\n",
    "    Given a text string, returns the CLS token embedding as a list.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the first token's embedding (commonly the [CLS] token).\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.squeeze().cpu().numpy().tolist()\n",
    "\n",
    "# Paths to the merged JSON file.\n",
    "merged_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_merged_explanations.json\"\n",
    "output_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_embeddings_explanations_.json\"\n",
    "\n",
    "with open(merged_json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# For each entry, compute embeddings for the concatenation of doc, code, and explanation.\n",
    "# The resulting embedding is stored for each model key.\n",
    "for entry in tqdm(data, desc=\"Computing combined embeddings\"):\n",
    "    entry[\"embeddings\"] = {}\n",
    "    doc = entry.get(\"doc\", \"\").strip()\n",
    "    code = entry.get(\"code\", \"\").strip()\n",
    "    explanations = entry.get(\"explanations\", {})\n",
    "    for model_key, explanation in explanations.items():\n",
    "        explanation = explanation.strip()\n",
    "        # Combine doc, code, and explanation into one text.\n",
    "        combined_text = f\"Doc: {doc}\\nCode: {code}\\nExplanation: {explanation}\"\n",
    "        if not combined_text.strip():\n",
    "            entry[\"embeddings\"][model_key] = []\n",
    "        else:\n",
    "            embedding = get_cls_embedding(combined_text)\n",
    "            entry[\"embeddings\"][model_key] = embedding\n",
    "\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Combined embeddings have been saved to\", output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing combined embeddings: 100%|██████████| 313/313 [00:05<00:00, 55.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged explanations with embeddings have been saved to /work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_embeddings_explanations_.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_minilm_embedding(combined_text: str) -> list:\n",
    "    \n",
    "    embedding = encoder.encode(combined_text, show_progress_bar=False)\n",
    "    return embedding.tolist()\n",
    "\n",
    "merged_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_merged_explanations.json\"\n",
    "output_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_embeddings_explanations_.json\"\n",
    "\n",
    "with open(merged_json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for entry in tqdm(data, desc=\"Computing combined embeddings\"):\n",
    "    entry[\"embeddings\"] = {}\n",
    "    doc = entry.get(\"doc\", \"\").strip()\n",
    "    code = entry.get(\"code\", \"\").strip()\n",
    "    explanations = entry.get(\"explanations\", {})\n",
    "    for model_key, explanation in explanations.items():\n",
    "        explanation = explanation.strip()\n",
    "        combined_text = f\"Doc: {doc}\\nCode: {code}\\nExplanation: {explanation}\"\n",
    "        if not combined_text.strip():\n",
    "            entry[\"embeddings\"][model_key] = []\n",
    "        else:\n",
    "            embedding = get_minilm_embedding(combined_text)\n",
    "            entry[\"embeddings\"][model_key] = embedding\n",
    "\n",
    "with open(output_json_path, \"w\") as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"Merged explanations with embeddings have been saved to\", output_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"sentence-transformers/LaBSE\"\n",
    "# encoder = SentenceTransformer(model_name)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# encoder.to(device)\n",
    "\n",
    "# def get_sbert_embedding(text):\n",
    "\n",
    "#     embedding = encoder.encode(text, convert_to_tensor=True, device=device)\n",
    "#     return embedding.cpu().numpy().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged explanations loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing entries: 100%|██████████| 313/313 [00:00<00:00, 1422.47it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### getting the similarity score for each embeddings\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    return float(np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + 1e-8))\n",
    "\n",
    "\n",
    "input_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa-dev_embeddings_explanations_.json\"  # update this path\n",
    "# output_json_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/similarity_scores.json\"                   # update this path\n",
    "\n",
    "with open(input_json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(\"Merged explanations loaded.\")\n",
    "\n",
    "###### questions ####\n",
    "query = \"How to round a float to int\"  \n",
    "query_embedding = get_minilm_embedding(query)\n",
    "\n",
    "\n",
    "results = pd.DataFrame(columns=['doc','code','best_explanation','max_similarity','best_model'])\n",
    "for entry in tqdm(data, desc=\"Processing entries\"):\n",
    "    max_sim = -1.0\n",
    "    best_model = None\n",
    "    embeddings = entry.get(\"embeddings\", {})\n",
    "    for model_key, emb in embeddings.items():\n",
    "        if emb:  # Ensure the embedding is not empty\n",
    "            emb_np = np.array(emb)\n",
    "            sim = cosine_similarity(query_embedding, emb_np)\n",
    "            if sim > max_sim:\n",
    "                max_sim = sim\n",
    "                best_model = model_key\n",
    "    new_row = {\n",
    "        \"doc\": entry.get(\"doc\", \"\"),\n",
    "        \"code\": entry.get(\"code\", \"\"),\n",
    "        \"best_explanation\": entry.get(\"explanations\", {}).get(best_model, \"\"),\n",
    "        \"max_similarity\": max_sim,\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "    \n",
    "    results.loc[len(results)] = new_row\n",
    "\n",
    "\n",
    "\n",
    "# df = pd.DataFrame(results)\n",
    "# print(\"DataFrame created:\")\n",
    "# print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>code</th>\n",
       "      <th>best_explanation</th>\n",
       "      <th>max_similarity</th>\n",
       "      <th>best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>python split strings into list of lines</td>\n",
       "      <td>def split_multiline(value):\\n    \"\"\"Split a mu...</td>\n",
       "      <td>&lt;/think&gt;\\n\\nThe function `split_multiline` tak...</td>\n",
       "      <td>0.021005</td>\n",
       "      <td>deepseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loading a series of images in python and resiz...</td>\n",
       "      <td>def load_preprocess_images(image_paths: List[s...</td>\n",
       "      <td>The code loads a list of images specified with...</td>\n",
       "      <td>0.019301</td>\n",
       "      <td>llama2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>python save graph into file</td>\n",
       "      <td>def to_dotfile(G: nx.DiGraph, filename: str):\\...</td>\n",
       "      <td>The code defines a function called `to_dotfile...</td>\n",
       "      <td>-0.031910</td>\n",
       "      <td>gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>add color to print python</td>\n",
       "      <td>def write_color(string, name, style='normal', ...</td>\n",
       "      <td>The code defines a function called `write_colo...</td>\n",
       "      <td>-0.027941</td>\n",
       "      <td>gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>python limit number to two decimals</td>\n",
       "      <td>def truncate(value: Decimal, n_digits: int) -&gt;...</td>\n",
       "      <td>This function truncates a decimal value to a m...</td>\n",
       "      <td>0.424926</td>\n",
       "      <td>llama2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>get window title in python</td>\n",
       "      <td>def title(self):\\n        \"\"\" The title of thi...</td>\n",
       "      <td>The doc string explains that the title of a wi...</td>\n",
       "      <td>-0.042896</td>\n",
       "      <td>llama3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>if matches a set of strings python</td>\n",
       "      <td>def any_contains_any(strings, candidates):\\n  ...</td>\n",
       "      <td>The docstring defines a function that checks i...</td>\n",
       "      <td>-0.023504</td>\n",
       "      <td>llama3-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>python detect type of namedtuple</td>\n",
       "      <td>def isnamedtuple(obj):\\n    \"\"\"Heuristic check...</td>\n",
       "      <td>The code defines a function `isnamedtuple` tha...</td>\n",
       "      <td>0.030626</td>\n",
       "      <td>gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>how to define a empty data frame in python</td>\n",
       "      <td>def add_blank_row(self, label):\\n        \"\"\"\\n...</td>\n",
       "      <td>This code snippet defines a function called `a...</td>\n",
       "      <td>0.034638</td>\n",
       "      <td>llama3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>zstring pad zero python</td>\n",
       "      <td>def zfill(x, width):\\n    \"\"\"zfill(x, width) -...</td>\n",
       "      <td>The above code and doc explain how the zfill()...</td>\n",
       "      <td>0.309740</td>\n",
       "      <td>llama2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   doc  \\\n",
       "0              python split strings into list of lines   \n",
       "1    loading a series of images in python and resiz...   \n",
       "2                          python save graph into file   \n",
       "3                            add color to print python   \n",
       "4                  python limit number to two decimals   \n",
       "..                                                 ...   \n",
       "308                         get window title in python   \n",
       "309                 if matches a set of strings python   \n",
       "310                   python detect type of namedtuple   \n",
       "311         how to define a empty data frame in python   \n",
       "312                            zstring pad zero python   \n",
       "\n",
       "                                                  code  \\\n",
       "0    def split_multiline(value):\\n    \"\"\"Split a mu...   \n",
       "1    def load_preprocess_images(image_paths: List[s...   \n",
       "2    def to_dotfile(G: nx.DiGraph, filename: str):\\...   \n",
       "3    def write_color(string, name, style='normal', ...   \n",
       "4    def truncate(value: Decimal, n_digits: int) ->...   \n",
       "..                                                 ...   \n",
       "308  def title(self):\\n        \"\"\" The title of thi...   \n",
       "309  def any_contains_any(strings, candidates):\\n  ...   \n",
       "310  def isnamedtuple(obj):\\n    \"\"\"Heuristic check...   \n",
       "311  def add_blank_row(self, label):\\n        \"\"\"\\n...   \n",
       "312  def zfill(x, width):\\n    \"\"\"zfill(x, width) -...   \n",
       "\n",
       "                                      best_explanation  max_similarity  \\\n",
       "0    </think>\\n\\nThe function `split_multiline` tak...        0.021005   \n",
       "1    The code loads a list of images specified with...        0.019301   \n",
       "2    The code defines a function called `to_dotfile...       -0.031910   \n",
       "3    The code defines a function called `write_colo...       -0.027941   \n",
       "4    This function truncates a decimal value to a m...        0.424926   \n",
       "..                                                 ...             ...   \n",
       "308  The doc string explains that the title of a wi...       -0.042896   \n",
       "309  The docstring defines a function that checks i...       -0.023504   \n",
       "310  The code defines a function `isnamedtuple` tha...        0.030626   \n",
       "311  This code snippet defines a function called `a...        0.034638   \n",
       "312  The above code and doc explain how the zfill()...        0.309740   \n",
       "\n",
       "          best_model  \n",
       "0           deepseek  \n",
       "1             llama2  \n",
       "2              gemma  \n",
       "3              gemma  \n",
       "4             llama2  \n",
       "..               ...  \n",
       "308           llama3  \n",
       "309  llama3-instruct  \n",
       "310            gemma  \n",
       "311           llama3  \n",
       "312           llama2  \n",
       "\n",
       "[313 rows x 5 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.073342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.111136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.097918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.046716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.127472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.663343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       max_similarity\n",
       "count      313.000000\n",
       "mean         0.073342\n",
       "std          0.111136\n",
       "min         -0.097918\n",
       "25%          0.000571\n",
       "50%          0.046716\n",
       "75%          0.127472\n",
       "max          0.663343"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>code</th>\n",
       "      <th>best_explanation</th>\n",
       "      <th>max_similarity</th>\n",
       "      <th>best_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>python 3 rounding or floats</td>\n",
       "      <td>def py3round(number):\\n    \"\"\"Unified rounding...</td>\n",
       "      <td>The code defines a function `py3round` that un...</td>\n",
       "      <td>0.663343</td>\n",
       "      <td>gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>python round to three significant digits</td>\n",
       "      <td>def round_sig(x, sig):\\n    \"\"\"Round the numbe...</td>\n",
       "      <td>The doc string provides a description of the f...</td>\n",
       "      <td>0.580077</td>\n",
       "      <td>llama3-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>f strings number rounding formatting python</td>\n",
       "      <td>def _saferound(value, decimal_places):\\n    \"\"...</td>\n",
       "      <td>_saferound is a function to round a float valu...</td>\n",
       "      <td>0.525706</td>\n",
       "      <td>llama2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>how to get a random float between 0 and 1 in p...</td>\n",
       "      <td>def money(min=0, max=10):\\n    \"\"\"Return a str...</td>\n",
       "      <td>&lt;/think&gt;\\n\\nTo generate a random float between...</td>\n",
       "      <td>0.467388</td>\n",
       "      <td>deepseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>generates a random decimal number between 1 an...</td>\n",
       "      <td>def money(min=0, max=10):\\n    \"\"\"Return a str...</td>\n",
       "      <td>Doc string: generates a random decimal number ...</td>\n",
       "      <td>0.439706</td>\n",
       "      <td>llama2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>check if a string start with a prefix in python</td>\n",
       "      <td>def starts_with_prefix_in_list(text, prefixes)...</td>\n",
       "      <td>The code checks if a given text starts with an...</td>\n",
       "      <td>-0.071353</td>\n",
       "      <td>deepseek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>checking what linux distibution is being used ...</td>\n",
       "      <td>def is_archlinux():\\n    \"\"\"return True if the...</td>\n",
       "      <td>The above code is used to determine whether th...</td>\n",
       "      <td>-0.084472</td>\n",
       "      <td>llama3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>python function to remove all non english letters</td>\n",
       "      <td>def clean(self, text):\\n        \"\"\"Remove all ...</td>\n",
       "      <td>The docstring explains the purpose of the func...</td>\n",
       "      <td>-0.085364</td>\n",
       "      <td>llama3-instruct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>escaping characters for non printable characte...</td>\n",
       "      <td>def _escape(s):\\n    \"\"\" Helper method that es...</td>\n",
       "      <td>The code defines a helper function _escape tha...</td>\n",
       "      <td>-0.089189</td>\n",
       "      <td>gemma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>how to remove links with certain characters py...</td>\n",
       "      <td>def escape_link(url):\\n    \"\"\"Remove dangerous...</td>\n",
       "      <td>The doc string describes a function, escape_li...</td>\n",
       "      <td>-0.097918</td>\n",
       "      <td>llama2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>313 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   doc  \\\n",
       "164                        python 3 rounding or floats   \n",
       "86            python round to three significant digits   \n",
       "267        f strings number rounding formatting python   \n",
       "254  how to get a random float between 0 and 1 in p...   \n",
       "117  generates a random decimal number between 1 an...   \n",
       "..                                                 ...   \n",
       "148    check if a string start with a prefix in python   \n",
       "227  checking what linux distibution is being used ...   \n",
       "135  python function to remove all non english letters   \n",
       "301  escaping characters for non printable characte...   \n",
       "130  how to remove links with certain characters py...   \n",
       "\n",
       "                                                  code  \\\n",
       "164  def py3round(number):\\n    \"\"\"Unified rounding...   \n",
       "86   def round_sig(x, sig):\\n    \"\"\"Round the numbe...   \n",
       "267  def _saferound(value, decimal_places):\\n    \"\"...   \n",
       "254  def money(min=0, max=10):\\n    \"\"\"Return a str...   \n",
       "117  def money(min=0, max=10):\\n    \"\"\"Return a str...   \n",
       "..                                                 ...   \n",
       "148  def starts_with_prefix_in_list(text, prefixes)...   \n",
       "227  def is_archlinux():\\n    \"\"\"return True if the...   \n",
       "135  def clean(self, text):\\n        \"\"\"Remove all ...   \n",
       "301  def _escape(s):\\n    \"\"\" Helper method that es...   \n",
       "130  def escape_link(url):\\n    \"\"\"Remove dangerous...   \n",
       "\n",
       "                                      best_explanation  max_similarity  \\\n",
       "164  The code defines a function `py3round` that un...        0.663343   \n",
       "86   The doc string provides a description of the f...        0.580077   \n",
       "267  _saferound is a function to round a float valu...        0.525706   \n",
       "254  </think>\\n\\nTo generate a random float between...        0.467388   \n",
       "117  Doc string: generates a random decimal number ...        0.439706   \n",
       "..                                                 ...             ...   \n",
       "148  The code checks if a given text starts with an...       -0.071353   \n",
       "227  The above code is used to determine whether th...       -0.084472   \n",
       "135  The docstring explains the purpose of the func...       -0.085364   \n",
       "301  The code defines a helper function _escape tha...       -0.089189   \n",
       "130  The doc string describes a function, escape_li...       -0.097918   \n",
       "\n",
       "          best_model  \n",
       "164            gemma  \n",
       "86   llama3-instruct  \n",
       "267           llama2  \n",
       "254         deepseek  \n",
       "117           llama2  \n",
       "..               ...  \n",
       "148         deepseek  \n",
       "227           llama3  \n",
       "135  llama3-instruct  \n",
       "301            gemma  \n",
       "130           llama2  \n",
       "\n",
       "[313 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.sort_values(by=['max_similarity'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
