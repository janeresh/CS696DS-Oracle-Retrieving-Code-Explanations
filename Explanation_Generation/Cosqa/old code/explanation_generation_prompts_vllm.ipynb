{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-18 13:59:19,368\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams, LLMEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "num_gpus = torch.cuda.device_count()\n",
    "print(num_gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/explanation_generation_prompts_vllm.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgypsum-gpu182.unity.rc.umass.edu/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/explanation_generation_prompts_vllm.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdatasets\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgypsum-gpu182.unity.rc.umass.edu/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/explanation_generation_prompts_vllm.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load the dataset\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgypsum-gpu182.unity.rc.umass.edu/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/explanation_generation_prompts_vllm.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m dataset \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39m\u001b[39mCoIR-Retrieval/cosqa-queries-corpus\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"CoIR-Retrieval/cosqa-queries-corpus\")\n",
    "print(dataset.keys())\n",
    "new_dataset=pd.DataFrame(dataset['corpus']['text'])\n",
    "new_dataset_queries=pd.DataFrame(dataset['queries']['text'])\n",
    "print(new_dataset_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_code_df = pd.DataFrame({\n",
    "    \"doc\": new_dataset_queries.iloc[:, 0],\n",
    "    \"code\": new_dataset.iloc[:, 0]\n",
    "})\n",
    "\n",
    "doc_code_df.head(5)\n",
    "doc_code_df.to_csv(\"cosqa-queries-corpus.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variable to help with memory fragmentation.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "\n",
    "def clean_output(text,keyword):\n",
    "    # keyword = \"Answer:\"\n",
    "    index = text.rfind(keyword)  # Find the last occurrence of \"Answer:\"\n",
    "    if index != -1:\n",
    "        return text[index + len(keyword):].strip()  \n",
    "    return text\n",
    "\n",
    "\n",
    "class ExplanationGeneratorLama:\n",
    "    def __init__(self, model_name, max_new_tokens=500):\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # self.tokenizer.padding_side = \"left\"\n",
    "        \n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=max_new_tokens\n",
    "        )\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            dtype=\"half\"  # using half precision for GPU\n",
    "        )\n",
    "        # self.engine = LLMEngine(model_name, sampling_params=self.sampling_params)\n",
    "\n",
    "    def generate_explanations_batch(self, entries, max_new_tokens=500):\n",
    "        # Create prompts by combining each entry with each prompt template.\n",
    "        \n",
    "        prompts = []\n",
    "        \n",
    "        for entry in entries:\n",
    "            prompt_templates = [\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Provide a concise explanation of what the above doc and code mean. \"\n",
    "                \"Generate strictly less than 100 words in total. Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "                , \n",
    "\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Provide a detailed line-by-line explanation of this code snippet, describing the purpose and functionality of each statement, function, and control structure. \"\n",
    "                \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "                ,\n",
    "\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Summarize what this code snippet does in simple, non-technical language, focusing on its overall purpose and key operations for someone with little programming experience. \"\n",
    "                \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "                ,\n",
    "\n",
    "                f\"Doc string: {entry['doc']}\\n\"\n",
    "                f\"Code snippet: {entry['code']}\\n\"\n",
    "                \"Instruction: Generate an explanation of the code snippet in such a way that it can regenerate the code based on this explanation. \"\n",
    "                \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "                \"Answer: \\n\"\n",
    "            ]\n",
    "            \n",
    "            for template in prompt_templates:\n",
    "                prompt = (\n",
    "                    f\"Doc string: {entry['doc']}\\n\"\n",
    "                    f\"Code snippet: {entry['code']}\\n\"\n",
    "                    f\"{template}\\n\"\n",
    "                    \"Answer: \\n\"\n",
    "                )\n",
    "                prompts.append(prompt)\n",
    "                \n",
    "        # results = self.engine.generate(prompts)\n",
    "        results = self.llm.generate(prompts, self.sampling_params)\n",
    "        explanations = [result.outputs[0].text for result in results]\n",
    "        \n",
    "        # Regroup explanations by entry.\n",
    "        n_prompts = len(prompt_templates)\n",
    "        grouped_explanations = []\n",
    "        for i in range(0, len(explanations), n_prompts):\n",
    "            grouped_explanations.append(explanations[i:i+n_prompts])\n",
    "            \n",
    "        return grouped_explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded from CSV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model granite...\n",
      "INFO 03-18 13:59:23 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-18 13:59:23 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-18 13:59:45 config.py:549] This model supports multiple tasks: {'reward', 'classify', 'embed', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 03-18 13:59:45 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f', speculative_config=None, tokenizer='/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-18 13:59:46 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-18 13:59:46 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-18 13:59:48 model_runner.py:1110] Starting to load model /datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W318 13:59:48.843524685 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:14<00:14, 14.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:14<00:00,  7.27s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-18 14:00:04 model_runner.py:1115] Loading model weights took 4.7199 GB\n",
      "INFO 03-18 14:00:06 worker.py:267] Memory profiling takes 1.71 seconds\n",
      "INFO 03-18 14:00:06 worker.py:267] the current vLLM instance can use total_gpu_memory (10.75GiB) x gpu_memory_utilization (0.90) = 9.67GiB\n",
      "INFO 03-18 14:00:06 worker.py:267] model weights take 4.72GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.47GiB; the rest of the memory reserved for KV Cache is 4.43GiB.\n",
      "INFO 03-18 14:00:06 executor_base.py:111] # cuda blocks: 3627, # CPU blocks: 3276\n",
      "INFO 03-18 14:00:06 executor_base.py:116] Maximum concurrency for 4096 tokens per request: 14.17x\n",
      "INFO 03-18 14:00:10 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:21<00:00,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-18 14:00:31 model_runner.py:1562] Graph capturing finished in 21 secs, took 0.28 GiB\n",
      "INFO 03-18 14:00:31 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 27.82 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.14it/s, est. speed input: 479.03 toks/s, output: 440.98 toks/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.02it/s, est. speed input: 603.91 toks/s, output: 574.04 toks/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.15it/s, est. speed input: 408.36 toks/s, output: 518.09 toks/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.27it/s, est. speed input: 529.98 toks/s, output: 522.17 toks/s]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Processed prompts: 100%|██████████| 16/16 [00:07<00:00,  2.27it/s, est. speed input: 514.07 toks/s, output: 611.83 toks/s]\n",
      "Processing batches: 100%|██████████| 5/5 [00:38<00:00,  7.77s/it]\n",
      "Processing models: 100%|██████████| 1/1 [01:48<00:00, 108.04s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch, gc\n",
    "from tqdm import tqdm\n",
    "# Import or define your ExplanationGeneratorLama class here\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/cosqa_queries_code_corpus.csv\"  # change this to your CSV input path\n",
    "    output_csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CoSQA_granite_explanations_vllm.csv\"\n",
    "    \n",
    "    # Load the CSV file into a DataFrame\n",
    "    df = pd.read_csv(csv_path).head(20)\n",
    "    print(\"Data loaded from CSV\")\n",
    "    \n",
    "\n",
    "    models_dict = {\n",
    "        # \"deepseek\": \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "        'granite': \"/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f\"\n",
    "\n",
    "    }    \n",
    "    \n",
    "    batch_size = 4\n",
    "    \n",
    "    for model_key, model_path in tqdm(models_dict.items(), desc=\"Processing models\"):\n",
    "        print(f\"\\nProcessing model {model_key}...\")\n",
    "        generator = ExplanationGeneratorLama(model_path)\n",
    "        if hasattr(generator, 'model'):\n",
    "            generator.model.eval()\n",
    "        \n",
    "        # Process the DataFrame in batches\n",
    "        for i in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "            # Create a batch of entries (each is a dict with \"doc\" and \"code\")\n",
    "            batch_entries = df.iloc[i:i+batch_size][[\"corpus_id\", \"query_id\", \"doc\", \"code\"]].to_dict(\"records\")\n",
    "            \n",
    "            # Wrap inference in a no_grad context to prevent gradient computations.\n",
    "            with torch.no_grad():\n",
    "                batch_explanations = generator.generate_explanations_batch(batch_entries)\n",
    "            \n",
    "            for j, explanation_variants in enumerate(batch_explanations):\n",
    "                for idx, raw_text in enumerate(explanation_variants):\n",
    "                    # print(raw_text)\n",
    "                    cleaned_text = clean_output(raw_text,\"Answer:\")\n",
    "                    cleaned_text = clean_output(cleaned_text,\"</think>\")\n",
    "                    df.loc[i+j, f'explanation_{model_key}_{idx+1}'] = cleaned_text\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "        \n",
    "        del generator\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Save the DataFrame with the new explanation columns to a CSV file\n",
    "    # df.to_csv(output_csv_path, index=False)\n",
    "    # print(f\"\\nExplanations from all models have been saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>code</th>\n",
       "      <th>explanation_granite_1</th>\n",
       "      <th>explanation_granite_2</th>\n",
       "      <th>explanation_granite_3</th>\n",
       "      <th>explanation_granite_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>python code to write bool value 1</td>\n",
       "      <td>d1</td>\n",
       "      <td>def writeBoolean(self, n):\\n        \"\"\"\\n     ...</td>\n",
       "      <td>\\nThe code snippet is a method named `writeBoo...</td>\n",
       "      <td>\\n```python\\ndef writeBoolean(self, n):\\n    \"...</td>\n",
       "      <td>\\nThis code snippet is a part of a class that ...</td>\n",
       "      <td>\\nThis Python code snippet defines a method ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q2</td>\n",
       "      <td>\"python how to manipulate clipboard\"</td>\n",
       "      <td>d2</td>\n",
       "      <td>def paste(xsel=False):\\n    \"\"\"Returns system ...</td>\n",
       "      <td>\\nThe provided Python code snippet defines a f...</td>\n",
       "      <td>\\n1. `def paste(xsel=False):` - This line defi...</td>\n",
       "      <td>\\nThis code snippet is a function in Python th...</td>\n",
       "      <td>\\nThis Python code snippet defines a function ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>python colored output to html</td>\n",
       "      <td>d3</td>\n",
       "      <td>def _format_json(data, theme):\\n    \"\"\"Pretty ...</td>\n",
       "      <td>\\nThe provided Python code defines a function ...</td>\n",
       "      <td>\\n1. `def _format_json(data, theme):` - This l...</td>\n",
       "      <td>\\nThis code snippet, named `_format_json`, is ...</td>\n",
       "      <td>\\nThis code snippet defines a function called ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>q4</td>\n",
       "      <td>python \"create directory\" using \"relative path\"</td>\n",
       "      <td>d4</td>\n",
       "      <td>def create_path(path):\\n    \"\"\"Creates a absol...</td>\n",
       "      <td>\\nThe provided code defines a function called ...</td>\n",
       "      <td>\\nThis code snippet defines a function called ...</td>\n",
       "      <td>\\nThis code snippet helps create a new directo...</td>\n",
       "      <td>\\nThe provided code snippet defines a function...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q5</td>\n",
       "      <td>python column of an array</td>\n",
       "      <td>d5</td>\n",
       "      <td>def _vector_or_scalar(x, type='row'):\\n    \"\"\"...</td>\n",
       "      <td>\\nThe provided Python code defines a function ...</td>\n",
       "      <td>\\n1. `def _vector_or_scalar(x, type='row')`: T...</td>\n",
       "      <td>\\nThis code snippet helps convert a list or a ...</td>\n",
       "      <td>\\nThe code snippet provided is a Python functi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                              doc corpus_id  \\\n",
       "0       q1                python code to write bool value 1        d1   \n",
       "1       q2             \"python how to manipulate clipboard\"        d2   \n",
       "2       q3                    python colored output to html        d3   \n",
       "3       q4  python \"create directory\" using \"relative path\"        d4   \n",
       "4       q5                        python column of an array        d5   \n",
       "\n",
       "                                                code  \\\n",
       "0  def writeBoolean(self, n):\\n        \"\"\"\\n     ...   \n",
       "1  def paste(xsel=False):\\n    \"\"\"Returns system ...   \n",
       "2  def _format_json(data, theme):\\n    \"\"\"Pretty ...   \n",
       "3  def create_path(path):\\n    \"\"\"Creates a absol...   \n",
       "4  def _vector_or_scalar(x, type='row'):\\n    \"\"\"...   \n",
       "\n",
       "                               explanation_granite_1  \\\n",
       "0  \\nThe code snippet is a method named `writeBoo...   \n",
       "1  \\nThe provided Python code snippet defines a f...   \n",
       "2  \\nThe provided Python code defines a function ...   \n",
       "3  \\nThe provided code defines a function called ...   \n",
       "4  \\nThe provided Python code defines a function ...   \n",
       "\n",
       "                               explanation_granite_2  \\\n",
       "0  \\n```python\\ndef writeBoolean(self, n):\\n    \"...   \n",
       "1  \\n1. `def paste(xsel=False):` - This line defi...   \n",
       "2  \\n1. `def _format_json(data, theme):` - This l...   \n",
       "3  \\nThis code snippet defines a function called ...   \n",
       "4  \\n1. `def _vector_or_scalar(x, type='row')`: T...   \n",
       "\n",
       "                               explanation_granite_3  \\\n",
       "0  \\nThis code snippet is a part of a class that ...   \n",
       "1  \\nThis code snippet is a function in Python th...   \n",
       "2  \\nThis code snippet, named `_format_json`, is ...   \n",
       "3  \\nThis code snippet helps create a new directo...   \n",
       "4  \\nThis code snippet helps convert a list or a ...   \n",
       "\n",
       "                               explanation_granite_4  \n",
       "0  \\nThis Python code snippet defines a method ca...  \n",
       "1  \\nThis Python code snippet defines a function ...  \n",
       "2  \\nThis code snippet defines a function called ...  \n",
       "3  \\nThe provided code snippet defines a function...  \n",
       "4  \\nThe code snippet provided is a Python functi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pmangina_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-17 14:30:44,152\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:30:45 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 03-17 14:30:45 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 03-17 14:31:00 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "WARNING 03-17 14:31:00 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 03-17 14:31:00 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 03-17 14:31:00 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', speculative_config=None, tokenizer='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 03-17 14:31:02 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 03-17 14:31:02 cuda.py:226] Using XFormers backend.\n",
      "INFO 03-17 14:31:03 model_runner.py:1110] Starting to load model /datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.10s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:09<00:00,  9.10s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:31:13 model_runner.py:1115] Loading model weights took 3.3460 GB\n",
      "INFO 03-17 14:31:14 worker.py:267] Memory profiling takes 1.27 seconds\n",
      "INFO 03-17 14:31:14 worker.py:267] the current vLLM instance can use total_gpu_memory (47.45GiB) x gpu_memory_utilization (0.90) = 42.70GiB\n",
      "INFO 03-17 14:31:14 worker.py:267] model weights take 3.35GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 37.91GiB.\n",
      "INFO 03-17 14:31:14 executor_base.py:111] # cuda blocks: 88728, # CPU blocks: 9362\n",
      "INFO 03-17 14:31:14 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 10.83x\n",
      "INFO 03-17 14:31:21 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:19<00:00,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 03-17 14:31:40 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.20 GiB\n",
      "INFO 03-17 14:31:40 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 27.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:14<00:00, 14.87s/it, est. speed input: 0.34 toks/s, output: 67.24 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RequestOutput(request_id=0, prompt='What is AI?', prompt_token_ids=[151646, 3838, 374, 15235, 30], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Can you explain it in simple terms?\\n\\nWhat is the difference between AI and machine learning?\\n\\nWhat is the difference between AI and deep learning?\\n\\nWhat is the difference between AI and NLP?\\n\\nWhat is the difference between AI and NLP? Wait, that\\'s the same as the previous question.\\n\\nWhat is the difference between AI and NLP?\\n\\nWait, that\\'s the same as the previous question.\\n\\nWait, I think I need to clarify this.\\n\\nWait, no, the user is asking for a list of differences between AI and NLP, but in the initial question, they are asking for the same thing twice.\\n\\nSo perhaps the user is confused, and wants to know the differences between AI and NLP, and between AI and machine learning, and between AI and deep learning.\\n\\nSo perhaps I should explain the differences between AI and NLP, and AI vs machine learning, and AI vs deep learning.\\n\\nWait, the user wrote: \"What is the difference between AI and NLP? Wait, that\\'s the same as the previous question.\" So maybe the user wants to know the differences between AI and NLP, and also AI vs ML and AI vs DL.\\n\\nSo perhaps the user is confused, and wants to clarify.\\n\\nSo perhaps I should answer the initial question, and then explain the differences between AI and NLP, and between AI and ML, and AI and DL.\\n\\nAlternatively, perhaps the user wants a list of differences between AI and NLP, but it\\'s the same question twice, so perhaps they want to know the differences between AI and NLP, and also AI vs ML, and AI vs DL.\\n\\nSo perhaps I should answer the initial question, and then explain the differences between AI and NLP, and AI vs ML, and AI vs DL.\\n\\nAlternatively, perhaps the user is confused and wants to know the differences between AI and NLP, and also AI vs ML and AI vs DL.\\n\\nSo perhaps I should answer the initial question, and then explain the differences between AI and NLP, and between AI and ML, and between AI and DL.\\n\\nBut perhaps the user wants to clarify the differences between AI and NLP, and also the differences between AI and ML, and AI and DL.\\n\\nSo perhaps I should structure the answer to cover all three differences.\\n\\nAlternatively, perhaps the user is trying to get a comprehensive list of differences between AI and NLP, but in the initial question, they only asked for the same thing twice, so perhaps I should answer the initial question, and then provide explanations for the differences between AI and NLP, AI vs ML, and AI vs DL.\\n\\nSo perhaps I should first answer the initial question, and then provide explanations for the other differences.\\n\\nAlternatively, perhaps the user wants to know all the differences between AI and NLP, and also between AI and ML and AI and DL, so perhaps I should explain all three.\\n\\nAlternatively, perhaps the user is confused and wants to know the differences between AI and NLP, and also the differences between AI and ML and AI and DL.\\n\\nSo perhaps I should explain the differences between AI and NLP, and between AI and ML, and between AI and DL.\\n\\nSo perhaps I can structure the answer as follows:\\n\\n1. Explain what AI is, in simple terms.\\n\\n2. Answer the initial question about the difference between AI and NLP.\\n\\n3. Explain the difference between AI and machine learning.\\n\\n4. Explain the difference between AI and deep learning.\\n\\n5. Explain the difference between AI and NLP again.\\n\\nWait, but the user wrote: \"What is the difference between AI and NLP? Wait, that\\'s the same as the previous question.\" So perhaps the user wants to know the difference between AI and NLP, and also the difference between AI and machine learning, and the difference between AI and deep learning.\\n\\nSo perhaps I should answer the initial question, and then explain the differences between AI and NLP, AI vs ML, and AI vs DL.\\n\\nAlternatively, perhaps the user wants to know all three differences: AI vs NLP, AI vs ML, and AI vs DL.\\n\\nSo perhaps I should explain all three.\\n\\nAlternatively, perhaps the user is just trying to get a clear understanding of the differences between AI and NLP, and also the differences between AI and ML and AI and DL.\\n\\nSo perhaps I should answer the initial question, and then explain the differences between AI and NLP, AI vs ML, and AI vs DL.\\n\\nAlternatively, perhaps the user is just asking for a list of differences between AI and NLP, but in the initial question, they are asking for the same thing twice, so perhaps they just need to know the difference between AI and NLP, and also the differences between AI and ML and AI and DL.\\n\\nSo perhaps I should explain all three differences.\\n\\nAlternatively, perhaps the user is just confused and wants to know the differences between AI and NLP, and also the differences between AI and ML and AI and', token_ids=(2980, 498, 10339, 432, 304, 4285, 3793, 1939, 3838, 374, 279, 6672, 1948, 15235, 323, 5662, 6832, 1939, 3838, 374, 279, 6672, 1948, 15235, 323, 5538, 6832, 1939, 3838, 374, 279, 6672, 1948, 15235, 323, 451, 12567, 1939, 3838, 374, 279, 6672, 1948, 15235, 323, 451, 12567, 30, 13824, 11, 429, 594, 279, 1852, 438, 279, 3681, 3405, 382, 3838, 374, 279, 6672, 1948, 15235, 323, 451, 12567, 1939, 14190, 11, 429, 594, 279, 1852, 438, 279, 3681, 3405, 382, 14190, 11, 358, 1744, 358, 1184, 311, 37163, 419, 382, 14190, 11, 902, 11, 279, 1196, 374, 10161, 369, 264, 1140, 315, 11799, 1948, 15235, 323, 451, 12567, 11, 714, 304, 279, 2856, 3405, 11, 807, 525, 10161, 369, 279, 1852, 3166, 10917, 382, 4416, 8365, 279, 1196, 374, 21815, 11, 323, 6801, 311, 1414, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1948, 15235, 323, 5662, 6832, 11, 323, 1948, 15235, 323, 5538, 6832, 382, 4416, 8365, 358, 1265, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 15235, 6165, 5662, 6832, 11, 323, 15235, 6165, 5538, 6832, 382, 14190, 11, 279, 1196, 6139, 25, 330, 3838, 374, 279, 6672, 1948, 15235, 323, 451, 12567, 30, 13824, 11, 429, 594, 279, 1852, 438, 279, 3681, 3405, 1189, 2055, 7196, 279, 1196, 6801, 311, 1414, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 15235, 6165, 19614, 323, 15235, 6165, 32975, 382, 4416, 8365, 279, 1196, 374, 21815, 11, 323, 6801, 311, 37163, 382, 4416, 8365, 358, 1265, 4226, 279, 2856, 3405, 11, 323, 1221, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1948, 15235, 323, 19614, 11, 323, 15235, 323, 32975, 382, 92014, 11, 8365, 279, 1196, 6801, 264, 1140, 315, 11799, 1948, 15235, 323, 451, 12567, 11, 714, 432, 594, 279, 1852, 3405, 10917, 11, 773, 8365, 807, 1366, 311, 1414, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 15235, 6165, 19614, 11, 323, 15235, 6165, 32975, 382, 4416, 8365, 358, 1265, 4226, 279, 2856, 3405, 11, 323, 1221, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 15235, 6165, 19614, 11, 323, 15235, 6165, 32975, 382, 92014, 11, 8365, 279, 1196, 374, 21815, 323, 6801, 311, 1414, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 15235, 6165, 19614, 323, 15235, 6165, 32975, 382, 4416, 8365, 358, 1265, 4226, 279, 2856, 3405, 11, 323, 1221, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1948, 15235, 323, 19614, 11, 323, 1948, 15235, 323, 32975, 382, 3983, 8365, 279, 1196, 6801, 311, 37163, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 279, 11799, 1948, 15235, 323, 19614, 11, 323, 15235, 323, 32975, 382, 4416, 8365, 358, 1265, 5944, 279, 4226, 311, 3421, 678, 2326, 11799, 382, 92014, 11, 8365, 279, 1196, 374, 4460, 311, 633, 264, 15817, 1140, 315, 11799, 1948, 15235, 323, 451, 12567, 11, 714, 304, 279, 2856, 3405, 11, 807, 1172, 4588, 369, 279, 1852, 3166, 10917, 11, 773, 8365, 358, 1265, 4226, 279, 2856, 3405, 11, 323, 1221, 3410, 40841, 369, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 15235, 6165, 19614, 11, 323, 15235, 6165, 32975, 382, 4416, 8365, 358, 1265, 1156, 4226, 279, 2856, 3405, 11, 323, 1221, 3410, 40841, 369, 279, 1008, 11799, 382, 92014, 11, 8365, 279, 1196, 6801, 311, 1414, 678, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 1948, 15235, 323, 19614, 323, 15235, 323, 32975, 11, 773, 8365, 358, 1265, 10339, 678, 2326, 382, 92014, 11, 8365, 279, 1196, 374, 21815, 323, 6801, 311, 1414, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 279, 11799, 1948, 15235, 323, 19614, 323, 15235, 323, 32975, 382, 4416, 8365, 358, 1265, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1948, 15235, 323, 19614, 11, 323, 1948, 15235, 323, 32975, 382, 4416, 8365, 358, 646, 5944, 279, 4226, 438, 11017, 1447, 16, 13, 81917, 1128, 15235, 374, 11, 304, 4285, 3793, 382, 17, 13, 21806, 279, 2856, 3405, 911, 279, 6672, 1948, 15235, 323, 451, 12567, 382, 18, 13, 81917, 279, 6672, 1948, 15235, 323, 5662, 6832, 382, 19, 13, 81917, 279, 6672, 1948, 15235, 323, 5538, 6832, 382, 20, 13, 81917, 279, 6672, 1948, 15235, 323, 451, 12567, 1549, 382, 14190, 11, 714, 279, 1196, 6139, 25, 330, 3838, 374, 279, 6672, 1948, 15235, 323, 451, 12567, 30, 13824, 11, 429, 594, 279, 1852, 438, 279, 3681, 3405, 1189, 2055, 8365, 279, 1196, 6801, 311, 1414, 279, 6672, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 279, 6672, 1948, 15235, 323, 5662, 6832, 11, 323, 279, 6672, 1948, 15235, 323, 5538, 6832, 382, 4416, 8365, 358, 1265, 4226, 279, 2856, 3405, 11, 323, 1221, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 15235, 6165, 19614, 11, 323, 15235, 6165, 32975, 382, 92014, 11, 8365, 279, 1196, 6801, 311, 1414, 678, 2326, 11799, 25, 15235, 6165, 451, 12567, 11, 15235, 6165, 19614, 11, 323, 15235, 6165, 32975, 382, 4416, 8365, 358, 1265, 10339, 678, 2326, 382, 92014, 11, 8365, 279, 1196, 374, 1101, 4460, 311, 633, 264, 2797, 8660, 315, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 279, 11799, 1948, 15235, 323, 19614, 323, 15235, 323, 32975, 382, 4416, 8365, 358, 1265, 4226, 279, 2856, 3405, 11, 323, 1221, 10339, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 15235, 6165, 19614, 11, 323, 15235, 6165, 32975, 382, 92014, 11, 8365, 279, 1196, 374, 1101, 10161, 369, 264, 1140, 315, 11799, 1948, 15235, 323, 451, 12567, 11, 714, 304, 279, 2856, 3405, 11, 807, 525, 10161, 369, 279, 1852, 3166, 10917, 11, 773, 8365, 807, 1101, 1184, 311, 1414, 279, 6672, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 279, 11799, 1948, 15235, 323, 19614, 323, 15235, 323, 32975, 382, 4416, 8365, 358, 1265, 10339, 678, 2326, 11799, 382, 92014, 11, 8365, 279, 1196, 374, 1101, 21815, 323, 6801, 311, 1414, 279, 11799, 1948, 15235, 323, 451, 12567, 11, 323, 1083, 279, 11799, 1948, 15235, 323, 19614, 323, 15235, 323), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1742221903.087968, last_token_time=1742221917.9579773, first_scheduled_time=1742221903.0940526, first_token_time=1742221910.126209, time_in_queue=0.006084442138671875, finished_time=1742221917.9584186, scheduler_time=0.1147465892136097, model_forward_time=None, model_execute_time=None), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "    dtype=\"half\"  # using half precision for GPU\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=1000)\n",
    "\n",
    "outputs = llm.generate([\"What is AI?\"], sampling_params)\n",
    "\n",
    "print(outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TORCH_USE_CUDA_DSA=1  # Optional: for better error messages during debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
