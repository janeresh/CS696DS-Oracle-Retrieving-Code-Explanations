{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CS696DS-Oracle-Retrieving-Code-Explanations/Explanation_Generation/Cosqa/raw_data/cosqa_queries_code_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20604, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-20 18:04:30,455\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-20 18:04:32 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-20 18:04:32 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-20 18:04:46 config.py:549] This model supports multiple tasks: {'embed', 'score', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 04-20 18:04:46 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-20 18:04:46 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-20 18:04:46 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='//datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6', speculative_config=None, tokenizer='//datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=//datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-20 18:04:48 cuda.py:178] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 04-20 18:04:48 cuda.py:226] Using XFormers backend.\n",
      "INFO 04-20 18:04:49 model_runner.py:1110] Starting to load model //datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:03<00:00,  3.05s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-20 18:04:53 model_runner.py:1115] Loading model weights took 2.3185 GB\n",
      "INFO 04-20 18:04:54 worker.py:267] Memory profiling takes 0.73 seconds\n",
      "INFO 04-20 18:04:54 worker.py:267] the current vLLM instance can use total_gpu_memory (47.27GiB) x gpu_memory_utilization (0.90) = 42.54GiB\n",
      "INFO 04-20 18:04:54 worker.py:267] model weights take 2.32GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 38.99GiB.\n",
      "INFO 04-20 18:04:54 executor_base.py:111] # cuda blocks: 79845, # CPU blocks: 8192\n",
      "INFO 04-20 18:04:54 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 9.75x\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 47.27 GiB of which 433.19 MiB is free. Process 94173 has 41.83 GiB memory in use. Including non-PyTorch memory, this process has 5.01 GiB memory in use. Of the allocated memory 4.76 GiB is allocated by PyTorch, and 29.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df1 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2. Initialize vLLM client once\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m//datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhalf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 3. Define deterministic sampling\u001b[39;00m\n\u001b[1;32m     11\u001b[0m params \u001b[38;5;241m=\u001b[39m SamplingParams(\n\u001b[1;32m     12\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     13\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m     14\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     15\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py:1022\u001b[0m, in \u001b[0;36mdeprecate_args.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1017\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1018\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1019\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m         )\n\u001b[0;32m-> 1022\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/entrypoints/llm.py:242\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, hf_overrides, mm_processor_kwargs, task, override_pooler_config, compilation_config, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Logic to switch between engines is done at runtime instead of import\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# to avoid import order issues\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_engine_class()\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/llm_engine.py:489\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    487\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/llm_engine.py:276\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, mm_registry, use_cached_outputs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m executor_class(vllm_config\u001b[38;5;241m=\u001b[39mvllm_config, )\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mrunner_type \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpooling\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_kv_caches\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_usage_stats_enabled():\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/llm_engine.py:434\u001b[0m, in \u001b[0;36mLLMEngine._initialize_kv_caches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 434\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    436\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit engine (profile, create kv cache, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarmup model) took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m), elapsed)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/executor/executor_base.py:122\u001b[0m, in \u001b[0;36mExecutorBase.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;241m=\u001b[39m num_gpu_blocks\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_cpu_blocks \u001b[38;5;241m=\u001b[39m num_cpu_blocks\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minitialize_cache\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m                    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cpu_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py:56\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 56\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py:2196\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker.py:306\u001b[0m, in \u001b[0;36mWorker.initialize_cache\u001b[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001b[0m\n\u001b[1;32m    304\u001b[0m     context \u001b[38;5;241m=\u001b[39m nullcontext()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_cache_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warm_up_model()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker.py:311\u001b[0m, in \u001b[0;36mWorker._init_cache_engine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    312\u001b[0m         CacheEngine(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config,\n\u001b[1;32m    313\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_config)\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    315\u001b[0m     ]\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine[ve]\u001b[38;5;241m.\u001b[39mgpu_cache\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[1;32m    320\u001b[0m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompilation_config\u001b[38;5;241m.\u001b[39mstatic_forward_context,\n\u001b[1;32m    321\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker.py:312\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_init_cache_engine\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_config\u001b[38;5;241m.\u001b[39mnum_gpu_blocks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 312\u001b[0m         \u001b[43mCacheEngine\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    315\u001b[0m     ]\n\u001b[1;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_engine[ve]\u001b[38;5;241m.\u001b[39mgpu_cache\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m ve \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel_config\u001b[38;5;241m.\u001b[39mpipeline_parallel_size)\n\u001b[1;32m    319\u001b[0m     ]\n\u001b[1;32m    320\u001b[0m     bind_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompilation_config\u001b[38;5;241m.\u001b[39mstatic_forward_context,\n\u001b[1;32m    321\u001b[0m                   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/cache_engine.py:69\u001b[0m, in \u001b[0;36mCacheEngine.__init__\u001b[0;34m(self, cache_config, model_config, parallel_config, device_config)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_backend \u001b[38;5;241m=\u001b[39m get_attn_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size,\n\u001b[1;32m     62\u001b[0m                                      model_config\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     63\u001b[0m                                      cache_config\u001b[38;5;241m.\u001b[39mcache_dtype,\n\u001b[1;32m     64\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock_size,\n\u001b[1;32m     65\u001b[0m                                      model_config\u001b[38;5;241m.\u001b[39mis_attention_free,\n\u001b[1;32m     66\u001b[0m                                      use_mla\u001b[38;5;241m=\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39muse_mla)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Initialize the cache.\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgpu_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_allocate_kv_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_gpu_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpu_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allocate_kv_cache(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_cpu_blocks, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/cache_engine.py:103\u001b[0m, in \u001b[0;36mCacheEngine._allocate_kv_cache\u001b[0;34m(self, num_blocks, device)\u001b[0m\n\u001b[1;32m     97\u001b[0m     alloc_shape \u001b[38;5;241m=\u001b[39m kv_cache_shape\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_layers):\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# null block in CpuGpuBlockAllocator requires at least that\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# block to be zeroed-out.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# We zero-out everything for simplicity.\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     layer_kv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43malloc_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpin_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpin_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# If we allocated with padding for alignment reasons truncate the\u001b[39;00m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;66;03m# shape while preserving the aligned stride\u001b[39;00m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malign_cache:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.44 GiB. GPU 0 has a total capacity of 47.27 GiB of which 433.19 MiB is free. Process 94173 has 41.83 GiB memory in use. Including non-PyTorch memory, this process has 5.01 GiB memory in use. Of the allocated memory 4.76 GiB is allocated by PyTorch, and 29.71 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from vllm import LLM\n",
    "from vllm.sampling_params import SamplingParams\n",
    "import pandas as pd\n",
    "\n",
    "df1 = df.head(10)\n",
    "\n",
    "# 2. Initialize vLLM client once\n",
    "client = LLM(model=\"/datasets/ai/llama3/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6\", dtype=\"half\")\n",
    "\n",
    "# 3. Define deterministic sampling\n",
    "params = SamplingParams(\n",
    "    max_tokens=2,\n",
    "    temperature=0.0,\n",
    "    top_p=1.0\n",
    "    )\n",
    "\n",
    "# 4. Row-wise function that builds the prompt, calls vLLM, and returns “yes”/“no”\n",
    "def impl_check(row):\n",
    "    prompt = f\"\"\"\n",
    "    Docstring:\n",
    "    {row['doc']}\n",
    "\n",
    "    Code:\n",
    "    {row['code']}\n",
    "\n",
    "    Question: Does the code implement the behavior described in the docstring? Answer “yes” or “no” only.\n",
    "    Answer:\\n\n",
    "    \"\"\"\n",
    "\n",
    "    response = client.generate([{\"prompt\": prompt}], sampling_params=params)\n",
    "    # grab the single-token reply, strip whitespace, force lowercase\n",
    "    return response[0].outputs[0].text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.25s/it, est. speed input: 22.14 toks/s, output: 0.47 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 31.15it/s, est. speed input: 3727.41 toks/s, output: 62.59 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 34.29it/s, est. speed input: 4579.24 toks/s, output: 69.33 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 35.27it/s, est. speed input: 3425.76 toks/s, output: 71.32 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 35.34it/s, est. speed input: 4404.59 toks/s, output: 70.95 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 32.98it/s, est. speed input: 3155.56 toks/s, output: 66.38 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 31.76it/s, est. speed input: 7316.99 toks/s, output: 64.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 33.92it/s, est. speed input: 4865.98 toks/s, output: 68.48 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 35.99it/s, est. speed input: 3491.95 toks/s, output: 72.69 toks/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00, 35.40it/s, est. speed input: 3868.19 toks/s, output: 71.57 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  query_id                                              doc corpus_id  \\\n",
      "0       q1                python code to write bool value 1        d1   \n",
      "1       q2             \"python how to manipulate clipboard\"        d2   \n",
      "2       q3                    python colored output to html        d3   \n",
      "3       q4  python \"create directory\" using \"relative path\"        d4   \n",
      "4       q5                        python column of an array        d5   \n",
      "\n",
      "                                                code validation_flag  \n",
      "0  def writeBoolean(self, n):\\n        \"\"\"\\n     ...             yes  \n",
      "1  def paste(xsel=False):\\n    \"\"\"Returns system ...             yes  \n",
      "2  def _format_json(data, theme):\\n    \"\"\"Pretty ...              no  \n",
      "3  def create_path(path):\\n    \"\"\"Creates a absol...             yes  \n",
      "4  def _vector_or_scalar(x, type='row'):\\n    \"\"\"...              no  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_94173/462374724.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df1['validation_flag'] = df1.apply(impl_check, axis=1)\n"
     ]
    }
   ],
   "source": [
    "# 5. Apply it across your DataFrame\n",
    "df['validation_flag'] = df.apply(impl_check, axis=1)\n",
    "\n",
    "# 6. Now df has a new column 'implements' with “yes” or “no”\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>code</th>\n",
       "      <th>validation_flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>q3</td>\n",
       "      <td>python colored output to html</td>\n",
       "      <td>d3</td>\n",
       "      <td>def _format_json(data, theme):\\n    \"\"\"Pretty print a dict as a JSON, with colors if pygments is present.\"\"\"\\n    output = json.dumps(data, indent=2, sort_keys=True)\\n\\n    if pygments and sys.stdout.isatty():\\n        style = get_style_by_name(theme)\\n        formatter = Terminal256Formatter(style=style)\\n        return pygments.highlight(output, JsonLexer(), formatter)\\n\\n    return output</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>q5</td>\n",
       "      <td>python column of an array</td>\n",
       "      <td>d5</td>\n",
       "      <td>def _vector_or_scalar(x, type='row'):\\n    \"\"\"Convert an object to either a scalar or a row or column vector.\"\"\"\\n    if isinstance(x, (list, tuple)):\\n        x = np.array(x)\\n    if isinstance(x, np.ndarray):\\n        assert x.ndim == 1\\n        if type == 'column':\\n            x = x[:, None]\\n    return x</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>q7</td>\n",
       "      <td>python combine wav file into one as separate channels</td>\n",
       "      <td>d7</td>\n",
       "      <td>def data_from_file(file):\\n    \"\"\"Return (first channel data, sample frequency, sample width) from a .wav\\n    file.\"\"\"\\n    fp = wave.open(file, 'r')\\n    data = fp.readframes(fp.getnframes())\\n    channels = fp.getnchannels()\\n    freq = fp.getframerate()\\n    bits = fp.getsampwidth()\\n\\n    # Unpack bytes -- warning currently only tested with 16 bit wavefiles. 32\\n    # bit not supported.\\n    data = struct.unpack(('%sh' % fp.getnframes()) * channels, data)\\n\\n    # Only use first channel...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q8</td>\n",
       "      <td>+how to use range with a dictionary python</td>\n",
       "      <td>d8</td>\n",
       "      <td>def source_range(start, end, nr_var_dict):\\n    \"\"\"\\n    Given a range of source numbers, as well as a dictionary\\n    containing the numbers of each source, returns a dictionary\\n    containing tuples of the start and end index\\n    for each source variable type.\\n    \"\"\"\\n\\n    return OrderedDict((k, e-s)\\n        for k, (s, e)\\n        in source_range_tuple(start, end, nr_var_dict).iteritems())</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>q9</td>\n",
       "      <td>python compare timespan to number</td>\n",
       "      <td>d9</td>\n",
       "      <td>def timespan(start_time):\\n    \"\"\"Return time in milliseconds from start_time\"\"\"\\n\\n    timespan = datetime.datetime.now() - start_time\\n    timespan_ms = timespan.total_seconds() * 1000\\n    return timespan_ms</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>q10</td>\n",
       "      <td>1d array in char datatype in python</td>\n",
       "      <td>d10</td>\n",
       "      <td>def _convert_to_array(array_like, dtype):\\n        \"\"\"\\n        Convert Matrix attributes which are array-like or buffer to array.\\n        \"\"\"\\n        if isinstance(array_like, bytes):\\n            return np.frombuffer(array_like, dtype=dtype)\\n        return np.asarray(array_like, dtype=dtype)</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  query_id                                                    doc corpus_id  \\\n",
       "2       q3                          python colored output to html        d3   \n",
       "4       q5                              python column of an array        d5   \n",
       "6       q7  python combine wav file into one as separate channels        d7   \n",
       "7       q8             +how to use range with a dictionary python        d8   \n",
       "8       q9                      python compare timespan to number        d9   \n",
       "9      q10                    1d array in char datatype in python       d10   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  code  \\\n",
       "2                                                                                                           def _format_json(data, theme):\\n    \"\"\"Pretty print a dict as a JSON, with colors if pygments is present.\"\"\"\\n    output = json.dumps(data, indent=2, sort_keys=True)\\n\\n    if pygments and sys.stdout.isatty():\\n        style = get_style_by_name(theme)\\n        formatter = Terminal256Formatter(style=style)\\n        return pygments.highlight(output, JsonLexer(), formatter)\\n\\n    return output   \n",
       "4                                                                                                                                                                                               def _vector_or_scalar(x, type='row'):\\n    \"\"\"Convert an object to either a scalar or a row or column vector.\"\"\"\\n    if isinstance(x, (list, tuple)):\\n        x = np.array(x)\\n    if isinstance(x, np.ndarray):\\n        assert x.ndim == 1\\n        if type == 'column':\\n            x = x[:, None]\\n    return x   \n",
       "6  def data_from_file(file):\\n    \"\"\"Return (first channel data, sample frequency, sample width) from a .wav\\n    file.\"\"\"\\n    fp = wave.open(file, 'r')\\n    data = fp.readframes(fp.getnframes())\\n    channels = fp.getnchannels()\\n    freq = fp.getframerate()\\n    bits = fp.getsampwidth()\\n\\n    # Unpack bytes -- warning currently only tested with 16 bit wavefiles. 32\\n    # bit not supported.\\n    data = struct.unpack(('%sh' % fp.getnframes()) * channels, data)\\n\\n    # Only use first channel...   \n",
       "7                                                                                                     def source_range(start, end, nr_var_dict):\\n    \"\"\"\\n    Given a range of source numbers, as well as a dictionary\\n    containing the numbers of each source, returns a dictionary\\n    containing tuples of the start and end index\\n    for each source variable type.\\n    \"\"\"\\n\\n    return OrderedDict((k, e-s)\\n        for k, (s, e)\\n        in source_range_tuple(start, end, nr_var_dict).iteritems())   \n",
       "8                                                                                                                                                                                                                                                                                                   def timespan(start_time):\\n    \"\"\"Return time in milliseconds from start_time\"\"\"\\n\\n    timespan = datetime.datetime.now() - start_time\\n    timespan_ms = timespan.total_seconds() * 1000\\n    return timespan_ms   \n",
       "9                                                                                                                                                                                                            def _convert_to_array(array_like, dtype):\\n        \"\"\"\\n        Convert Matrix attributes which are array-like or buffer to array.\\n        \"\"\"\\n        if isinstance(array_like, bytes):\\n            return np.frombuffer(array_like, dtype=dtype)\\n        return np.asarray(array_like, dtype=dtype)   \n",
       "\n",
       "  validation_flag  \n",
       "2              no  \n",
       "4              no  \n",
       "6              no  \n",
       "7              no  \n",
       "8              no  \n",
       "9              no  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df1[df1['validation_flag']=='no']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New row data as a dictionary\n",
    "new_row = {'query_id': 'test', 'doc': 'python add two numbers', 'corpus_id': 'test', 'code': \"def add_numbers(a, b):\\n  return a + b\n",
    "\"}\n",
    "\n",
    "# Add the new row using loc\n",
    "df1.loc[len(df1)] = new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def _vector_or_scalar(x, type='row'):\n",
      " \"\"\"Convert an object to either a scalar or a row or column vector.\"\"\"\n",
      " if isinstance(x, (list, tuple)):\n",
      " x = np.array(x)\n",
      " if isinstance(x, np.ndarray):\n",
      " assert x.ndim == 1\n",
      " if type == 'column':\n",
      " x = x[:, None]\n",
      " return x\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "'''''''''def _vector_or_scalar(x, type='row'):\\n \"\"\"Convert an object to either a scalar or a row or column vector.\"\"\"\\n if isinstance(x, (list, tuple)):\\n x = np.array(x)\\n if isinstance(x, np.ndarray):\\n assert x.ndim == 1\\n if type == 'column':\\n x = x[:, None]\\n return x'''''''''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'validation_flag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_flag'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mvalidation_flag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'validation_flag'"
     ]
    }
   ],
   "source": [
    "df['validation_flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "valid_df = pd.read_csv('/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CS696DS-Oracle-Retrieving-Code-Explanations/Explanation_Generation/Cosqa/pre_processing_COSQA/COSQA_validation_query_code.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "validation_flag\n",
       "no                13291\n",
       "yes                6236\n",
       "the code            191\n",
       "\"yes                131\n",
       "```python           127\n",
       "                  ...  \n",
       "python strings        1\n",
       "python syntax         1\n",
       "python tkinter        1\n",
       "python trim           1\n",
       "python sets           1\n",
       "Name: count, Length: 241, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df['validation_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_json(\"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CodeXGLUE/Text-Code/NL-code-search-WebQuery/CoSQA/cosqa-all.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>doc</th>\n",
       "      <th>code</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cosqa-train-18103</td>\n",
       "      <td>python remove all empty items in list</td>\n",
       "      <td>def remove_empty_text(utterances: List[Utteran...</td>\n",
       "      <td>def remove_empty_text ( utterances : List [ Ut...</td>\n",
       "      <td>Remove empty utterances from a list of utteran...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cosqa-train-11841</td>\n",
       "      <td>python return property objectno not value</td>\n",
       "      <td>def value(self):\\n        \"\"\"Value of property...</td>\n",
       "      <td>def value ( self ) : if self . _prop . fget is...</td>\n",
       "      <td>Value of property .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cosqa-train-3015</td>\n",
       "      <td>python not null dict</td>\n",
       "      <td>def purge_dict(idict):\\n    \"\"\"Remove null ite...</td>\n",
       "      <td>def purge_dict ( idict ) : odict = { } for key...</td>\n",
       "      <td>Remove null items from a dictionary</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cosqa-train-5561</td>\n",
       "      <td>how to turn of traceback in python</td>\n",
       "      <td>def format_exception(e):\\n    \"\"\"Returns a str...</td>\n",
       "      <td>def format_exception ( e ) : from . utils . pr...</td>\n",
       "      <td>Returns a string containing the type and text ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cosqa-train-9028</td>\n",
       "      <td>generate list of fixed size python</td>\n",
       "      <td>def batch(items, size):\\n    \"\"\"Batches a list...</td>\n",
       "      <td>def batch ( items , size ) : return [ items [ ...</td>\n",
       "      <td>Batches a list into a list of lists with sub -...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 idx                                        doc  \\\n",
       "0  cosqa-train-18103      python remove all empty items in list   \n",
       "1  cosqa-train-11841  python return property objectno not value   \n",
       "2   cosqa-train-3015                       python not null dict   \n",
       "3   cosqa-train-5561         how to turn of traceback in python   \n",
       "4   cosqa-train-9028         generate list of fixed size python   \n",
       "\n",
       "                                                code  \\\n",
       "0  def remove_empty_text(utterances: List[Utteran...   \n",
       "1  def value(self):\\n        \"\"\"Value of property...   \n",
       "2  def purge_dict(idict):\\n    \"\"\"Remove null ite...   \n",
       "3  def format_exception(e):\\n    \"\"\"Returns a str...   \n",
       "4  def batch(items, size):\\n    \"\"\"Batches a list...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  def remove_empty_text ( utterances : List [ Ut...   \n",
       "1  def value ( self ) : if self . _prop . fget is...   \n",
       "2  def purge_dict ( idict ) : odict = { } for key...   \n",
       "3  def format_exception ( e ) : from . utils . pr...   \n",
       "4  def batch ( items , size ) : return [ items [ ...   \n",
       "\n",
       "                                    docstring_tokens  label  \n",
       "0  Remove empty utterances from a list of utteran...      0  \n",
       "1                                Value of property .      1  \n",
       "2                Remove null items from a dictionary      0  \n",
       "3  Returns a string containing the type and text ...      0  \n",
       "4  Batches a list into a list of lists with sub -...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20604"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_df['doc'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.merge(valid_df, label_df[['doc','label']], on='doc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(temp.groupby(['validation_flag','label'])['query_id'].count()).to_csv('/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CS696DS-Oracle-Retrieving-Code-Explanations/Explanation_Generation/Cosqa/pre_processing_COSQA/COSQA_validation_analysis.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
