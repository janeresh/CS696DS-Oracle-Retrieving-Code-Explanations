{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 12832/20604 [01:28<00:51, 151.49it/s]<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:2: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<string>:1: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "100%|██████████| 20604/20604 [02:22<00:00, 144.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned CSV written to: /work/pi_wenlongzhao_umass_edu/27/anamikaghosh/Cosqa/pre_processing_COSQA/COSQA_cleaned_code.csv in 142.67305779457092 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import tokenize\n",
    "from io import StringIO\n",
    "import textwrap\n",
    "import re\n",
    "import autopep8\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Patch legacy Python 2 exceptions ---\n",
    "def fix_legacy_exception_syntax(code: str) -> str:\n",
    "    return re.sub(r'except\\s+(\\w+)\\s*,\\s*(\\w+):', r'except \\1 as \\2:', code)\n",
    "\n",
    "# --- Fallback: Regex-based docstring remover ---\n",
    "def remove_docstrings_fallback(code: str) -> str:\n",
    "    return re.sub(r'(\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\')', '', code, flags=re.DOTALL)\n",
    "\n",
    "# --- Robust AST-based docstring remover with fallback ---\n",
    "def remove_docstrings(source_code: str) -> str:\n",
    "    source_code = fix_legacy_exception_syntax(source_code)\n",
    "    lines = source_code.splitlines()\n",
    "    try:\n",
    "        tree = ast.parse(source_code)\n",
    "    except SyntaxError:\n",
    "        return remove_docstrings_fallback(source_code)\n",
    "\n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef, ast.Module)):\n",
    "            if not node.body:\n",
    "                continue\n",
    "            first_stmt = node.body[0]\n",
    "            if (\n",
    "                isinstance(first_stmt, ast.Expr)\n",
    "                and isinstance(first_stmt.value, ast.Constant)\n",
    "                and isinstance(first_stmt.value.value, str)\n",
    "            ):\n",
    "                start = first_stmt.lineno - 1\n",
    "                end = getattr(first_stmt, 'end_lineno', start + len(first_stmt.value.value.splitlines()))\n",
    "                for i in range(start, end):\n",
    "                    if 0 <= i < len(lines):\n",
    "                        lines[i] = ''\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "# --- Remove all `#` comments with robust indentation fixing ---\n",
    "def remove_all_comments(code: str) -> (str, bool):\n",
    "\n",
    "    attempts = 0\n",
    "    max_attempts = 1\n",
    "    failed = False\n",
    "    while attempts < max_attempts:\n",
    "        try:\n",
    "            io_obj = StringIO(code)\n",
    "            tokens = list(tokenize.generate_tokens(io_obj.readline))\n",
    "            break  # Successful tokenization, exit loop.\n",
    "        except IndentationError as e:\n",
    "            attempts += 1\n",
    "            code = autopep8.fix_code(code)\n",
    "    else:\n",
    "        # If we exceed max_attempts, flag the issue and return code unchanged.\n",
    "        failed = True\n",
    "        return code, failed\n",
    "\n",
    "    # Build code output from tokens, skipping those with type COMMENT.\n",
    "    output = \"\"\n",
    "    last_lineno = -1\n",
    "    last_col = 0\n",
    "    for tok in tokens:\n",
    "        token_type, token_string, start, end, _ = tok\n",
    "        if token_type == tokenize.COMMENT:\n",
    "            continue\n",
    "        sline, scol = start\n",
    "        eline, ecol = end\n",
    "        if sline > last_lineno:\n",
    "            output += \"\\n\" * (sline - last_lineno - 1)\n",
    "            last_col = 0\n",
    "        if scol > last_col:\n",
    "            output += \" \" * (scol - last_col)\n",
    "        output += token_string\n",
    "        last_lineno = eline\n",
    "        last_col = ecol\n",
    "    return output, failed\n",
    "\n",
    "# --- Remove extra blank lines ---\n",
    "def remove_extra_blank_lines(code: str) -> str:\n",
    "    cleaned_lines = []\n",
    "    previous_blank = False\n",
    "    for line in code.splitlines():\n",
    "        if line.strip() == \"\":\n",
    "            if not previous_blank:\n",
    "                cleaned_lines.append(\"\")\n",
    "                previous_blank = True\n",
    "        else:\n",
    "            cleaned_lines.append(line.rstrip())\n",
    "            previous_blank = False\n",
    "    return \"\\n\".join(cleaned_lines)\n",
    "\n",
    "# --- Fix indentation using autopep8 ---\n",
    "def fix_indentation(code: str) -> str:\n",
    "    return autopep8.fix_code(code)\n",
    "\n",
    "# --- Full pipeline ---\n",
    "def split_code_and_comments(source_code: str) -> (str, bool):\n",
    "    # First, fix indentation issues and dedent.\n",
    "    source_code = fix_indentation(source_code)\n",
    "    source_code = textwrap.dedent(source_code)\n",
    "    # Remove docstrings.\n",
    "    no_docstrings = remove_docstrings(source_code)\n",
    "    # Remove inline comments and get flag.\n",
    "    no_comments, flag = remove_all_comments(no_docstrings)\n",
    "    # Remove extra blank lines.\n",
    "    final_code = remove_extra_blank_lines(no_comments)\n",
    "    return final_code, flag\n",
    "\n",
    "def clean_code_column(input_csv: str, code_column: str = \"code\"):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    cleaned_codes = []\n",
    "    flags = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        final_code, flag = split_code_and_comments(str(row[code_column]))\n",
    "        cleaned_codes.append(final_code)\n",
    "        flags.append(flag)\n",
    "    df[\"cleaned_code\"] = cleaned_codes\n",
    "    # Record indices where remove_all_comments returned code unchanged (flag True)\n",
    "    df[\"remove_all_comments_issue\"] = flags\n",
    "    return df\n",
    "\n",
    "# --- Step 6: Run it ---\n",
    "if __name__ == \"__main__\":\n",
    "    input_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/Cosqa/raw_data/cosqa_queries_code_corpus.csv\"\n",
    "    start = time.time()\n",
    "    df = clean_code_column(input_path)\n",
    "    output_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/Cosqa/pre_processing_COSQA/COSQA_cleaned_code.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    end = time.time()\n",
    "    print(f\"Cleaned CSV written to: {output_path} in {end-start} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
