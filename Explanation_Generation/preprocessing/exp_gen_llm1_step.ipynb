{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-06 21:26:36,565\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams, LLMEngine\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "# Set environment variable to help with memory fragmentation.\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_output(text,keyword):\n",
    "    # keyword = \"Answer:\"\n",
    "    index = text.rfind(keyword)  # Find the last occurrence of \"Answer:\"\n",
    "    if index != -1:\n",
    "        return text[index + len(keyword):].strip()  \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ExplanationGeneratorLama:\n",
    "#     def __init__(self, model_name, max_new_tokens=500):\n",
    "#         # self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         # self.tokenizer.padding_side = \"left\"\n",
    "        \n",
    "#         self.max_new_tokens = max_new_tokens\n",
    "#         self.sampling_params = SamplingParams(\n",
    "#             temperature=0.7,\n",
    "#             top_p=0.95,\n",
    "#             max_tokens=max_new_tokens\n",
    "#         )\n",
    "#         self.llm = LLM(\n",
    "#             model=model_name,\n",
    "#             dtype=\"half\"  # using half precision for GPU\n",
    "#         )\n",
    "#         # self.engine = LLMEngine(model_name, sampling_params=self.sampling_params)\n",
    "\n",
    "#     def generate_explanations_batch(self, entries, max_new_tokens=500):\n",
    "#         # Create prompts by combining each entry with each prompt template.\n",
    "        \n",
    "#         prompts = []\n",
    "        \n",
    "#         for entry in entries:\n",
    "#             prompt_templates = [\n",
    "#                 f\"Doc string: {entry['doc']}\\n\"\n",
    "#                 f\"Code snippet: {entry['code']}\\n\"\n",
    "#                 \"Instruction: Provide a concise explanation of what the above doc and code mean. \"\n",
    "#                 \"Generate strictly less than 100 words in total. Please give the output just as text only. Do not return anything else.\\n\"\n",
    "#                 \"Answer: \\n\"\n",
    "#                 , \n",
    "\n",
    "#                 f\"Doc string: {entry['doc']}\\n\"\n",
    "#                 f\"Code snippet: {entry['code']}\\n\"\n",
    "#                 \"Instruction: Provide a detailed line-by-line explanation of this code snippet, describing the purpose and functionality of each statement, function, and control structure. \"\n",
    "#                 \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "#                 \"Answer: \\n\"\n",
    "#                 ,\n",
    "\n",
    "#                 f\"Doc string: {entry['doc']}\\n\"\n",
    "#                 f\"Code snippet: {entry['code']}\\n\"\n",
    "#                 \"Instruction: Summarize what this code snippet does in simple, non-technical language, focusing on its overall purpose and key operations for someone with little programming experience. \"\n",
    "#                 \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "#                 \"Answer: \\n\"\n",
    "#                 ,\n",
    "\n",
    "#                 f\"Doc string: {entry['doc']}\\n\"\n",
    "#                 f\"Code snippet: {entry['code']}\\n\"\n",
    "#                 \"Instruction: Generate an explanation of the code snippet in such a way that it can regenerate the code based on this explanation. \"\n",
    "#                 \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "#                 \"Answer: \\n\"\n",
    "#                 ,\n",
    "\n",
    "#                 f\"Doc string - entry['doc'] : {entry['doc']}\\n\" \\\n",
    "#                 f\"Code snippet - entry['code'] : {entry['code']}\\n\" \\\n",
    "#                 \"Instruction: Explain how the code snippet in entry['code'] implements or achieves the functionality described in the doc string in entry['doc']. Please provide the explanation as text only without any additional content.\\n\" \\\n",
    "#                 \"Answer: \\n\"\n",
    "#             ]\n",
    "            \n",
    "#             for template in prompt_templates:\n",
    "#                 prompt = (\n",
    "#                     f\"Doc string: {entry['doc']}\\n\"\n",
    "#                     f\"Code snippet: {entry['code']}\\n\"\n",
    "#                     f\"{template}\\n\"\n",
    "#                     \"Answer: \\n\"\n",
    "#                 )\n",
    "#                 prompts.append(prompt)\n",
    "                \n",
    "#         # results = self.engine.generate(prompts)\n",
    "#         results = self.llm.generate(prompts, self.sampling_params)\n",
    "#         explanations = [result.outputs[0].text for result in results]\n",
    "        \n",
    "#         # Regroup explanations by entry.\n",
    "#         n_prompts = len(prompt_templates)\n",
    "#         grouped_explanations = []\n",
    "#         for i in range(0, len(explanations), n_prompts):\n",
    "#             grouped_explanations.append(explanations[i:i+n_prompts])\n",
    "            \n",
    "#         return grouped_explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/CodeSearchNet_Python_valid.csv\"  # change this to your CSV input path\n",
    "#     output_csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/explanations/CodeSearchNet_Python_valid_vllm.csv\"\n",
    "    \n",
    "#     # Load the CSV file into a DataFrame\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     print(\"Data loaded from CSV\")\n",
    "    \n",
    "\n",
    "#     models_dict = {\n",
    "#         \"deepseek\": \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "#         'granite': \"/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f\"\n",
    "#     }\n",
    "    \n",
    "    \n",
    "#     batch_size = 200\n",
    "    \n",
    "#     for model_key, model_path in tqdm(models_dict.items(), desc=\"Processing models\"):\n",
    "#         print(f\"\\nProcessing model {model_key}...\")\n",
    "#         generator = ExplanationGeneratorLama(model_path)\n",
    "#         if hasattr(generator, 'model'):\n",
    "#             generator.model.eval()\n",
    "        \n",
    "#         # Process the DataFrame in batches\n",
    "#         for i in tqdm(range(0, len(df), batch_size), desc=\"Processing batches\"):\n",
    "#             batch_entries = df.iloc[i:i+batch_size][[\"corpus_id\", \"query_id\", \"doc\", \"code\"]].to_dict(\"records\")\n",
    "            \n",
    "#             # Wrap inference in a no_grad context to prevent gradient computations.\n",
    "#             with torch.no_grad():\n",
    "#                 batch_explanations = generator.generate_explanations_batch(batch_entries)\n",
    "            \n",
    "#             for j, explanation_variants in enumerate(batch_explanations):\n",
    "#                 for idx, raw_text in enumerate(explanation_variants):\n",
    "                    \n",
    "#                     df.loc[i+j, f'explanation_{model_key}_{idx+1}'] = raw_text\n",
    "            \n",
    "#             # torch.cuda.empty_cache()\n",
    "#             # gc.collect()\n",
    "        \n",
    "#         del generator\n",
    "#         # torch.cuda.empty_cache()\n",
    "#         # gc.collect()\n",
    "#         df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "#     # df.to_csv(output_csv_path, index=False)\n",
    "#     print(f\"\\nExplanations from all models have been saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data batch by batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = {'Unnamed: 0': 0, 'query_id': 'q251820', 'corpus_id': 'c251820', 'score': 1, 'doc': 'Save model to a pickle located at `path`', 'code': 'def save_act(self, path=None):\\n        \"\"\"Save model to a pickle located at `path`\"\"\"\\n        if path is None:\\n            path = os.path.join(logger.get_dir(), \"model.pkl\")\\n\\n        with tempfile.TemporaryDirectory() as td:\\n            save_variables(os.path.join(td, \"model\"))\\n            arc_name = os.path.join(td, \"packed.zip\")\\n            with zipfile.ZipFile(arc_name, \\'w\\') as zipf:\\n                for root, dirs, files in os.walk(td):\\n                    for fname in files:\\n                        file_path = os.path.join(root, fname)\\n                        if file_path != arc_name:\\n                            zipf.write(file_path, os.path.relpath(file_path, td))\\n            with open(arc_name, \"rb\") as f:\\n                model_data = f.read()\\n        with open(path, \"wb\") as f:\\n            cloudpickle.dump((model_data, self._act_params), f)'}\n",
    "# pd.DataFrame(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded\n",
      "Starting from line 0\n",
      "INFO 04-06 21:26:39 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-06 21:26:39 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-06 21:26:50 config.py:549] This model supports multiple tasks: {'generate', 'score', 'embed', 'classify', 'reward'}. Defaulting to 'generate'.\n",
      "WARNING 04-06 21:26:50 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-06 21:26:50 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-06 21:26:50 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', speculative_config=None, tokenizer='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-06 21:26:52 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-06 21:26:52 model_runner.py:1110] Starting to load model /datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W406 21:26:52.883931974 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.45it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 21:26:53 model_runner.py:1115] Loading model weights took 3.3414 GB\n",
      "INFO 04-06 21:26:54 worker.py:267] Memory profiling takes 0.44 seconds\n",
      "INFO 04-06 21:26:54 worker.py:267] the current vLLM instance can use total_gpu_memory (44.52GiB) x gpu_memory_utilization (0.90) = 40.07GiB\n",
      "INFO 04-06 21:26:54 worker.py:267] model weights take 3.34GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 35.25GiB.\n",
      "INFO 04-06 21:26:54 executor_base.py:111] # cuda blocks: 82516, # CPU blocks: 9362\n",
      "INFO 04-06 21:26:54 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 10.07x\n",
      "INFO 04-06 21:26:56 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 21:27:08 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.21 GiB\n",
      "INFO 04-06 21:27:08 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 15.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2500/2500 [02:25<00:00, 17.21it/s, est. speed input: 6662.78 toks/s, output: 4874.60 toks/s]\n",
      "Streaming dataset:   7%|▋         | 999/13788 [02:46<35:26,  6.02it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 156\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# === RUN ===\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 115\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 115\u001b[0m         all_explanations \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_explanations_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m entry, explanations \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(buffer, all_explanations):\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, explanation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(explanations):\n",
      "Cell \u001b[0;32mIn[3], line 52\u001b[0m, in \u001b[0;36mExplanationGeneratorLama.generate_explanations_batch\u001b[0;34m(self, entries)\u001b[0m\n\u001b[1;32m     16\u001b[0m     prompt_templates \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode snippet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m ]\n\u001b[1;32m     50\u001b[0m     prompts\u001b[38;5;241m.\u001b[39mextend(prompt_templates)\n\u001b[0;32m---> 52\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [res\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Group by original entry\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py:1057\u001b[0m, in \u001b[0;36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1052\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1053\u001b[0m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[1;32m   1054\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m         )\n\u001b[0;32m-> 1057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/entrypoints/llm.py:469\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[0m\n\u001b[1;32m    459\u001b[0m     sampling_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_default_sampling_params()\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_add_requests(\n\u001b[1;32m    462\u001b[0m     prompts\u001b[38;5;241m=\u001b[39mparsed_prompts,\n\u001b[1;32m    463\u001b[0m     params\u001b[38;5;241m=\u001b[39msampling_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    466\u001b[0m     guided_options\u001b[38;5;241m=\u001b[39mguided_options_request,\n\u001b[1;32m    467\u001b[0m     priority\u001b[38;5;241m=\u001b[39mpriority)\n\u001b[0;32m--> 469\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43muse_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_tqdm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_class\u001b[38;5;241m.\u001b[39mvalidate_outputs(outputs, RequestOutput)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/entrypoints/llm.py:1397\u001b[0m, in \u001b[0;36mLLM._run_engine\u001b[0;34m(self, use_tqdm)\u001b[0m\n\u001b[1;32m   1395\u001b[0m total_out_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine\u001b[38;5;241m.\u001b[39mhas_unfinished_requests():\n\u001b[0;32m-> 1397\u001b[0m     step_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[1;32m   1399\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output\u001b[38;5;241m.\u001b[39mfinished:\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/llm_engine.py:1391\u001b[0m, in \u001b[0;36mLLMEngine.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[1;32m   1388\u001b[0m     execute_model_req\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_callbacks[\n\u001b[1;32m   1389\u001b[0m         virtual_engine]\n\u001b[0;32m-> 1391\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[1;32m   1395\u001b[0m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[1;32m   1396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler_config\u001b[38;5;241m.\u001b[39mis_multi_step:\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/executor/executor_base.py:139\u001b[0m, in \u001b[0;36mExecutorBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mexecute_model\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[1;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[0;32m--> 139\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollective_rpc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexecute_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecute_model_req\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py:56\u001b[0m, in \u001b[0;36mUniProcExecutor.collective_rpc\u001b[0;34m(self, method, timeout, args, kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 56\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mrun_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [answer]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py:2196\u001b[0m, in \u001b[0;36mrun_method\u001b[0;34m(obj, method, args, kwargs)\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2195\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(method, obj)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker_base.py:420\u001b[0m, in \u001b[0;36mLocalOrDistributedWorkerBase.execute_model\u001b[0;34m(self, execute_model_req)\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    416\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservability_config\u001b[38;5;241m.\u001b[39mcollect_model_execute_time):\n\u001b[1;32m    417\u001b[0m         orig_model_execute_time \u001b[38;5;241m=\u001b[39m intermediate_tensors\u001b[38;5;241m.\u001b[39mtensors\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    418\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_execute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 420\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mworker_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvirtual_engine\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m model_execute_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group()\u001b[38;5;241m.\u001b[39mis_last_rank:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/model_runner.py:1779\u001b[0m, in \u001b[0;36mModelRunner.execute_model\u001b[0;34m(self, model_input, kv_caches, intermediate_tensors, num_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1776\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[1;32m   1778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_input\u001b[38;5;241m.\u001b[39masync_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1779\u001b[0m     \u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masync_callback\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m output: SamplerOutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m   1783\u001b[0m     logits\u001b[38;5;241m=\u001b[39mlogits,\n\u001b[1;32m   1784\u001b[0m     sampling_metadata\u001b[38;5;241m=\u001b[39mmodel_input\u001b[38;5;241m.\u001b[39msampling_metadata,\n\u001b[1;32m   1785\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py:1120\u001b[0m, in \u001b[0;36mweak_bind.<locals>.weak_bound\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mweak_bound\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inst \u001b[38;5;241m:=\u001b[39m ref():\n\u001b[0;32m-> 1120\u001b[0m         \u001b[43munbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/llm_engine.py:1112\u001b[0m, in \u001b[0;36mLLMEngine._process_model_outputs\u001b[0;34m(self, ctx, request_id)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_processor\u001b[38;5;241m.\u001b[39mprocess_prompt_logprob(seq_group, output)\n\u001b[1;32m   1111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m seq_group_meta\u001b[38;5;241m.\u001b[39mdo_sample:\n\u001b[0;32m-> 1112\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_processor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_async\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m seq_group\u001b[38;5;241m.\u001b[39mis_finished():\n\u001b[1;32m   1116\u001b[0m     finished_now\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/output_processor/single_step.py:97\u001b[0m, in \u001b[0;36mSingleStepOutputProcessor.process_outputs\u001b[0;34m(self, sequence_group, outputs, is_async)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Append all new tokens to sequences in the sequence group. Fork any\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03msurviving beam candidates; free any unsurviving ones.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    externally (before the next schedule() call)\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(outputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     96\u001b[0m         ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not support multiple outputs per step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_sequence_group_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_group\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mis_async\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/output_processor/single_step.py:124\u001b[0m, in \u001b[0;36mSingleStepOutputProcessor._process_sequence_group_outputs\u001b[0;34m(self, seq_group, outputs, is_async)\u001b[0m\n\u001b[1;32m    122\u001b[0m     seq\u001b[38;5;241m.\u001b[39mappend_token_id(sample\u001b[38;5;241m.\u001b[39moutput_token, sample\u001b[38;5;241m.\u001b[39mlogprobs)\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sampling_params\u001b[38;5;241m.\u001b[39mdetokenize \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenizer:\n\u001b[0;32m--> 124\u001b[0m     new_char_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode_sequence_inplace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     new_char_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/transformers_utils/detokenizer.py:165\u001b[0m, in \u001b[0;36mDetokenizer.decode_sequence_inplace\u001b[0;34m(self, seq, prms)\u001b[0m\n\u001b[1;32m    163\u001b[0m seq\u001b[38;5;241m.\u001b[39mprefix_offset \u001b[38;5;241m=\u001b[39m prefix_offset\n\u001b[1;32m    164\u001b[0m seq\u001b[38;5;241m.\u001b[39mread_offset \u001b[38;5;241m=\u001b[39m read_offset\n\u001b[0;32m--> 165\u001b[0m \u001b[43mseq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_text\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_decoded_token_text\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(new_decoded_token_text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Your custom class\n",
    "class ExplanationGeneratorLama:\n",
    "    def __init__(self, model_name, max_new_tokens=500):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=max_new_tokens\n",
    "        )\n",
    "        self.llm = LLM(model=model_name, dtype=\"half\")  # GPU half precision\n",
    "\n",
    "    def generate_explanations_batch(self, entries):\n",
    "        prompts = []\n",
    "        for entry in entries:\n",
    "            code = entry['code']\n",
    "            prompt_templates = [\n",
    "            \n",
    "            f\"Code snippet: {code}\\n\"\n",
    "            \"Instruction: Provide a concise explanation of what the above code mean. \"\n",
    "            \"Generate strictly less than 100 words in total. Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\"\n",
    "            , \n",
    "\n",
    "            \n",
    "            f\"Code snippet: {code}\\n\"\n",
    "            \"Instruction: Provide a detailed line-by-line explanation of this code snippet, describing the purpose and functionality of each statement, function, and control structure. \"\n",
    "            \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\"\n",
    "            ,\n",
    "\n",
    "            \n",
    "            f\"Code snippet: {code}\\n\"\n",
    "            \"Instruction: Summarize what this code snippet does in simple, non-technical language, focusing on its overall purpose and key operations for someone with little programming experience. \"\n",
    "            \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\"\n",
    "            ,\n",
    "\n",
    "            \n",
    "            f\"Code snippet: {code}\\n\"\n",
    "            \"Instruction: Generate an explanation of the code snippet in such a way that it can regenerate the code based on this explanation. \"\n",
    "            \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\"\n",
    "            ,\n",
    "\n",
    "        \n",
    "            f\"Code snippet - entry['code'] : {code}\\n\" \\\n",
    "            \"Instruction: Explain how the code snippet in entry['code'] implements. Please provide the explanation as text only without any additional content.\\n\" \\\n",
    "            \"Answer: \\n\"\n",
    "        ]\n",
    "            prompts.extend(prompt_templates)\n",
    "\n",
    "        results = self.llm.generate(prompts, self.sampling_params)\n",
    "        outputs = [res.outputs[0].text for res in results]\n",
    "\n",
    "        # Group by original entry\n",
    "        grouped = []\n",
    "        for i in range(0, len(outputs), 5):\n",
    "            grouped.append(outputs[i:i+5])\n",
    "\n",
    "        return grouped\n",
    "\n",
    "\n",
    "# === CONFIG ===\n",
    "csv_path = '/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/CodeSearchNet_Python_valid.csv'\n",
    "output_path = '/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/output.csv'\n",
    "checkpoint_path = '/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/checkpoint.txt'\n",
    "model_path = \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\"\n",
    "model = 'deepseek'\n",
    "batch_size = 128  # adjust to your GPU memory\n",
    "\n",
    "# === HELPERS ===\n",
    "def load_checkpoint():\n",
    "    return int(open(checkpoint_path).read()) if os.path.exists(checkpoint_path) else 0\n",
    "\n",
    "def save_checkpoint(line_num):\n",
    "    with open(checkpoint_path, 'w') as f:\n",
    "        f.write(str(line_num))\n",
    "\n",
    "def write_header_if_needed(output_path, fieldnames):\n",
    "    if not os.path.exists(output_path) or os.stat(output_path).st_size == 0:\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "# === MAIN ===\n",
    "def main():\n",
    "    dataset = load_dataset('csv', data_files=csv_path, split='train', streaming=True)\n",
    "    print('Dataset loaded')\n",
    "\n",
    "    start_line = load_checkpoint()\n",
    "    print(f'Starting from line {start_line}')\n",
    "\n",
    "    generator = ExplanationGeneratorLama(model_path)\n",
    "    if hasattr(generator, 'model'):\n",
    "        generator.model.eval()\n",
    "    print('Model loaded')\n",
    "\n",
    "    buffer = []\n",
    "    line_num = 0\n",
    "    written_header = False\n",
    "    total_len = sum(1 for _ in dataset)\n",
    "\n",
    "    with open(output_path, 'a', newline='', encoding='utf-8') as outfile:\n",
    "        writer = None\n",
    "\n",
    "        for row in tqdm(dataset, desc=\"Streaming dataset\", total=total_len):\n",
    "            line_num += 1\n",
    "            if line_num <= start_line:\n",
    "                continue\n",
    "\n",
    "            buffer.append(row)\n",
    "\n",
    "            if len(buffer) == batch_size:\n",
    "                with torch.no_grad():\n",
    "                    all_explanations = generator.generate_explanations_batch(buffer)\n",
    "\n",
    "                for entry, explanations in zip(buffer, all_explanations):\n",
    "                    for idx, explanation in enumerate(explanations):\n",
    "                        entry[f'explanation_deepseek_{idx+1}'] = explanation\n",
    "\n",
    "                    # Setup writer on first pass\n",
    "                    if writer is None:\n",
    "                        fieldnames = list(entry.keys())\n",
    "                        write_header_if_needed(output_path, fieldnames)\n",
    "                        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "                    writer.writerow(entry)\n",
    "\n",
    "                outfile.flush()\n",
    "                save_checkpoint(line_num)\n",
    "                buffer.clear()\n",
    "\n",
    "        # Final batch (if any)\n",
    "        if buffer:\n",
    "            with torch.no_grad():\n",
    "                all_explanations = generator.generate_explanations_batch(buffer)\n",
    "\n",
    "            for entry, explanations in zip(buffer, all_explanations):\n",
    "                for idx, explanation in enumerate(explanations):\n",
    "                    entry[f'explanation_{model}_{idx+1}'] = explanation\n",
    "\n",
    "                if writer is None:\n",
    "                    fieldnames = list(entry.keys())\n",
    "                    write_header_if_needed(output_path, fieldnames)\n",
    "                    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "\n",
    "                writer.writerow(entry)\n",
    "\n",
    "            outfile.flush()\n",
    "            save_checkpoint(line_num)\n",
    "\n",
    "    print(f\"✅ Finished processing. Output saved to {output_path}\")\n",
    "\n",
    "# === RUN ===\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Dataloader (ignore) - Slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationGeneratorLama:\n",
    "    def __init__(self, model_name, max_new_tokens=500):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=max_new_tokens\n",
    "        )\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            dtype=\"half\"  # using half precision for GPU\n",
    "        )\n",
    "\n",
    "    def generate_explanations_batch(self, entries, max_new_tokens=500):\n",
    "        prompts = []\n",
    "        # List of prompt templates for generating various explanations\n",
    "        prompt_templates = [\n",
    "            f\"Doc string: {entries['doc']}\\n\"\n",
    "            f\"Code snippet: {entries['code']}\\n\"\n",
    "            \"Instruction: Provide a concise explanation of what the above doc and code mean. \"\n",
    "            \"Generate strictly less than 100 words in total. Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\",\n",
    "\n",
    "            f\"Doc string: {entries['doc']}\\n\"\n",
    "            f\"Code snippet: {entries['code']}\\n\"\n",
    "            \"Instruction: Provide a detailed line-by-line explanation of this code snippet, describing the purpose and functionality of each statement, function, and control structure. \"\n",
    "            \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\",\n",
    "\n",
    "            f\"Doc string: {entries['doc']}\\n\"\n",
    "            f\"Code snippet: {entries['code']}\\n\"\n",
    "            \"Instruction: Summarize what this code snippet does in simple, non-technical language, focusing on its overall purpose and key operations for someone with little programming experience. \"\n",
    "            \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\",\n",
    "\n",
    "            f\"Doc string: {entries['doc']}\\n\"\n",
    "            f\"Code snippet: {entries['code']}\\n\"\n",
    "            \"Instruction: Generate an explanation of the code snippet in such a way that it can regenerate the code based on this explanation. \"\n",
    "            \"Please give the output just as text only. Do not return anything else.\\n\"\n",
    "            \"Answer: \\n\",\n",
    "\n",
    "            f\"Doc string - entry['doc'] : {entries['doc']}\\n\"\n",
    "            f\"Code snippet - entry['code'] : {entries['code']}\\n\"\n",
    "            \"Instruction: Explain how the code snippet in entry['code'] implements or achieves the functionality described in the doc string in entry['doc']. Please provide the explanation as text only without any additional content.\\n\"\n",
    "            \"Answer: \\n\"\n",
    "        ]\n",
    "        \n",
    "        # Create a prompt for each template\n",
    "        for template in prompt_templates:\n",
    "            prompt = (\n",
    "                f\"Doc string: {entries['doc']}\\n\"\n",
    "                f\"Code snippet: {entries['code']}\\n\"\n",
    "                f\"{template}\\n\"\n",
    "                \"Answer: \\n\"\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "                \n",
    "        results = self.llm.generate(prompts, self.sampling_params)\n",
    "        explanations = [result.outputs[0].text for result in results]\n",
    "        return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "csv_path = '/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/CodeSearchNet_Python_valid.csv'\n",
    "output_path = '/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/output.csv'\n",
    "checkpoint_path = '/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/checkpoint.txt'\n",
    "\n",
    "# Checkpoint functions to resume processing\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        with open(checkpoint_path, 'r') as f:\n",
    "            return int(f.read().strip())\n",
    "    return 0\n",
    "\n",
    "def save_checkpoint(line_num):\n",
    "    with open(checkpoint_path, 'w') as f:\n",
    "        f.write(str(line_num))\n",
    "\n",
    "# Custom PyTorch Dataset to read from the DataFrame\n",
    "class CodeExplanationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        return row.to_dict()\n",
    "\n",
    "# Process a batch of rows from the DataLoader\n",
    "def process_batch(batch, generator):\n",
    "    processed_rows = []\n",
    "    # batch is a dictionary where each key maps to a list of values (one per row in the batch)\n",
    "    batch_size = len(batch['doc'])\n",
    "    for i in range(batch_size):\n",
    "        entry = {\n",
    "            'doc': batch['doc'][i],\n",
    "            'code': batch['code'][i]\n",
    "        }\n",
    "        explanations = generator.generate_explanations_batch(entry)\n",
    "        # Create a new dictionary for the processed row that includes original fields\n",
    "        row_dict = {key: batch[key][i] for key in batch}\n",
    "        for idx, explanation in enumerate(explanations):\n",
    "            row_dict[f'explanation_deepseek_{idx+1}'] = explanation\n",
    "        processed_rows.append(row_dict)\n",
    "    return processed_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Read the entire CSV file using pandas\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Load checkpoint and skip already processed rows\n",
    "    start_line = load_checkpoint()\n",
    "    if start_line > 0:\n",
    "        df = df.iloc[start_line:]\n",
    "    \n",
    "    dataset = CodeExplanationDataset(df)\n",
    "    # Adjust the batch size as needed (e.g., 4)\n",
    "    batch_size = 10\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    model_path = \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\"\n",
    "    generator = ExplanationGeneratorLama(model_path)\n",
    "    if hasattr(generator, 'model'):\n",
    "        generator.model.eval()\n",
    "    \n",
    "    with open(output_path, 'a', newline='', encoding='utf-8') as outfile:\n",
    "        writer = None\n",
    "        processed_count = start_line\n",
    "        \n",
    "        for batch in tqdm(dataloader):\n",
    "            processed_rows = process_batch(batch, generator)\n",
    "            for row in processed_rows:\n",
    "                processed_count += 1\n",
    "                # Initialize CSV writer if it has not been created yet\n",
    "                if writer is None:\n",
    "                    fieldnames = list(row.keys())\n",
    "                    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "                    # Write header if output file is empty\n",
    "                    if os.stat(output_path).st_size == 0:\n",
    "                        writer.writeheader()\n",
    "                writer.writerow(row)\n",
    "                save_checkpoint(processed_count)\n",
    "                print(f'Processed row no. {processed_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 20:39:17 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-06 20:39:17 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-06 20:39:28 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'classify', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 04-06 20:39:28 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-06 20:39:28 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-06 20:39:28 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', speculative_config=None, tokenizer='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-06 20:39:29 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-06 20:39:30 model_runner.py:1110] Starting to load model /datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W406 20:39:30.563524972 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.47it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 20:39:31 model_runner.py:1115] Loading model weights took 3.3414 GB\n",
      "INFO 04-06 20:39:31 worker.py:267] Memory profiling takes 0.44 seconds\n",
      "INFO 04-06 20:39:31 worker.py:267] the current vLLM instance can use total_gpu_memory (44.52GiB) x gpu_memory_utilization (0.90) = 40.07GiB\n",
      "INFO 04-06 20:39:31 worker.py:267] model weights take 3.34GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 35.25GiB.\n",
      "INFO 04-06 20:39:31 executor_base.py:111] # cuda blocks: 82516, # CPU blocks: 9362\n",
      "INFO 04-06 20:39:31 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 10.07x\n",
      "INFO 04-06 20:39:34 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:12<00:00,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-06 20:39:46 model_runner.py:1562] Graph capturing finished in 12 secs, took 0.21 GiB\n",
      "INFO 04-06 20:39:46 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 15.08 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, est. speed input: 1365.19 toks/s, output: 369.98 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.48it/s, est. speed input: 1345.83 toks/s, output: 532.75 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s, est. speed input: 617.67 toks/s, output: 430.26 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:02<00:00,  1.96it/s, est. speed input: 5007.79 toks/s, output: 266.36 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.41it/s, est. speed input: 3183.81 toks/s, output: 436.02 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, est. speed input: 770.04 toks/s, output: 575.66 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, est. speed input: 846.72 toks/s, output: 434.71 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, est. speed input: 739.72 toks/s, output: 534.76 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.46it/s, est. speed input: 1927.44 toks/s, output: 450.13 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.52it/s, est. speed input: 687.95 toks/s, output: 508.00 toks/s]\n",
      "  0%|          | 1/1375 [00:33<12:45:41, 33.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row no. 45\n",
      "Processed row no. 46\n",
      "Processed row no. 47\n",
      "Processed row no. 48\n",
      "Processed row no. 49\n",
      "Processed row no. 50\n",
      "Processed row no. 51\n",
      "Processed row no. 52\n",
      "Processed row no. 53\n",
      "Processed row no. 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, est. speed input: 924.36 toks/s, output: 444.25 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.50it/s, est. speed input: 977.17 toks/s, output: 490.53 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:02<00:00,  1.99it/s, est. speed input: 2145.58 toks/s, output: 468.46 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s, est. speed input: 1214.38 toks/s, output: 490.58 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.49it/s, est. speed input: 1293.47 toks/s, output: 410.43 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.57it/s, est. speed input: 798.12 toks/s, output: 335.04 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.55it/s, est. speed input: 989.52 toks/s, output: 383.14 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, est. speed input: 855.86 toks/s, output: 432.92 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.54it/s, est. speed input: 976.54 toks/s, output: 397.77 toks/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, est. speed input: 1220.88 toks/s, output: 393.52 toks/s]\n",
      "  0%|          | 2/1375 [01:05<12:30:04, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed row no. 55\n",
      "Processed row no. 56\n",
      "Processed row no. 57\n",
      "Processed row no. 58\n",
      "Processed row no. 59\n",
      "Processed row no. 60\n",
      "Processed row no. 61\n",
      "Processed row no. 62\n",
      "Processed row no. 63\n",
      "Processed row no. 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 5/5 [00:03<00:00,  1.51it/s, est. speed input: 1238.73 toks/s, output: 459.18 toks/s]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 sec per row (or) example using dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
