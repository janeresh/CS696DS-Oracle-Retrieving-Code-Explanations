{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-05 15:57:28,730\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset, Dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set environment variable to help with memory fragmentation\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Track progress\n",
    "PROGRESS_FILE = \"last_processed.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_progress():\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        with open(PROGRESS_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_progress(progress_dict):\n",
    "    with open(PROGRESS_FILE, \"w\") as f:\n",
    "        json.dump(progress_dict, f)\n",
    "\n",
    "def clean_output(text, keyword):\n",
    "    index = text.rfind(keyword)\n",
    "    if index != -1:\n",
    "        return text[index + len(keyword):].strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplanationGeneratorLama:\n",
    "    def __init__(self, model_name, max_new_tokens=500):\n",
    "        self.max_new_tokens = max_new_tokens\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            max_tokens=max_new_tokens\n",
    "        )\n",
    "        self.llm = LLM(\n",
    "            model=model_name,\n",
    "            dtype=\"half\"\n",
    "        )\n",
    "\n",
    "    def build_prompts(self, entry):\n",
    "        prompt_templates = [\n",
    "            \"Instruction: Provide a concise explanation of what the above code mean. Generate strictly less than 100 words in total. Please give the output just as text only. Do not return anything else. Answer: \\n\",\n",
    "            \"Instruction: Provide a detailed line-by-line explanation of this code snippet, describing the purpose and functionality of each statement, function, and control structure. Please give the output just as text only. Do not return anything else. Answer: \\n\",\n",
    "            \"Instruction: Summarize what this code snippet does in simple, non-technical language, focusing on its overall purpose and key operations for someone with little programming experience. Please give the output just as text only. Do not return anything else. Answer: \\n\",\n",
    "            \"Instruction: Generate an explanation of the code snippet in such a way that it can regenerate the code based on this explanation. Please give the output just as text only. Do not return anything else. Answer: \\n\",\n",
    "            \"Instruction: Explain how the code snippet  is implemented. Please provide the explanation as text only without any additional content. Answer: \\n\"\n",
    "        ]\n",
    "        prompts = []\n",
    "        for template in prompt_templates:\n",
    "            prompt = (\n",
    "                f\"Code snippet: {entry}\\n\"\n",
    "                f\"{template}\"\n",
    "            )\n",
    "            prompts.append(prompt)\n",
    "        return prompts\n",
    "\n",
    "    def generate_explanations_batch(self, batch):\n",
    "        all_prompts = []\n",
    "        print(batch)\n",
    "        print(type(batch))\n",
    "        for entry in batch['code']:\n",
    "            all_prompts.extend(self.build_prompts(entry))\n",
    "        results = self.llm.generate(all_prompts, self.sampling_params)\n",
    "        explanations = [res.outputs[0].text for res in results]\n",
    "\n",
    "        # Group explanations\n",
    "        grouped = [explanations[i:i+5] for i in range(0, len(explanations), 5)]\n",
    "        return grouped\n",
    "\n",
    "def process_and_save(batch, generator, model_key, output_path, progress_dict):\n",
    "    explanations = generator.generate_explanations_batch(batch)\n",
    "    new_data = []\n",
    "    for i, entry in enumerate(batch):\n",
    "        row = {\n",
    "            \"corpus_id\": entry[\"corpus_id\"],\n",
    "            \"query_id\": entry[\"query_id\"],\n",
    "            \"doc\": entry[\"doc\"],\n",
    "            \"code\": entry[\"code\"]\n",
    "        }\n",
    "        for j, explanation in enumerate(explanations[i]):\n",
    "            row[f\"explanation_{model_key}_{j+1}\"] = explanation\n",
    "        new_data.append(row)\n",
    "\n",
    "    df_batch = pd.DataFrame(new_data)\n",
    "    df_batch.to_csv(output_path, mode='a', index=False, header=not os.path.exists(output_path))\n",
    "\n",
    "    # Only update progress after successful write\n",
    "    last_id = batch[-1][\"corpus_id\"]\n",
    "    progress_dict[model_key] = last_id\n",
    "    save_progress(progress_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing model deepseek\n",
      "INFO 04-05 15:57:30 __init__.py:207] Automatically detected platform cuda.\n",
      "WARNING 04-05 15:57:30 config.py:2448] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 04-05 15:57:38 config.py:549] This model supports multiple tasks: {'embed', 'reward', 'score', 'classify', 'generate'}. Defaulting to 'generate'.\n",
      "WARNING 04-05 15:57:38 arg_utils.py:1187] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 04-05 15:57:38 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 04-05 15:57:38 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', speculative_config=None, tokenizer='/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-05 15:57:39 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-05 15:57:40 model_runner.py:1110] Starting to load model /datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W405 15:57:40.630285986 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n",
      "\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.31it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 15:57:41 model_runner.py:1115] Loading model weights took 3.3414 GB\n",
      "INFO 04-05 15:57:41 worker.py:267] Memory profiling takes 0.48 seconds\n",
      "INFO 04-05 15:57:41 worker.py:267] the current vLLM instance can use total_gpu_memory (44.52GiB) x gpu_memory_utilization (0.90) = 40.07GiB\n",
      "INFO 04-05 15:57:41 worker.py:267] model weights take 3.34GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 35.25GiB.\n",
      "INFO 04-05 15:57:42 executor_base.py:111] # cuda blocks: 82516, # CPU blocks: 9362\n",
      "INFO 04-05 15:57:42 executor_base.py:116] Maximum concurrency for 131072 tokens per request: 10.07x\n",
      "INFO 04-05 15:57:44 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-05 15:57:58 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.21 GiB\n",
      "INFO 04-05 15:57:58 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 17.69 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameter 'function'=<function process_batch at 0x7ddc6c3fed30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'query_id': ['q251820', 'q251821', 'q251822', 'q251823', 'q251824', 'q251825', 'q251826', 'q251827', 'q251828', 'q251829', 'q251830', 'q251831', 'q251832', 'q251833', 'q251834', 'q251835', 'q251837', 'q251838', 'q251839', 'q251840', 'q251841', 'q251842', 'q251843', 'q251844', 'q251845', 'q251846', 'q251847', 'q251848', 'q251849', 'q251850', 'q251851', 'q251852', 'q251853', 'q251854', 'q251855', 'q251856', 'q251857', 'q251858', 'q251859', 'q251860'], 'corpus_id': ['c251820', 'c251821', 'c251822', 'c251823', 'c251824', 'c251825', 'c251826', 'c251827', 'c251828', 'c251829', 'c251830', 'c251831', 'c251832', 'c251833', 'c251834', 'c251835', 'c251836', 'c251837', 'c251838', 'c251839', 'c251840', 'c251841', 'c251842', 'c251843', 'c251844', 'c251845', 'c251846', 'c251847', 'c251848', 'c251849', 'c251850', 'c251851', 'c251852', 'c251853', 'c251854', 'c251855', 'c251856', 'c251857', 'c251858', 'c251859'], 'score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'doc': ['Save model to a pickle located at `path`', 'CNN from Nature paper.', 'convolutions-only net\\n\\n    Parameters:\\n    ----------\\n\\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\\n\\n    Returns:\\n\\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer', 'Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.', 'Parse arguments not consumed by arg parser into a dicitonary', 'from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\\n    Processes.', 'Demmel p 312', 'Create placeholder to feed observations into of the size appropriate to the observation space\\n\\n    Parameters:\\n    ----------\\n\\n    ob_space: gym.Space     observation space\\n\\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\\n\\n    name: str               name of the placeholder\\n\\n    Returns:\\n    -------\\n\\n    tensorflow placeholder tensor', 'Create placeholder to feed observations into of the size appropriate to the observation space, and add input\\n    encoder of the appropriate type.', 'Encode input in the way that is appropriate to the observation space\\n\\n    Parameters:\\n    ----------\\n\\n    ob_space: gym.Space             observation space\\n\\n    placeholder: tf.placeholder     observation input placeholder', 'Pickles the current policy for later inspection.', 'Generates a dictionary that contains all collected statistics.', \"Smooth signal y, where radius is determines the size of the window\\n\\n    mode='twosided':\\n        average over the window [max(index - radius, 0), min(index + radius, len(y)-1)]\\n    mode='causal':\\n        average over the window [max(index - radius, 0), index]\\n\\n    valid_only: put nan in entries where the full-sized window is not available\", 'Deep-copy an observation dict.', 'Get dict-structured information about a gym.Space.\\n\\n    Returns:\\n      A tuple (keys, shapes, dtypes):\\n        keys: a list of dict keys.\\n        shapes: a dict mapping keys to shapes.\\n        dtypes: a dict mapping keys to dtypes.', 'Calculates q_retrace targets\\n\\n    :param R: Rewards\\n    :param D: Dones\\n    :param q_i: Q values for actions taken\\n    :param v: V values\\n    :param rho_i: Importance weight for each action\\n    :return: Q_retrace values', 'Control a single environment instance using IPC and\\n    shared memory.', \"Main entrypoint for A2C algorithm. Train a policy with given network architecture on a given environment using a2c algorithm.\\n\\n    Parameters:\\n    -----------\\n\\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\\n\\n\\n    env:                RL environment. Should implement interface similar to VecEnv (baselines.common/vec_env) or be wrapped with DummyVecEnv (baselines.common/vec_env/dummy_vec_env.py)\\n\\n\\n    seed:               seed to make random number sequence in the alorightm reproducible. By default is None which means seed from system noise generator (not reproducible)\\n\\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\\n                        nenv is number of environment copies simulated in parallel)\\n\\n    total_timesteps:    int, total number of timesteps to train on (default: 80M)\\n\\n    vf_coef:            float, coefficient in front of value function loss in the total loss function (default: 0.5)\\n\\n    ent_coef:           float, coeffictiant in front of the policy entropy in the total loss function (default: 0.01)\\n\\n    max_gradient_norm:  float, gradient is clipped to have global L2 norm no more than this value (default: 0.5)\\n\\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\\n\\n    lrschedule:         schedule of learning rate. Can be 'linear', 'constant', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\\n                        returns fraction of the learning rate (specified as lr) as output\\n\\n    epsilon:            float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\\n\\n    alpha:              float, RMSProp decay parameter (default: 0.99)\\n\\n    gamma:              float, reward discounting parameter (default: 0.99)\\n\\n    log_interval:       int, specifies how frequently the logs are printed out (default: 100)\\n\\n    **network_kwargs:   keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\\n                        For instance, 'mlp' network architecture has arguments num_hidden and num_layers.\", 'swap and then flatten axes 0 and 1', 'Print the number of seconds in human readable format.\\n\\n    Examples:\\n    2 days\\n    2 hours and 37 minutes\\n    less than a minute\\n\\n    Paramters\\n    ---------\\n    seconds_left: int\\n        Number of seconds to be converted to the ETA\\n    Returns\\n    -------\\n    eta: str\\n        String representing the pretty ETA.', 'Add a boolean flag to argparse parser.\\n\\n    Parameters\\n    ----------\\n    parser: argparse.Parser\\n        parser to add the flag to\\n    name: str\\n        --<name> will enable the flag, while --no-<name> will disable it\\n    default: bool or None\\n        default value of the flag\\n    help: str\\n        help string for the flag', 'Given an a gym environment possibly wrapped multiple times, returns a wrapper\\n    of class named classname or raises ValueError if no such wrapper was applied\\n\\n    Parameters\\n    ----------\\n    env: gym.Env of gym.Wrapper\\n        gym environment\\n    classname: str\\n        name of the wrapper\\n\\n    Returns\\n    -------\\n    wrapper: gym.Wrapper\\n        wrapper named classname', 'Unpickle a possible compressed pickle.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        path to the output file\\n    compression: bool\\n        if true assumes that pickle was compressed when created and attempts decompression.\\n\\n    Returns\\n    -------\\n    obj: object\\n        the unpickled object', 'Update the estimate.\\n\\n        Parameters\\n        ----------\\n        new_val: float\\n            new observated value of estimated quantity.', 'Stores provided method args as instance attributes.', 'Flattens a variables and their gradients.', 'Creates a simple neural network', 'Re-launches the current script with workers\\n    Returns \"parent\" for original parent, \"child\" for MPI children', 'Get default session or create one with a given config', 'Initialize all the uninitialized variables in the global scope.', 'adjust shape of the data to the shape of the placeholder if possible.\\n    If shape is incompatible, AssertionError is thrown\\n\\n    Parameters:\\n        placeholder     tensorflow input placeholder\\n\\n        data            input data to be (potentially) reshaped to be fed into placeholder\\n\\n    Returns:\\n        reshaped data', 'Configure environment for DeepMind-style Atari.', 'Reset only when lives are exhausted.\\n        This way all states are still reachable even though lives are episodic,\\n        and the learner need not know about any of this behind-the-scenes.', 'Count the GPUs on this machine.', 'Set CUDA_VISIBLE_DEVICES to MPI rank if not already set', 'Returns the rank of each process on its machine\\n    The processes on a given machine will be assigned ranks\\n        0, 1, 2, ..., N-1,\\n    where N is the number of processes on this machine.\\n\\n    Useful if you want to assign one gpu per machine', 'Copies the file from rank 0 to all other ranks\\n    Puts it in the same place on all machines', 'Perform a reduction operation over dicts', 'computes discounted sums along 0th dimension of x.\\n\\n    inputs\\n    ------\\n    x: ndarray\\n    gamma: float\\n\\n    outputs\\n    -------\\n    y: ndarray with same shape as x, satisfying\\n\\n        y[t] = x[t] + gamma*x[t+1] + gamma^2*x[t+2] + ... + gamma^k x[t+k],\\n                where k = len(x) - t - 1', 'See ReplayBuffer.store_effect'], 'code': ['def save_act(self, path=None):\\n        \"\"\"Save model to a pickle located at `path`\"\"\"\\n        if path is None:\\n            path = os.path.join(logger.get_dir(), \"model.pkl\")\\n\\n        with tempfile.TemporaryDirectory() as td:\\n            save_variables(os.path.join(td, \"model\"))\\n            arc_name = os.path.join(td, \"packed.zip\")\\n            with zipfile.ZipFile(arc_name, \\'w\\') as zipf:\\n                for root, dirs, files in os.walk(td):\\n                    for fname in files:\\n                        file_path = os.path.join(root, fname)\\n                        if file_path != arc_name:\\n                            zipf.write(file_path, os.path.relpath(file_path, td))\\n            with open(arc_name, \"rb\") as f:\\n                model_data = f.read()\\n        with open(path, \"wb\") as f:\\n            cloudpickle.dump((model_data, self._act_params), f)', 'def nature_cnn(unscaled_images, **conv_kwargs):\\n    \"\"\"\\n    CNN from Nature paper.\\n    \"\"\"\\n    scaled_images = tf.cast(unscaled_images, tf.float32) / 255.\\n    activ = tf.nn.relu\\n    h = activ(conv(scaled_images, \\'c1\\', nf=32, rf=8, stride=4, init_scale=np.sqrt(2),\\n                   **conv_kwargs))\\n    h2 = activ(conv(h, \\'c2\\', nf=64, rf=4, stride=2, init_scale=np.sqrt(2), **conv_kwargs))\\n    h3 = activ(conv(h2, \\'c3\\', nf=64, rf=3, stride=1, init_scale=np.sqrt(2), **conv_kwargs))\\n    h3 = conv_to_fc(h3)\\n    return activ(fc(h3, \\'fc1\\', nh=512, init_scale=np.sqrt(2)))', 'def conv_only(convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)], **conv_kwargs):\\n    \\'\\'\\'\\n    convolutions-only net\\n\\n    Parameters:\\n    ----------\\n\\n    conv:       list of triples (filter_number, filter_size, stride) specifying parameters for each layer.\\n\\n    Returns:\\n\\n    function that takes tensorflow tensor as input and returns the output of the last convolutional layer\\n\\n    \\'\\'\\'\\n\\n    def network_fn(X):\\n        out = tf.cast(X, tf.float32) / 255.\\n        with tf.variable_scope(\"convnet\"):\\n            for num_outputs, kernel_size, stride in convs:\\n                out = layers.convolution2d(out,\\n                                           num_outputs=num_outputs,\\n                                           kernel_size=kernel_size,\\n                                           stride=stride,\\n                                           activation_fn=tf.nn.relu,\\n                                           **conv_kwargs)\\n\\n        return out\\n    return network_fn', 'def make_vec_env(env_id, env_type, num_env, seed,\\n                 wrapper_kwargs=None,\\n                 start_index=0,\\n                 reward_scale=1.0,\\n                 flatten_dict_observations=True,\\n                 gamestate=None):\\n    \"\"\"\\n    Create a wrapped, monitored SubprocVecEnv for Atari and MuJoCo.\\n    \"\"\"\\n    wrapper_kwargs = wrapper_kwargs or {}\\n    mpi_rank = MPI.COMM_WORLD.Get_rank() if MPI else 0\\n    seed = seed + 10000 * mpi_rank if seed is not None else None\\n    logger_dir = logger.get_dir()\\n    def make_thunk(rank):\\n        return lambda: make_env(\\n            env_id=env_id,\\n            env_type=env_type,\\n            mpi_rank=mpi_rank,\\n            subrank=rank,\\n            seed=seed,\\n            reward_scale=reward_scale,\\n            gamestate=gamestate,\\n            flatten_dict_observations=flatten_dict_observations,\\n            wrapper_kwargs=wrapper_kwargs,\\n            logger_dir=logger_dir\\n        )\\n\\n    set_global_seeds(seed)\\n    if num_env > 1:\\n        return SubprocVecEnv([make_thunk(i + start_index) for i in range(num_env)])\\n    else:\\n        return DummyVecEnv([make_thunk(start_index)])', 'def parse_unknown_args(args):\\n    \"\"\"\\n    Parse arguments not consumed by arg parser into a dicitonary\\n    \"\"\"\\n    retval = {}\\n    preceded_by_key = False\\n    for arg in args:\\n        if arg.startswith(\\'--\\'):\\n            if \\'=\\' in arg:\\n                key = arg.split(\\'=\\')[0][2:]\\n                value = arg.split(\\'=\\')[1]\\n                retval[key] = value\\n            else:\\n                key = arg[2:]\\n                preceded_by_key = True\\n        elif preceded_by_key:\\n            retval[key] = arg\\n            preceded_by_key = False\\n\\n    return retval', 'def clear_mpi_env_vars():\\n    \"\"\"\\n    from mpi4py import MPI will call MPI_Init by default.  If the child process has MPI environment variables, MPI will think that the child process is an MPI process just like the parent and do bad things such as hang.\\n    This context manager is a hacky way to clear those environment variables temporarily such as when we are starting multiprocessing\\n    Processes.\\n    \"\"\"\\n    removed_environment = {}\\n    for k, v in list(os.environ.items()):\\n        for prefix in [\\'OMPI_\\', \\'PMI_\\']:\\n            if k.startswith(prefix):\\n                removed_environment[k] = v\\n                del os.environ[k]\\n    try:\\n        yield\\n    finally:\\n        os.environ.update(removed_environment)', 'def cg(f_Ax, b, cg_iters=10, callback=None, verbose=False, residual_tol=1e-10):\\n    \"\"\"\\n    Demmel p 312\\n    \"\"\"\\n    p = b.copy()\\n    r = b.copy()\\n    x = np.zeros_like(b)\\n    rdotr = r.dot(r)\\n\\n    fmtstr =  \"%10i %10.3g %10.3g\"\\n    titlestr =  \"%10s %10s %10s\"\\n    if verbose: print(titlestr % (\"iter\", \"residual norm\", \"soln norm\"))\\n\\n    for i in range(cg_iters):\\n        if callback is not None:\\n            callback(x)\\n        if verbose: print(fmtstr % (i, rdotr, np.linalg.norm(x)))\\n        z = f_Ax(p)\\n        v = rdotr / p.dot(z)\\n        x += v*p\\n        r -= v*z\\n        newrdotr = r.dot(r)\\n        mu = newrdotr/rdotr\\n        p = r + mu*p\\n\\n        rdotr = newrdotr\\n        if rdotr < residual_tol:\\n            break\\n\\n    if callback is not None:\\n        callback(x)\\n    if verbose: print(fmtstr % (i+1, rdotr, np.linalg.norm(x)))  # pylint: disable=W0631\\n    return x', \"def observation_placeholder(ob_space, batch_size=None, name='Ob'):\\n    '''\\n    Create placeholder to feed observations into of the size appropriate to the observation space\\n\\n    Parameters:\\n    ----------\\n\\n    ob_space: gym.Space     observation space\\n\\n    batch_size: int         size of the batch to be fed into input. Can be left None in most cases.\\n\\n    name: str               name of the placeholder\\n\\n    Returns:\\n    -------\\n\\n    tensorflow placeholder tensor\\n    '''\\n\\n    assert isinstance(ob_space, Discrete) or isinstance(ob_space, Box) or isinstance(ob_space, MultiDiscrete), \\\\\\n        'Can only deal with Discrete and Box observation spaces for now'\\n\\n    dtype = ob_space.dtype\\n    if dtype == np.int8:\\n        dtype = np.uint8\\n\\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)\", \"def observation_input(ob_space, batch_size=None, name='Ob'):\\n    '''\\n    Create placeholder to feed observations into of the size appropriate to the observation space, and add input\\n    encoder of the appropriate type.\\n    '''\\n\\n    placeholder = observation_placeholder(ob_space, batch_size, name)\\n    return placeholder, encode_observation(ob_space, placeholder)\", \"def encode_observation(ob_space, placeholder):\\n    '''\\n    Encode input in the way that is appropriate to the observation space\\n\\n    Parameters:\\n    ----------\\n\\n    ob_space: gym.Space             observation space\\n\\n    placeholder: tf.placeholder     observation input placeholder\\n    '''\\n    if isinstance(ob_space, Discrete):\\n        return tf.to_float(tf.one_hot(placeholder, ob_space.n))\\n    elif isinstance(ob_space, Box):\\n        return tf.to_float(placeholder)\\n    elif isinstance(ob_space, MultiDiscrete):\\n        placeholder = tf.cast(placeholder, tf.int32)\\n        one_hots = [tf.to_float(tf.one_hot(placeholder[..., i], ob_space.nvec[i])) for i in range(placeholder.shape[-1])]\\n        return tf.concat(one_hots, axis=-1)\\n    else:\\n        raise NotImplementedError\", 'def save_policy(self, path):\\n        \"\"\"Pickles the current policy for later inspection.\\n        \"\"\"\\n        with open(path, \\'wb\\') as f:\\n            pickle.dump(self.policy, f)', 'def logs(self, prefix=\\'worker\\'):\\n        \"\"\"Generates a dictionary that contains all collected statistics.\\n        \"\"\"\\n        logs = []\\n        logs += [(\\'success_rate\\', np.mean(self.success_history))]\\n        if self.compute_Q:\\n            logs += [(\\'mean_Q\\', np.mean(self.Q_history))]\\n        logs += [(\\'episode\\', self.n_episodes)]\\n\\n        if prefix != \\'\\' and not prefix.endswith(\\'/\\'):\\n            return [(prefix + \\'/\\' + key, val) for key, val in logs]\\n        else:\\n            return logs', \"def smooth(y, radius, mode='two_sided', valid_only=False):\\n    '''\\n    Smooth signal y, where radius is determines the size of the window\\n\\n    mode='twosided':\\n        average over the window [max(index - radius, 0), min(index + radius, len(y)-1)]\\n    mode='causal':\\n        average over the window [max(index - radius, 0), index]\\n\\n    valid_only: put nan in entries where the full-sized window is not available\\n\\n    '''\\n    assert mode in ('two_sided', 'causal')\\n    if len(y) < 2*radius+1:\\n        return np.ones_like(y) * y.mean()\\n    elif mode == 'two_sided':\\n        convkernel = np.ones(2 * radius+1)\\n        out = np.convolve(y, convkernel,mode='same') / np.convolve(np.ones_like(y), convkernel, mode='same')\\n        if valid_only:\\n            out[:radius] = out[-radius:] = np.nan\\n    elif mode == 'causal':\\n        convkernel = np.ones(radius)\\n        out = np.convolve(y, convkernel,mode='full') / np.convolve(np.ones_like(y), convkernel, mode='full')\\n        out = out[:-radius+1]\\n        if valid_only:\\n            out[:radius] = np.nan\\n    return out\", 'def copy_obs_dict(obs):\\n    \"\"\"\\n    Deep-copy an observation dict.\\n    \"\"\"\\n    return {k: np.copy(v) for k, v in obs.items()}', 'def obs_space_info(obs_space):\\n    \"\"\"\\n    Get dict-structured information about a gym.Space.\\n\\n    Returns:\\n      A tuple (keys, shapes, dtypes):\\n        keys: a list of dict keys.\\n        shapes: a dict mapping keys to shapes.\\n        dtypes: a dict mapping keys to dtypes.\\n    \"\"\"\\n    if isinstance(obs_space, gym.spaces.Dict):\\n        assert isinstance(obs_space.spaces, OrderedDict)\\n        subspaces = obs_space.spaces\\n    else:\\n        subspaces = {None: obs_space}\\n    keys = []\\n    shapes = {}\\n    dtypes = {}\\n    for key, box in subspaces.items():\\n        keys.append(key)\\n        shapes[key] = box.shape\\n        dtypes[key] = box.dtype\\n    return keys, shapes, dtypes', 'def q_retrace(R, D, q_i, v, rho_i, nenvs, nsteps, gamma):\\n    \"\"\"\\n    Calculates q_retrace targets\\n\\n    :param R: Rewards\\n    :param D: Dones\\n    :param q_i: Q values for actions taken\\n    :param v: V values\\n    :param rho_i: Importance weight for each action\\n    :return: Q_retrace values\\n    \"\"\"\\n    rho_bar = batch_to_seq(tf.minimum(1.0, rho_i), nenvs, nsteps, True)  # list of len steps, shape [nenvs]\\n    rs = batch_to_seq(R, nenvs, nsteps, True)  # list of len steps, shape [nenvs]\\n    ds = batch_to_seq(D, nenvs, nsteps, True)  # list of len steps, shape [nenvs]\\n    q_is = batch_to_seq(q_i, nenvs, nsteps, True)\\n    vs = batch_to_seq(v, nenvs, nsteps + 1, True)\\n    v_final = vs[-1]\\n    qret = v_final\\n    qrets = []\\n    for i in range(nsteps - 1, -1, -1):\\n        check_shape([qret, ds[i], rs[i], rho_bar[i], q_is[i], vs[i]], [[nenvs]] * 6)\\n        qret = rs[i] + gamma * qret * (1.0 - ds[i])\\n        qrets.append(qret)\\n        qret = (rho_bar[i] * (qret - q_is[i])) + vs[i]\\n    qrets = qrets[::-1]\\n    qret = seq_to_batch(qrets, flat=True)\\n    return qret', 'def _subproc_worker(pipe, parent_pipe, env_fn_wrapper, obs_bufs, obs_shapes, obs_dtypes, keys):\\n    \"\"\"\\n    Control a single environment instance using IPC and\\n    shared memory.\\n    \"\"\"\\n    def _write_obs(maybe_dict_obs):\\n        flatdict = obs_to_dict(maybe_dict_obs)\\n        for k in keys:\\n            dst = obs_bufs[k].get_obj()\\n            dst_np = np.frombuffer(dst, dtype=obs_dtypes[k]).reshape(obs_shapes[k])  # pylint: disable=W0212\\n            np.copyto(dst_np, flatdict[k])\\n\\n    env = env_fn_wrapper.x()\\n    parent_pipe.close()\\n    try:\\n        while True:\\n            cmd, data = pipe.recv()\\n            if cmd == \\'reset\\':\\n                pipe.send(_write_obs(env.reset()))\\n            elif cmd == \\'step\\':\\n                obs, reward, done, info = env.step(data)\\n                if done:\\n                    obs = env.reset()\\n                pipe.send((_write_obs(obs), reward, done, info))\\n            elif cmd == \\'render\\':\\n                pipe.send(env.render(mode=\\'rgb_array\\'))\\n            elif cmd == \\'close\\':\\n                pipe.send(None)\\n                break\\n            else:\\n                raise RuntimeError(\\'Got unrecognized cmd %s\\' % cmd)\\n    except KeyboardInterrupt:\\n        print(\\'ShmemVecEnv worker: got KeyboardInterrupt\\')\\n    finally:\\n        env.close()', 'def learn(\\n    network,\\n    env,\\n    seed=None,\\n    nsteps=5,\\n    total_timesteps=int(80e6),\\n    vf_coef=0.5,\\n    ent_coef=0.01,\\n    max_grad_norm=0.5,\\n    lr=7e-4,\\n    lrschedule=\\'linear\\',\\n    epsilon=1e-5,\\n    alpha=0.99,\\n    gamma=0.99,\\n    log_interval=100,\\n    load_path=None,\\n    **network_kwargs):\\n\\n    \\'\\'\\'\\n    Main entrypoint for A2C algorithm. Train a policy with given network architecture on a given environment using a2c algorithm.\\n\\n    Parameters:\\n    -----------\\n\\n    network:            policy network architecture. Either string (mlp, lstm, lnlstm, cnn_lstm, cnn, cnn_small, conv_only - see baselines.common/models.py for full list)\\n                        specifying the standard network architecture, or a function that takes tensorflow tensor as input and returns\\n                        tuple (output_tensor, extra_feed) where output tensor is the last network layer output, extra_feed is None for feed-forward\\n                        neural nets, and extra_feed is a dictionary describing how to feed state into the network for recurrent neural nets.\\n                        See baselines.common/policies.py/lstm for more details on using recurrent nets in policies\\n\\n\\n    env:                RL environment. Should implement interface similar to VecEnv (baselines.common/vec_env) or be wrapped with DummyVecEnv (baselines.common/vec_env/dummy_vec_env.py)\\n\\n\\n    seed:               seed to make random number sequence in the alorightm reproducible. By default is None which means seed from system noise generator (not reproducible)\\n\\n    nsteps:             int, number of steps of the vectorized environment per update (i.e. batch size is nsteps * nenv where\\n                        nenv is number of environment copies simulated in parallel)\\n\\n    total_timesteps:    int, total number of timesteps to train on (default: 80M)\\n\\n    vf_coef:            float, coefficient in front of value function loss in the total loss function (default: 0.5)\\n\\n    ent_coef:           float, coeffictiant in front of the policy entropy in the total loss function (default: 0.01)\\n\\n    max_gradient_norm:  float, gradient is clipped to have global L2 norm no more than this value (default: 0.5)\\n\\n    lr:                 float, learning rate for RMSProp (current implementation has RMSProp hardcoded in) (default: 7e-4)\\n\\n    lrschedule:         schedule of learning rate. Can be \\'linear\\', \\'constant\\', or a function [0..1] -> [0..1] that takes fraction of the training progress as input and\\n                        returns fraction of the learning rate (specified as lr) as output\\n\\n    epsilon:            float, RMSProp epsilon (stabilizes square root computation in denominator of RMSProp update) (default: 1e-5)\\n\\n    alpha:              float, RMSProp decay parameter (default: 0.99)\\n\\n    gamma:              float, reward discounting parameter (default: 0.99)\\n\\n    log_interval:       int, specifies how frequently the logs are printed out (default: 100)\\n\\n    **network_kwargs:   keyword arguments to the policy / network builder. See baselines.common/policies.py/build_policy and arguments to a particular type of network\\n                        For instance, \\'mlp\\' network architecture has arguments num_hidden and num_layers.\\n\\n    \\'\\'\\'\\n\\n\\n\\n    set_global_seeds(seed)\\n\\n    # Get the nb of env\\n    nenvs = env.num_envs\\n    policy = build_policy(env, network, **network_kwargs)\\n\\n    # Instantiate the model object (that creates step_model and train_model)\\n    model = Model(policy=policy, env=env, nsteps=nsteps, ent_coef=ent_coef, vf_coef=vf_coef,\\n        max_grad_norm=max_grad_norm, lr=lr, alpha=alpha, epsilon=epsilon, total_timesteps=total_timesteps, lrschedule=lrschedule)\\n    if load_path is not None:\\n        model.load(load_path)\\n\\n    # Instantiate the runner object\\n    runner = Runner(env, model, nsteps=nsteps, gamma=gamma)\\n    epinfobuf = deque(maxlen=100)\\n\\n    # Calculate the batch_size\\n    nbatch = nenvs*nsteps\\n\\n    # Start total timer\\n    tstart = time.time()\\n\\n    for update in range(1, total_timesteps//nbatch+1):\\n        # Get mini batch of experiences\\n        obs, states, rewards, masks, actions, values, epinfos = runner.run()\\n        epinfobuf.extend(epinfos)\\n\\n        policy_loss, value_loss, policy_entropy = model.train(obs, states, rewards, masks, actions, values)\\n        nseconds = time.time()-tstart\\n\\n        # Calculate the fps (frame per second)\\n        fps = int((update*nbatch)/nseconds)\\n        if update % log_interval == 0 or update == 1:\\n            # Calculates if value function is a good predicator of the returns (ev > 1)\\n            # or if it\\'s just worse than predicting nothing (ev =< 0)\\n            ev = explained_variance(values, rewards)\\n            logger.record_tabular(\"nupdates\", update)\\n            logger.record_tabular(\"total_timesteps\", update*nbatch)\\n            logger.record_tabular(\"fps\", fps)\\n            logger.record_tabular(\"policy_entropy\", float(policy_entropy))\\n            logger.record_tabular(\"value_loss\", float(value_loss))\\n            logger.record_tabular(\"explained_variance\", float(ev))\\n            logger.record_tabular(\"eprewmean\", safemean([epinfo[\\'r\\'] for epinfo in epinfobuf]))\\n            logger.record_tabular(\"eplenmean\", safemean([epinfo[\\'l\\'] for epinfo in epinfobuf]))\\n            logger.dump_tabular()\\n    return model', 'def sf01(arr):\\n    \"\"\"\\n    swap and then flatten axes 0 and 1\\n    \"\"\"\\n    s = arr.shape\\n    return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])', 'def pretty_eta(seconds_left):\\n    \"\"\"Print the number of seconds in human readable format.\\n\\n    Examples:\\n    2 days\\n    2 hours and 37 minutes\\n    less than a minute\\n\\n    Paramters\\n    ---------\\n    seconds_left: int\\n        Number of seconds to be converted to the ETA\\n    Returns\\n    -------\\n    eta: str\\n        String representing the pretty ETA.\\n    \"\"\"\\n    minutes_left = seconds_left // 60\\n    seconds_left %= 60\\n    hours_left = minutes_left // 60\\n    minutes_left %= 60\\n    days_left = hours_left // 24\\n    hours_left %= 24\\n\\n    def helper(cnt, name):\\n        return \"{} {}{}\".format(str(cnt), name, (\\'s\\' if cnt > 1 else \\'\\'))\\n\\n    if days_left > 0:\\n        msg = helper(days_left, \\'day\\')\\n        if hours_left > 0:\\n            msg += \\' and \\' + helper(hours_left, \\'hour\\')\\n        return msg\\n    if hours_left > 0:\\n        msg = helper(hours_left, \\'hour\\')\\n        if minutes_left > 0:\\n            msg += \\' and \\' + helper(minutes_left, \\'minute\\')\\n        return msg\\n    if minutes_left > 0:\\n        return helper(minutes_left, \\'minute\\')\\n    return \\'less than a minute\\'', 'def boolean_flag(parser, name, default=False, help=None):\\n    \"\"\"Add a boolean flag to argparse parser.\\n\\n    Parameters\\n    ----------\\n    parser: argparse.Parser\\n        parser to add the flag to\\n    name: str\\n        --<name> will enable the flag, while --no-<name> will disable it\\n    default: bool or None\\n        default value of the flag\\n    help: str\\n        help string for the flag\\n    \"\"\"\\n    dest = name.replace(\\'-\\', \\'_\\')\\n    parser.add_argument(\"--\" + name, action=\"store_true\", default=default, dest=dest, help=help)\\n    parser.add_argument(\"--no-\" + name, action=\"store_false\", dest=dest)', 'def get_wrapper_by_name(env, classname):\\n    \"\"\"Given an a gym environment possibly wrapped multiple times, returns a wrapper\\n    of class named classname or raises ValueError if no such wrapper was applied\\n\\n    Parameters\\n    ----------\\n    env: gym.Env of gym.Wrapper\\n        gym environment\\n    classname: str\\n        name of the wrapper\\n\\n    Returns\\n    -------\\n    wrapper: gym.Wrapper\\n        wrapper named classname\\n    \"\"\"\\n    currentenv = env\\n    while True:\\n        if classname == currentenv.class_name():\\n            return currentenv\\n        elif isinstance(currentenv, gym.Wrapper):\\n            currentenv = currentenv.env\\n        else:\\n            raise ValueError(\"Couldn\\'t find wrapper named %s\" % classname)', 'def pickle_load(path, compression=False):\\n    \"\"\"Unpickle a possible compressed pickle.\\n\\n    Parameters\\n    ----------\\n    path: str\\n        path to the output file\\n    compression: bool\\n        if true assumes that pickle was compressed when created and attempts decompression.\\n\\n    Returns\\n    -------\\n    obj: object\\n        the unpickled object\\n    \"\"\"\\n\\n    if compression:\\n        with zipfile.ZipFile(path, \"r\", compression=zipfile.ZIP_DEFLATED) as myzip:\\n            with myzip.open(\"data\") as f:\\n                return pickle.load(f)\\n    else:\\n        with open(path, \"rb\") as f:\\n            return pickle.load(f)', 'def update(self, new_val):\\n        \"\"\"Update the estimate.\\n\\n        Parameters\\n        ----------\\n        new_val: float\\n            new observated value of estimated quantity.\\n        \"\"\"\\n        if self._value is None:\\n            self._value = new_val\\n        else:\\n            self._value = self._gamma * self._value + (1.0 - self._gamma) * new_val', 'def store_args(method):\\n    \"\"\"Stores provided method args as instance attributes.\\n    \"\"\"\\n    argspec = inspect.getfullargspec(method)\\n    defaults = {}\\n    if argspec.defaults is not None:\\n        defaults = dict(\\n            zip(argspec.args[-len(argspec.defaults):], argspec.defaults))\\n    if argspec.kwonlydefaults is not None:\\n        defaults.update(argspec.kwonlydefaults)\\n    arg_names = argspec.args[1:]\\n\\n    @functools.wraps(method)\\n    def wrapper(*positional_args, **keyword_args):\\n        self = positional_args[0]\\n        # Get default arg values\\n        args = defaults.copy()\\n        # Add provided arg values\\n        for name, value in zip(arg_names, positional_args[1:]):\\n            args[name] = value\\n        args.update(keyword_args)\\n        self.__dict__.update(args)\\n        return method(*positional_args, **keyword_args)\\n\\n    return wrapper', 'def flatten_grads(var_list, grads):\\n    \"\"\"Flattens a variables and their gradients.\\n    \"\"\"\\n    return tf.concat([tf.reshape(grad, [U.numel(v)])\\n                      for (v, grad) in zip(var_list, grads)], 0)', 'def nn(input, layers_sizes, reuse=None, flatten=False, name=\"\"):\\n    \"\"\"Creates a simple neural network\\n    \"\"\"\\n    for i, size in enumerate(layers_sizes):\\n        activation = tf.nn.relu if i < len(layers_sizes) - 1 else None\\n        input = tf.layers.dense(inputs=input,\\n                                units=size,\\n                                kernel_initializer=tf.contrib.layers.xavier_initializer(),\\n                                reuse=reuse,\\n                                name=name + \\'_\\' + str(i))\\n        if activation:\\n            input = activation(input)\\n    if flatten:\\n        assert layers_sizes[-1] == 1\\n        input = tf.reshape(input, [-1])\\n    return input', 'def mpi_fork(n, extra_mpi_args=[]):\\n    \"\"\"Re-launches the current script with workers\\n    Returns \"parent\" for original parent, \"child\" for MPI children\\n    \"\"\"\\n    if n <= 1:\\n        return \"child\"\\n    if os.getenv(\"IN_MPI\") is None:\\n        env = os.environ.copy()\\n        env.update(\\n            MKL_NUM_THREADS=\"1\",\\n            OMP_NUM_THREADS=\"1\",\\n            IN_MPI=\"1\"\\n        )\\n        # \"-bind-to core\" is crucial for good performance\\n        args = [\"mpirun\", \"-np\", str(n)] + \\\\\\n            extra_mpi_args + \\\\\\n            [sys.executable]\\n\\n        args += sys.argv\\n        subprocess.check_call(args, env=env)\\n        return \"parent\"\\n    else:\\n        install_mpi_excepthook()\\n        return \"child\"', 'def get_session(config=None):\\n    \"\"\"Get default session or create one with a given config\"\"\"\\n    sess = tf.get_default_session()\\n    if sess is None:\\n        sess = make_session(config=config, make_default=True)\\n    return sess', 'def initialize():\\n    \"\"\"Initialize all the uninitialized variables in the global scope.\"\"\"\\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\\n    get_session().run(tf.variables_initializer(new_variables))\\n    ALREADY_INITIALIZED.update(new_variables)', \"def adjust_shape(placeholder, data):\\n    '''\\n    adjust shape of the data to the shape of the placeholder if possible.\\n    If shape is incompatible, AssertionError is thrown\\n\\n    Parameters:\\n        placeholder     tensorflow input placeholder\\n\\n        data            input data to be (potentially) reshaped to be fed into placeholder\\n\\n    Returns:\\n        reshaped data\\n    '''\\n\\n    if not isinstance(data, np.ndarray) and not isinstance(data, list):\\n        return data\\n    if isinstance(data, list):\\n        data = np.array(data)\\n\\n    placeholder_shape = [x or -1 for x in placeholder.shape.as_list()]\\n\\n    assert _check_shape(placeholder_shape, data.shape), \\\\\\n        'Shape of data {} is not compatible with shape of the placeholder {}'.format(data.shape, placeholder_shape)\\n\\n    return np.reshape(data, placeholder_shape)\", 'def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):\\n    \"\"\"Configure environment for DeepMind-style Atari.\\n    \"\"\"\\n    if episode_life:\\n        env = EpisodicLifeEnv(env)\\n    if \\'FIRE\\' in env.unwrapped.get_action_meanings():\\n        env = FireResetEnv(env)\\n    env = WarpFrame(env)\\n    if scale:\\n        env = ScaledFloatFrame(env)\\n    if clip_rewards:\\n        env = ClipRewardEnv(env)\\n    if frame_stack:\\n        env = FrameStack(env, 4)\\n    return env', 'def reset(self, **kwargs):\\n        \"\"\"Reset only when lives are exhausted.\\n        This way all states are still reachable even though lives are episodic,\\n        and the learner need not know about any of this behind-the-scenes.\\n        \"\"\"\\n        if self.was_real_done:\\n            obs = self.env.reset(**kwargs)\\n        else:\\n            # no-op step to advance from terminal/lost life state\\n            obs, _, _, _ = self.env.step(0)\\n        self.lives = self.env.unwrapped.ale.lives()\\n        return obs', 'def gpu_count():\\n    \"\"\"\\n    Count the GPUs on this machine.\\n    \"\"\"\\n    if shutil.which(\\'nvidia-smi\\') is None:\\n        return 0\\n    output = subprocess.check_output([\\'nvidia-smi\\', \\'--query-gpu=gpu_name\\', \\'--format=csv\\'])\\n    return max(0, len(output.split(b\\'\\\\n\\')) - 2)', 'def setup_mpi_gpus():\\n    \"\"\"\\n    Set CUDA_VISIBLE_DEVICES to MPI rank if not already set\\n    \"\"\"\\n    if \\'CUDA_VISIBLE_DEVICES\\' not in os.environ:\\n        if sys.platform == \\'darwin\\': # This Assumes if you\\'re on OSX you\\'re just\\n            ids = []                 # doing a smoke test and don\\'t want GPUs\\n        else:\\n            lrank, _lsize = get_local_rank_size(MPI.COMM_WORLD)\\n            ids = [lrank]\\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(map(str, ids))', 'def get_local_rank_size(comm):\\n    \"\"\"\\n    Returns the rank of each process on its machine\\n    The processes on a given machine will be assigned ranks\\n        0, 1, 2, ..., N-1,\\n    where N is the number of processes on this machine.\\n\\n    Useful if you want to assign one gpu per machine\\n    \"\"\"\\n    this_node = platform.node()\\n    ranks_nodes = comm.allgather((comm.Get_rank(), this_node))\\n    node2rankssofar = defaultdict(int)\\n    local_rank = None\\n    for (rank, node) in ranks_nodes:\\n        if rank == comm.Get_rank():\\n            local_rank = node2rankssofar[node]\\n        node2rankssofar[node] += 1\\n    assert local_rank is not None\\n    return local_rank, node2rankssofar[this_node]', 'def share_file(comm, path):\\n    \"\"\"\\n    Copies the file from rank 0 to all other ranks\\n    Puts it in the same place on all machines\\n    \"\"\"\\n    localrank, _ = get_local_rank_size(comm)\\n    if comm.Get_rank() == 0:\\n        with open(path, \\'rb\\') as fh:\\n            data = fh.read()\\n        comm.bcast(data)\\n    else:\\n        data = comm.bcast(None)\\n        if localrank == 0:\\n            os.makedirs(os.path.dirname(path), exist_ok=True)\\n            with open(path, \\'wb\\') as fh:\\n                fh.write(data)\\n    comm.Barrier()', 'def dict_gather(comm, d, op=\\'mean\\', assert_all_have_data=True):\\n    \"\"\"\\n    Perform a reduction operation over dicts\\n    \"\"\"\\n    if comm is None: return d\\n    alldicts = comm.allgather(d)\\n    size = comm.size\\n    k2li = defaultdict(list)\\n    for d in alldicts:\\n        for (k,v) in d.items():\\n            k2li[k].append(v)\\n    result = {}\\n    for (k,li) in k2li.items():\\n        if assert_all_have_data:\\n            assert len(li)==size, \"only %i out of %i MPI workers have sent \\'%s\\'\" % (len(li), size, k)\\n        if op==\\'mean\\':\\n            result[k] = np.mean(li, axis=0)\\n        elif op==\\'sum\\':\\n            result[k] = np.sum(li, axis=0)\\n        else:\\n            assert 0, op\\n    return result', 'def discount(x, gamma):\\n    \"\"\"\\n    computes discounted sums along 0th dimension of x.\\n\\n    inputs\\n    ------\\n    x: ndarray\\n    gamma: float\\n\\n    outputs\\n    -------\\n    y: ndarray with same shape as x, satisfying\\n\\n        y[t] = x[t] + gamma*x[t+1] + gamma^2*x[t+2] + ... + gamma^k x[t+k],\\n                where k = len(x) - t - 1\\n\\n    \"\"\"\\n    assert x.ndim >= 1\\n    return scipy.signal.lfilter([1],[1,-gamma],x[::-1], axis=0)[::-1]', 'def add(self, *args, **kwargs):\\n        \"\"\"See ReplayBuffer.store_effect\"\"\"\\n        idx = self._next_idx\\n        super().add(*args, **kwargs)\\n        self._it_sum[idx] = self._max_priority ** self._alpha\\n        self._it_min[idx] = self._max_priority ** self._alpha']}\n",
      "<class 'datasets.formatting.formatting.LazyBatch'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797], 'query_id': ['q254604', 'q254605', 'q254606', 'q254607', 'q254608', 'q254609', 'q254610', 'q254611', 'q254612', 'q254613', 'q254614', 'q254615', 'q254616', 'q254617', 'q254618', 'q254619', 'q254620', 'q254621', 'q254622', 'q254623', 'q254624', 'q254625', 'q254626', 'q254627', 'q254628', 'q254629', 'q254630', 'q254631', 'q254632', 'q254633', 'q254634', 'q254635', 'q254636', 'q254637', 'q254638', 'q254639', 'q254640', 'q254641', 'q254642', 'q254643'], 'corpus_id': ['c254578', 'c254579', 'c254580', 'c254581', 'c254582', 'c254583', 'c254584', 'c254585', 'c254586', 'c254587', 'c254588', 'c254589', 'c254590', 'c254591', 'c254592', 'c254593', 'c254594', 'c254595', 'c254596', 'c254597', 'c254598', 'c254599', 'c254600', 'c254601', 'c254602', 'c254603', 'c254604', 'c254605', 'c254606', 'c254607', 'c254608', 'c254609', 'c254610', 'c254611', 'c254612', 'c254613', 'c254614', 'c254615', 'c254616', 'c254617'], 'score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'doc': ['Checks if any timeout for the requests to DigitalOcean is required.\\n            To set a timeout, use the REQUEST_TIMEOUT_ENV_VAR environment\\n            variable.', 'Class method that will return an Volume object by ID.', \"Creates a Block Storage volume\\n\\n        Note: Every argument and parameter given to this method will be\\n        assigned to the object.\\n\\n        Args:\\n            name: string - a name for the volume\\n            snapshot_id: string - unique identifier for the volume snapshot\\n            size_gigabytes: int - size of the Block Storage volume in GiB\\n            filesystem_type: string, optional - name of the filesystem type the\\n                volume will be formated with ('ext4' or 'xfs')\\n            filesystem_label: string, optional - the label to be applied to the\\n                filesystem, only used in conjunction with filesystem_type\\n\\n        Optional Args:\\n            description: string - text field to describe a volume\", 'Attach a Volume to a Droplet.\\n\\n        Args:\\n            droplet_id: int - droplet id\\n            region: string - slug identifier for the region', 'Detach a Volume to a Droplet.\\n\\n        Args:\\n            size_gigabytes: int - size of the Block Storage volume in GiB\\n            region: string - slug identifier for the region', 'Create a snapshot of the volume.\\n\\n        Args:\\n            name: string - a human-readable name for the snapshot', 'Retrieve the list of snapshots that have been created from a volume.\\n\\n        Args:', 'Class method that will return a Certificate object by its ID.', 'Load the Certificate object from DigitalOcean.\\n\\n            Requires self.id to be set.', 'Create the Certificate', 'Class method that will return an Image object by ID or slug.\\n\\n            This method is used to validate the type of the image. If it is a\\n            number, it will be considered as an Image ID, instead if it is a\\n            string, it will considered as slug.', 'Creates a new custom DigitalOcean Image from the Linux virtual machine\\n        image located at the provided `url`.', 'Load slug.\\n\\n            Loads by id, or by slug if id is not present or use slug is True.', 'Transfer the image', 'Rename an image', 'Convert transposed convolution layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert sum.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert reduce_sum layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert concatenation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert slice operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert clip operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert elementwise addition.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert elementwise subtraction.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert Linear.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert matmul layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert constant layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert transpose layer.\\n\\n   Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert reshape layer.\\n\\n   Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert squeeze operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert unsqueeze operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert shape operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert Average pooling.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert 3d Max pooling.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert convert_adaptive_max_pool2d layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert padding layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert batch normalization layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert instance normalization layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert dropout.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert relu layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers', 'Convert leaky relu layer.\\n\\n   Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers'], 'code': ['def get_timeout(self):\\n        \"\"\"\\n            Checks if any timeout for the requests to DigitalOcean is required.\\n            To set a timeout, use the REQUEST_TIMEOUT_ENV_VAR environment\\n            variable.\\n        \"\"\"\\n        timeout_str = os.environ.get(REQUEST_TIMEOUT_ENV_VAR)\\n        if timeout_str:\\n            try:\\n                return float(timeout_str)\\n            except:\\n                self._log.error(\\'Failed parsing the request read timeout of \\'\\n                                \\'\"%s\". Please use a valid float number!\\' %\\n                                        timeout_str)\\n        return None', 'def get_object(cls, api_token, volume_id):\\n        \"\"\"\\n        Class method that will return an Volume object by ID.\\n        \"\"\"\\n        volume = cls(token=api_token, id=volume_id)\\n        volume.load()\\n        return volume', 'def create_from_snapshot(self, *args, **kwargs):\\n        \"\"\"\\n        Creates a Block Storage volume\\n\\n        Note: Every argument and parameter given to this method will be\\n        assigned to the object.\\n\\n        Args:\\n            name: string - a name for the volume\\n            snapshot_id: string - unique identifier for the volume snapshot\\n            size_gigabytes: int - size of the Block Storage volume in GiB\\n            filesystem_type: string, optional - name of the filesystem type the\\n                volume will be formated with (\\'ext4\\' or \\'xfs\\')\\n            filesystem_label: string, optional - the label to be applied to the\\n                filesystem, only used in conjunction with filesystem_type\\n\\n        Optional Args:\\n            description: string - text field to describe a volume\\n        \"\"\"\\n        data = self.get_data(\\'volumes/\\',\\n                             type=POST,\\n                             params={\\'name\\': self.name,\\n                                     \\'snapshot_id\\': self.snapshot_id,\\n                                     \\'region\\': self.region,\\n                                     \\'size_gigabytes\\': self.size_gigabytes,\\n                                     \\'description\\': self.description,\\n                                     \\'filesystem_type\\': self.filesystem_type,\\n                                     \\'filesystem_label\\': self.filesystem_label\\n                                     })\\n\\n        if data:\\n            self.id = data[\\'volume\\'][\\'id\\']\\n            self.created_at = data[\\'volume\\'][\\'created_at\\']\\n\\n        return self', 'def attach(self, droplet_id, region):\\n        \"\"\"\\n        Attach a Volume to a Droplet.\\n\\n        Args:\\n            droplet_id: int - droplet id\\n            region: string - slug identifier for the region\\n        \"\"\"\\n        return self.get_data(\\n            \"volumes/%s/actions/\" % self.id,\\n            type=POST,\\n            params={\"type\": \"attach\",\\n                    \"droplet_id\": droplet_id,\\n                    \"region\": region}\\n        )', 'def resize(self, size_gigabytes, region):\\n        \"\"\"\\n        Detach a Volume to a Droplet.\\n\\n        Args:\\n            size_gigabytes: int - size of the Block Storage volume in GiB\\n            region: string - slug identifier for the region\\n        \"\"\"\\n        return self.get_data(\\n            \"volumes/%s/actions/\" % self.id,\\n            type=POST,\\n            params={\"type\": \"resize\",\\n                    \"size_gigabytes\": size_gigabytes,\\n                    \"region\": region}\\n        )', 'def snapshot(self, name):\\n        \"\"\"\\n        Create a snapshot of the volume.\\n\\n        Args:\\n            name: string - a human-readable name for the snapshot\\n        \"\"\"\\n        return self.get_data(\\n            \"volumes/%s/snapshots/\" % self.id,\\n            type=POST,\\n            params={\"name\": name}\\n        )', 'def get_snapshots(self):\\n        \"\"\"\\n        Retrieve the list of snapshots that have been created from a volume.\\n\\n        Args:\\n        \"\"\"\\n        data = self.get_data(\"volumes/%s/snapshots/\" % self.id)\\n        snapshots = list()\\n        for jsond in data[u\\'snapshots\\']:\\n            snapshot = Snapshot(**jsond)\\n            snapshot.token = self.token\\n            snapshots.append(snapshot)\\n\\n        return snapshots', 'def get_object(cls, api_token, cert_id):\\n        \"\"\"\\n            Class method that will return a Certificate object by its ID.\\n        \"\"\"\\n        certificate = cls(token=api_token, id=cert_id)\\n        certificate.load()\\n        return certificate', 'def load(self):\\n        \"\"\"\\n            Load the Certificate object from DigitalOcean.\\n\\n            Requires self.id to be set.\\n        \"\"\"\\n        data = self.get_data(\"certificates/%s\" % self.id)\\n        certificate = data[\"certificate\"]\\n\\n        for attr in certificate.keys():\\n            setattr(self, attr, certificate[attr])\\n\\n        return self', 'def create(self):\\n        \"\"\"\\n            Create the Certificate\\n        \"\"\"\\n        params = {\\n            \"name\": self.name,\\n            \"type\": self.type,\\n            \"dns_names\": self.dns_names,\\n            \"private_key\": self.private_key,\\n            \"leaf_certificate\": self.leaf_certificate,\\n            \"certificate_chain\": self.certificate_chain\\n        }\\n\\n        data = self.get_data(\"certificates/\", type=POST, params=params)\\n\\n        if data:\\n            self.id = data[\\'certificate\\'][\\'id\\']\\n            self.not_after = data[\\'certificate\\'][\\'not_after\\']\\n            self.sha1_fingerprint = data[\\'certificate\\'][\\'sha1_fingerprint\\']\\n            self.created_at = data[\\'certificate\\'][\\'created_at\\']\\n            self.type = data[\\'certificate\\'][\\'type\\']\\n            self.dns_names = data[\\'certificate\\'][\\'dns_names\\']\\n            self.state = data[\\'certificate\\'][\\'state\\']\\n\\n        return self', 'def get_object(cls, api_token, image_id_or_slug):\\n        \"\"\"\\n            Class method that will return an Image object by ID or slug.\\n\\n            This method is used to validate the type of the image. If it is a\\n            number, it will be considered as an Image ID, instead if it is a\\n            string, it will considered as slug.\\n        \"\"\"\\n        if cls._is_string(image_id_or_slug):\\n            image = cls(token=api_token, slug=image_id_or_slug)\\n            image.load(use_slug=True)\\n        else:\\n            image = cls(token=api_token, id=image_id_or_slug)\\n            image.load()\\n        return image', 'def create(self):\\n        \"\"\"\\n        Creates a new custom DigitalOcean Image from the Linux virtual machine\\n        image located at the provided `url`.\\n        \"\"\"\\n        params = {\\'name\\': self.name,\\n                  \\'region\\': self.region,\\n                  \\'url\\': self.url,\\n                  \\'distribution\\': self.distribution,\\n                  \\'description\\': self.description,\\n                  \\'tags\\': self.tags}\\n\\n        data = self.get_data(\\'images\\', type=POST, params=params)\\n\\n        if data:\\n            for attr in data[\\'image\\'].keys():\\n                setattr(self, attr, data[\\'image\\'][attr])\\n\\n        return self', 'def load(self, use_slug=False):\\n        \"\"\"\\n            Load slug.\\n\\n            Loads by id, or by slug if id is not present or use slug is True.\\n        \"\"\"\\n        identifier = None\\n        if use_slug or not self.id:\\n            identifier = self.slug\\n        else:\\n            identifier = self.id\\n        if not identifier:\\n            raise NotFoundError(\"One of self.id or self.slug must be set.\")\\n        data = self.get_data(\"images/%s\" % identifier)\\n        image_dict = data[\\'image\\']\\n\\n        # Setting the attribute values\\n        for attr in image_dict.keys():\\n            setattr(self, attr, image_dict[attr])\\n\\n        return self', 'def transfer(self, new_region_slug):\\n        \"\"\"\\n            Transfer the image\\n        \"\"\"\\n        return self.get_data(\\n            \"images/%s/actions/\" % self.id,\\n            type=POST,\\n            params={\"type\": \"transfer\", \"region\": new_region_slug}\\n        )', 'def rename(self, new_name):\\n        \"\"\"\\n            Rename an image\\n        \"\"\"\\n        return self.get_data(\\n            \"images/%s\" % self.id,\\n            type=PUT,\\n            params={\"name\": new_name}\\n        )', 'def convert_convtranspose(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert transposed convolution layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting transposed convolution ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'C\\' + random_string(7)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    bias_name = \\'{0}.bias\\'.format(w_name)\\n    weights_name = \\'{0}.weight\\'.format(w_name)\\n\\n    if len(weights[weights_name].numpy().shape) == 4:\\n        W = weights[weights_name].numpy().transpose(2, 3, 1, 0)\\n        height, width, n_filters, channels = W.shape\\n\\n        n_groups = params[\\'group\\']\\n        if n_groups > 1:\\n            raise AssertionError(\\'Cannot convert conv1d with groups != 1\\')\\n\\n        if params[\\'dilations\\'][0] > 1:\\n            raise AssertionError(\\'Cannot convert conv1d with dilation_rate != 1\\')\\n\\n        if bias_name in weights:\\n            biases = weights[bias_name].numpy()\\n            has_bias = True\\n        else:\\n            biases = None\\n            has_bias = False\\n\\n        input_name = inputs[0]\\n\\n        if has_bias:\\n            weights = [W, biases]\\n        else:\\n            weights = [W]\\n\\n        conv = keras.layers.Conv2DTranspose(\\n            filters=n_filters,\\n            kernel_size=(height, width),\\n            strides=(params[\\'strides\\'][0], params[\\'strides\\'][1]),\\n            padding=\\'valid\\',\\n            output_padding=0,\\n            weights=weights,\\n            use_bias=has_bias,\\n            activation=None,\\n            dilation_rate=params[\\'dilations\\'][0],\\n            bias_initializer=\\'zeros\\', kernel_initializer=\\'zeros\\',\\n            name=tf_name\\n        )\\n\\n        layers[scope_name] = conv(layers[input_name])\\n\\n        # Magic ad-hoc.\\n        # See the Keras issue: https://github.com/keras-team/keras/issues/6777\\n        layers[scope_name].set_shape(layers[scope_name]._keras_shape)\\n\\n        pads = params[\\'pads\\']\\n        if pads[0] > 0:\\n            assert(len(pads) == 2 or (pads[2] == pads[0] and pads[3] == pads[1]))\\n\\n            crop = keras.layers.Cropping2D(\\n                pads[:2],\\n                name=tf_name + \\'_crop\\'\\n            )\\n            layers[scope_name] = crop(layers[scope_name])\\n    else:\\n        raise AssertionError(\\'Layer is not supported for now\\')', 'def convert_sum(\\n    params, w_name, scope_name, inputs, layers, weights, names\\n):\\n    \"\"\"\\n    Convert sum.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting Sum ...\\')\\n\\n    def target_layer(x):\\n        import keras.backend as K\\n        return K.sum(x)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer)\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_reduce_sum(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert reduce_sum layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting reduce_sum ...\\')\\n\\n    keepdims = params[\\'keepdims\\'] > 0\\n    axis = params[\\'axes\\']\\n\\n    def target_layer(x, keepdims=keepdims, axis=axis):\\n        import keras.backend as K\\n        return K.sum(x, keepdims=keepdims, axis=axis)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer)\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_concat(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert concatenation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting concat ...\\')\\n    concat_nodes = [layers[i] for i in inputs]\\n\\n    if len(concat_nodes) == 1:\\n        # no-op\\n        layers[scope_name] = concat_nodes[0]\\n        return\\n\\n    if names == \\'short\\':\\n        tf_name = \\'CAT\\' + random_string(5)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    cat = keras.layers.Concatenate(name=tf_name, axis=params[\\'axis\\'])\\n    layers[scope_name] = cat(concat_nodes)', 'def convert_slice(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert slice operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting slice ...\\')\\n\\n    if len(params[\\'axes\\']) > 1:\\n        raise AssertionError(\\'Cannot convert slice by multiple dimensions\\')\\n\\n    if params[\\'axes\\'][0] not in [0, 1, 2, 3]:\\n        raise AssertionError(\\'Slice by dimension more than 3 or less than 0 is not supported\\')\\n\\n    def target_layer(x, axis=int(params[\\'axes\\'][0]), start=int(params[\\'starts\\'][0]), end=int(params[\\'ends\\'][0])):\\n        if axis == 0:\\n            return x[start:end]\\n        elif axis == 1:\\n            return x[:, start:end]\\n        elif axis == 2:\\n            return x[:, :, start:end]\\n        elif axis == 3:\\n            return x[:, :, :, start:end]\\n\\n    lambda_layer = keras.layers.Lambda(target_layer)\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_clip(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert clip operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting clip ...\\')\\n\\n    if params[\\'min\\'] == 0:\\n        print(\"using ReLU({0})\".format(params[\\'max\\']))\\n        layer = keras.layers.ReLU(max_value=params[\\'max\\'])\\n    else:\\n        def target_layer(x, vmin=params[\\'min\\'], vmax=params[\\'max\\']):\\n            import tensorflow as tf\\n            return tf.clip_by_value(x, vmin, vmax)\\n        layer = keras.layers.Lambda(target_layer)\\n\\n    layers[scope_name] = layer(layers[inputs[0]])', 'def convert_elementwise_add(\\n    params, w_name, scope_name, inputs, layers, weights, names\\n):\\n    \"\"\"\\n    Convert elementwise addition.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting elementwise_add ...\\')\\n    if \\'broadcast\\' in params:\\n        model0 = layers[inputs[0]]\\n        model1 = layers[inputs[1]]\\n\\n        if names == \\'short\\':\\n            tf_name = \\'A\\' + random_string(7)\\n        elif names == \\'keep\\':\\n            tf_name = w_name\\n        else:\\n            tf_name = w_name + str(random.random())\\n\\n        def target_layer(x):\\n            layer = tf.add(x[0], x[1])\\n            return layer\\n\\n        lambda_layer = keras.layers.Lambda(target_layer, name=tf_name)\\n        layers[scope_name] = lambda_layer([layers[inputs[0]], layers[inputs[1]]])\\n    else:\\n        model0 = layers[inputs[0]]\\n        model1 = layers[inputs[1]]\\n\\n        if names == \\'short\\':\\n            tf_name = \\'A\\' + random_string(7)\\n        elif names == \\'keep\\':\\n            tf_name = w_name\\n        else:\\n            tf_name = w_name + str(random.random())\\n\\n        add = keras.layers.Add(name=tf_name)\\n        layers[scope_name] = add([model0, model1])', 'def convert_elementwise_sub(\\n    params, w_name, scope_name, inputs, layers, weights, names\\n):\\n    \"\"\"\\n    Convert elementwise subtraction.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting elementwise_sub ...\\')\\n    model0 = layers[inputs[0]]\\n    model1 = layers[inputs[1]]\\n\\n    if names == \\'short\\':\\n        tf_name = \\'S\\' + random_string(7)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    sub = keras.layers.Subtract(name=tf_name)\\n    layers[scope_name] = sub([model0, model1])', 'def convert_gemm(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert Linear.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting Linear ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'FC\\' + random_string(6)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    bias_name = \\'{0}.bias\\'.format(w_name)\\n    weights_name = \\'{0}.weight\\'.format(w_name)\\n\\n    W = weights[weights_name].numpy().transpose()\\n    input_channels, output_channels = W.shape\\n\\n    keras_weights = [W]\\n    has_bias = False\\n    if bias_name in weights:\\n        bias = weights[bias_name].numpy()\\n        keras_weights = [W, bias]\\n        has_bias = True\\n\\n    dense = keras.layers.Dense(\\n        output_channels,\\n        weights=keras_weights, use_bias=has_bias, name=tf_name, bias_initializer=\\'zeros\\', kernel_initializer=\\'zeros\\',\\n    )\\n\\n    layers[scope_name] = dense(layers[inputs[0]])', 'def convert_matmul(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert matmul layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting matmul ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'MMUL\\' + random_string(4)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    if len(inputs) == 1:\\n        weights_name = \\'{0}.weight\\'.format(w_name)\\n\\n        W = weights[weights_name].numpy().transpose()\\n        input_channels, output_channels = W.shape\\n\\n        keras_weights = [W]\\n\\n        dense = keras.layers.Dense(\\n            output_channels,\\n            weights=keras_weights, use_bias=False, name=tf_name, bias_initializer=\\'zeros\\', kernel_initializer=\\'zeros\\',\\n        )\\n        layers[scope_name] = dense(layers[inputs[0]])\\n    elif len(inputs) == 2:\\n        weights_name = \\'{0}.weight\\'.format(w_name)\\n\\n        W = weights[weights_name].numpy().transpose()\\n        input_channels, output_channels = W.shape\\n\\n        keras_weights = [W]\\n\\n        dense = keras.layers.Dense(\\n            output_channels,\\n            weights=keras_weights, use_bias=False, name=tf_name, bias_initializer=\\'zeros\\', kernel_initializer=\\'zeros\\',\\n        )\\n        layers[scope_name] = dense(layers[inputs[0]])\\n    else:\\n        raise AssertionError(\\'Cannot convert matmul layer\\')', 'def convert_constant(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert constant layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting constant ...\\')\\n\\n    params_list = params[\\'value\\'].numpy()\\n\\n    def target_layer(x, value=params_list):\\n        return tf.constant(value.tolist(), shape=value.shape)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer)\\n    layers[scope_name + \\'_np\\'] = params_list  # ad-hoc\\n    layers[scope_name] = lambda_layer(layers[list(layers.keys())[0]])', 'def convert_transpose(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert transpose layer.\\n\\n   Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting transpose ...\\')\\n    if params[\\'perm\\'][0] != 0:\\n        if inputs[0] in layers:\\n            print(\\'!!! Cannot permute batch dimension. Result may be wrong !!!\\')\\n            layers[scope_name] = layers[inputs[0]]\\n        else:\\n            print(\\'Skip weight matrix transpose, result may be wrong.\\')\\n    else:\\n        if names:\\n            tf_name = \\'PERM\\' + random_string(4)\\n        else:\\n            tf_name = w_name + str(random.random())\\n        permute = keras.layers.Permute(params[\\'perm\\'][1:], name=tf_name)\\n        layers[scope_name] = permute(layers[inputs[0]])', 'def convert_reshape(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert reshape layer.\\n\\n   Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting reshape ...\\')\\n    if names == \\'short\\':\\n        tf_name = \\'RESH\\' + random_string(4)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    if len(inputs) > 1:\\n        if layers[inputs[1]][0] == -1:\\n            print(\\'Cannot deduct batch size! It will be omitted, but result may be wrong.\\')\\n\\n        reshape = keras.layers.Reshape(layers[inputs[1] + \\'_np\\'], name=tf_name)\\n        layers[scope_name] = reshape(layers[inputs[0]])\\n    else:\\n        if inputs[0] in layers:\\n            reshape = keras.layers.Reshape(params[\\'shape\\'][1:], name=tf_name)\\n            layers[scope_name] = reshape(layers[inputs[0]])\\n        else:\\n            print(\\'Skip weight matrix transpose, but result may be wrong.\\')', 'def convert_squeeze(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert squeeze operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting squeeze ...\\')\\n\\n    if len(params[\\'axes\\']) > 1:\\n        raise AssertionError(\\'Cannot convert squeeze by multiple dimensions\\')\\n\\n    def target_layer(x, axis=int(params[\\'axes\\'][0])):\\n        import tensorflow as tf\\n        return tf.squeeze(x, axis=axis)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer)\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_unsqueeze(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert unsqueeze operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting unsqueeze ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'UNSQ\\' + random_string(4)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    def target_layer(x):\\n        import keras\\n        return keras.backend.expand_dims(x)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer, name=tf_name + \\'E\\')\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_shape(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert shape operation.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting shape ...\\')\\n\\n    def target_layer(x):\\n        import tensorflow as tf\\n        return tf.shape(x)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer)\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_avgpool(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert Average pooling.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting pooling ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'P\\' + random_string(7)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    if \\'kernel_shape\\' in params:\\n        height, width = params[\\'kernel_shape\\']\\n    else:\\n        height, width = params[\\'kernel_size\\']\\n\\n    if \\'strides\\' in params:\\n        stride_height, stride_width = params[\\'strides\\']\\n    else:\\n        stride_height, stride_width = params[\\'stride\\']\\n\\n    if \\'pads\\' in params:\\n        padding_h, padding_w, _, _ = params[\\'pads\\']\\n    else:\\n        padding_h, padding_w = params[\\'padding\\']\\n\\n    input_name = inputs[0]\\n    pad = \\'valid\\' \\n\\n    if height % 2 == 1 and width % 2 == 1 and \\\\\\n       height // 2 == padding_h and width // 2 == padding_w and \\\\\\n       stride_height == 1 and stride_width == 1:\\n        pad = \\'same\\'\\n    else:\\n        padding_name = tf_name + \\'_pad\\'\\n        padding_layer = keras.layers.ZeroPadding2D(\\n            padding=(padding_h, padding_w),\\n            name=padding_name\\n        )\\n        layers[padding_name] = padding_layer(layers[inputs[0]])\\n        input_name = padding_name\\n\\n    # Pooling type AveragePooling2D\\n    pooling = keras.layers.AveragePooling2D(\\n        pool_size=(height, width),\\n        strides=(stride_height, stride_width),\\n        padding=pad,\\n        name=tf_name,\\n        data_format=\\'channels_first\\'\\n    )\\n\\n    layers[scope_name] = pooling(layers[input_name])', 'def convert_maxpool3(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert 3d Max pooling.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n\\n    print(\\'Converting pooling ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'P\\' + random_string(7)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    if \\'kernel_shape\\' in params:\\n        height, width, depth = params[\\'kernel_shape\\']\\n    else:\\n        height, width, depth = params[\\'kernel_size\\']\\n\\n    if \\'strides\\' in params:\\n        stride_height, stride_width, stride_depth = params[\\'strides\\']\\n    else:\\n        stride_height, stride_width, stride_depth = params[\\'stride\\']\\n\\n    if \\'pads\\' in params:\\n        padding_h, padding_w, padding_d, _, _ = params[\\'pads\\']\\n    else:\\n        padding_h, padding_w, padding_d = params[\\'padding\\']\\n\\n    input_name = inputs[0]\\n    if padding_h > 0 and padding_w > 0 and padding_d > 0:\\n        padding_name = tf_name + \\'_pad\\'\\n        padding_layer = keras.layers.ZeroPadding3D(\\n            padding=(padding_h, padding_w, padding_d),\\n            name=padding_name\\n        )\\n        layers[padding_name] = padding_layer(layers[inputs[0]])\\n        input_name = padding_name\\n\\n    # Pooling type\\n    pooling = keras.layers.MaxPooling3D(\\n        pool_size=(height, width, depth),\\n        strides=(stride_height, stride_width, stride_depth),\\n        padding=\\'valid\\',\\n        name=tf_name\\n    )\\n\\n    layers[scope_name] = pooling(layers[input_name])', 'def convert_adaptive_max_pool2d(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert convert_adaptive_max_pool2d layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting adaptive_avg_pool2d...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'APOL\\' + random_string(4)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    global_pool = keras.layers.GlobalMaxPooling2D(data_format=\\'channels_first\\', name=tf_name)\\n    layers[scope_name] = global_pool(layers[inputs[0]])\\n\\n    def target_layer(x):\\n        import keras\\n        return keras.backend.expand_dims(x)\\n\\n    lambda_layer = keras.layers.Lambda(target_layer, name=tf_name + \\'E\\')\\n    layers[scope_name] = lambda_layer(layers[scope_name])  # double expand dims\\n    layers[scope_name] = lambda_layer(layers[scope_name])', 'def convert_padding(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert padding layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting padding...\\')\\n\\n    if params[\\'mode\\'] == \\'constant\\':\\n        # raise AssertionError(\\'Cannot convert non-constant padding\\')\\n\\n        if params[\\'value\\'] != 0.0:\\n            raise AssertionError(\\'Cannot convert non-zero padding\\')\\n\\n        if names:\\n            tf_name = \\'PADD\\' + random_string(4)\\n        else:\\n            tf_name = w_name + str(random.random())\\n\\n        # Magic ordering\\n        padding_name = tf_name\\n        padding_layer = keras.layers.ZeroPadding2D(\\n            padding=((params[\\'pads\\'][2], params[\\'pads\\'][6]), (params[\\'pads\\'][3], params[\\'pads\\'][7])),\\n            name=padding_name\\n        )\\n\\n        layers[scope_name] = padding_layer(layers[inputs[0]])\\n    elif params[\\'mode\\'] == \\'reflect\\':\\n\\n        def target_layer(x, pads=params[\\'pads\\']):\\n            # x = tf.transpose(x, [0, 2, 3, 1])\\n            layer = tf.pad(x, [[0, 0], [0, 0], [pads[2], pads[6]], [pads[3], pads[7]]], \\'REFLECT\\')\\n            # layer = tf.transpose(layer, [0, 3, 1, 2])\\n            return layer\\n\\n        lambda_layer = keras.layers.Lambda(target_layer)\\n        layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_batchnorm(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert batch normalization layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting batchnorm ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'BN\\' + random_string(6)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    bias_name = \\'{0}.bias\\'.format(w_name)\\n    weights_name = \\'{0}.weight\\'.format(w_name)\\n    mean_name = \\'{0}.running_mean\\'.format(w_name)\\n    var_name = \\'{0}.running_var\\'.format(w_name)\\n\\n    if bias_name in weights:\\n        beta = weights[bias_name].numpy()\\n\\n    if weights_name in weights:\\n        gamma = weights[weights_name].numpy()\\n\\n    mean = weights[mean_name].numpy()\\n    variance = weights[var_name].numpy()\\n\\n    eps = params[\\'epsilon\\']\\n    momentum = params[\\'momentum\\']\\n\\n    if weights_name not in weights:\\n        bn = keras.layers.BatchNormalization(\\n            axis=1, momentum=momentum, epsilon=eps,\\n            center=False, scale=False,\\n            weights=[mean, variance],\\n            name=tf_name\\n        )\\n    else:\\n        bn = keras.layers.BatchNormalization(\\n            axis=1, momentum=momentum, epsilon=eps,\\n            weights=[gamma, beta, mean, variance],\\n            name=tf_name\\n        )\\n    layers[scope_name] = bn(layers[inputs[0]])', 'def convert_instancenorm(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert instance normalization layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting instancenorm ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'IN\\' + random_string(6)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    assert(len(inputs) == 3)\\n\\n    bias_name = \\'{0}.bias\\'.format(w_name)\\n    weights_name = \\'{0}.weight\\'.format(w_name)\\n\\n    # Use previously taken constants\\n    if inputs[-2] + \\'_np\\' in layers:\\n        gamma = layers[inputs[-2] + \\'_np\\']\\n    else:\\n        gamma = weights[weights_name].numpy()\\n\\n    if inputs[-1] + \\'_np\\' in layers:\\n        beta = layers[inputs[-1] + \\'_np\\']\\n    else:\\n        beta = weights[bias_name].numpy()\\n\\n    def target_layer(x, epsilon=params[\\'epsilon\\'], gamma=gamma, beta=beta):\\n        layer = tf.contrib.layers.instance_norm(\\n            x,\\n            param_initializers={\\'beta\\': tf.constant_initializer(beta), \\'gamma\\': tf.constant_initializer(gamma)},\\n            epsilon=epsilon, data_format=\\'NCHW\\',\\n            trainable=False\\n        )\\n        return layer\\n\\n    lambda_layer = keras.layers.Lambda(target_layer, name=tf_name)\\n    layers[scope_name] = lambda_layer(layers[inputs[0]])', 'def convert_dropout(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert dropout.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting dropout ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'DO\\' + random_string(6)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    dropout = keras.layers.Dropout(rate=params[\\'ratio\\'], name=tf_name)\\n    layers[scope_name] = dropout(layers[inputs[0]])', 'def convert_relu(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert relu layer.\\n\\n    Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting relu ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'RELU\\' + random_string(4)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    relu = keras.layers.Activation(\\'relu\\', name=tf_name)\\n    layers[scope_name] = relu(layers[inputs[0]])', 'def convert_lrelu(params, w_name, scope_name, inputs, layers, weights, names):\\n    \"\"\"\\n    Convert leaky relu layer.\\n\\n   Args:\\n        params: dictionary with layer parameters\\n        w_name: name prefix in state_dict\\n        scope_name: pytorch scope name\\n        inputs: pytorch node inputs\\n        layers: dictionary with keras tensors\\n        weights: pytorch state_dict\\n        names: use short names for keras layers\\n    \"\"\"\\n    print(\\'Converting lrelu ...\\')\\n\\n    if names == \\'short\\':\\n        tf_name = \\'lRELU\\' + random_string(3)\\n    elif names == \\'keep\\':\\n        tf_name = w_name\\n    else:\\n        tf_name = w_name + str(random.random())\\n\\n    leakyrelu = \\\\\\n        keras.layers.LeakyReLU(alpha=params[\\'alpha\\'], name=tf_name)\\n    layers[scope_name] = leakyrelu(layers[inputs[0]])']}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=5):   0%|          | 0/13788 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'datasets.formatting.formatting.LazyBatch'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [5516, 5517, 5518, 5519, 5520, 5521, 5522, 5523, 5524, 5525, 5526, 5527, 5528, 5529, 5530, 5531, 5532, 5533, 5534, 5535, 5536, 5537, 5538, 5539, 5540, 5541, 5542, 5543, 5544, 5545, 5546, 5547, 5548, 5549, 5550, 5551, 5552, 5553, 5554, 5555], 'query_id': ['q257387', 'q257388', 'q257389', 'q257390', 'q257391', 'q257392', 'q257393', 'q257394', 'q257395', 'q257396', 'q257397', 'q257398', 'q257399', 'q257400', 'q257401', 'q257402', 'q257403', 'q257404', 'q257405', 'q257406', 'q257407', 'q257408', 'q257409', 'q257410', 'q257411', 'q257412', 'q257413', 'q257414', 'q257415', 'q257416', 'q257417', 'q257418', 'q257419', 'q257420', 'q257421', 'q257422', 'q257423', 'q257424', 'q257425', 'q257426'], 'corpus_id': ['c257336', 'c257337', 'c257338', 'c257339', 'c257340', 'c257341', 'c257342', 'c257343', 'c257344', 'c257345', 'c257346', 'c257347', 'c257348', 'c257349', 'c257350', 'c257351', 'c257352', 'c257353', 'c257354', 'c257355', 'c257356', 'c257357', 'c257358', 'c257359', 'c257360', 'c257361', 'c257362', 'c257363', 'c257364', 'c257365', 'c257366', 'c257367', 'c257368', 'c257369', 'c257370', 'c257371', 'c257372', 'c257373', 'c257374', 'c257375'], 'score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'doc': ['Return process bounding box for zoom level.\\n\\n        Parameters\\n        ----------\\n        zoom : int or None\\n            if None, the union of all zoom level areas is returned\\n\\n        Returns\\n        -------\\n        process area : shapely geometry', 'Return process bounds for zoom level.\\n\\n        Parameters\\n        ----------\\n        zoom : integer or list\\n\\n        Returns\\n        -------\\n        process bounds : tuple\\n            left, bottom, right, top', 'Generate indexes for given zoom level.\\n\\n    Parameters\\n    ----------\\n    mp : Mapchete object\\n        process output to be indexed\\n    out_dir : path\\n        optionally override process output directory\\n    zoom : int\\n        zoom level to be processed\\n    geojson : bool\\n        generate GeoJSON index (default: False)\\n    gpkg : bool\\n        generate GeoPackage index (default: False)\\n    shapefile : bool\\n        generate Shapefile index (default: False)\\n    txt : bool\\n        generate tile path list textfile (default: False)\\n    vrt : bool\\n        GDAL-style VRT file (default: False)\\n    fieldname : str\\n        field name which contains paths of tiles (default: \"location\")\\n    basepath : str\\n        if set, use custom base path instead of output path\\n    for_gdal : bool\\n        use GDAL compatible remote paths, i.e. add \"/vsicurl/\" before path\\n        (default: True)', 'Return raster metadata.', 'Example process for testing.\\n\\n    Inputs:\\n    -------\\n    file1\\n        raster file\\n\\n    Parameters:\\n    -----------\\n\\n    Output:\\n    -------\\n    np.ndarray', 'Check if output format is valid with other process parameters.\\n\\n        Parameters\\n        ----------\\n        config : dictionary\\n            output configuration parameters\\n\\n        Returns\\n        -------\\n        is_valid : bool', \"Read data from process output.\\n\\n        Parameters\\n        ----------\\n        validity_check : bool\\n            run geometry validity check (default: True)\\n        no_neighbors : bool\\n            don't include neighbor tiles if there is a pixelbuffer (default:\\n            False)\\n\\n        Returns\\n        -------\\n        features : list\\n            GeoJSON-like list of features\", 'Return all available output formats.\\n\\n    Returns\\n    -------\\n    formats : list\\n        all available output formats', 'Return all available input formats.\\n\\n    Returns\\n    -------\\n    formats : list\\n        all available input formats', 'Return output class of driver.\\n\\n    Returns\\n    -------\\n    output : ``OutputData``\\n        output writer object', 'Return input class of driver.\\n\\n    Returns\\n    -------\\n    input_params : ``InputData``\\n        input parameters', 'Guess driver from file extension.\\n\\n    Returns\\n    -------\\n    driver : string\\n        driver name', 'Dump output JSON and verify parameters if output metadata exist.', 'Extract contour lines from an array.\\n\\n    Parameters\\n    ----------\\n    array : array\\n        input elevation data\\n    tile : Tile\\n        tile covering the array\\n    interval : integer\\n        elevation value interval when drawing contour lines\\n    field : string\\n        output field name containing elevation value\\n    base : integer\\n        elevation base value the intervals are computed from\\n\\n    Returns\\n    -------\\n    contours : iterable\\n        contours as GeoJSON-like pairs of properties and geometry', 'Return a list of values between min and max within an interval.', 'Create an empty Mapchete and process file in a given directory.', 'Determine target file path.\\n\\n        Parameters\\n        ----------\\n        tile : ``BufferedTile``\\n            must be member of output ``TilePyramid``\\n\\n        Returns\\n        -------\\n        path : string', 'Create directory and subdirectory if necessary.\\n\\n        Parameters\\n        ----------\\n        tile : ``BufferedTile``\\n            must be member of output ``TilePyramid``', 'Check whether process output is allowed with output driver.\\n\\n        Parameters\\n        ----------\\n        process_data : raw process output\\n\\n        Returns\\n        -------\\n        True or False', 'Return verified and cleaned output.\\n\\n        Parameters\\n        ----------\\n        process_data : raw process output\\n\\n        Returns\\n        -------\\n        NumPy array or list of features.', 'Extract subset from multiple tiles.\\n\\n        input_data_tiles : list of (``Tile``, process data) tuples\\n        out_tile : ``Tile``\\n\\n        Returns\\n        -------\\n        NumPy array or list of features.', 'Calculate slope and aspect map.\\n\\n    Return a pair of arrays 2 pixels smaller than the input elevation array.\\n\\n    Slope is returned in radians, from 0 for sheer face to pi/2 for\\n    flat ground. Aspect is returned in radians, counterclockwise from -pi\\n    at north around to pi.\\n\\n    Logic here is borrowed from hillshade.cpp:\\n    http://www.perrygeo.net/wordpress/?p=7\\n\\n    Parameters\\n    ----------\\n    elevation : array\\n        input elevation data\\n    xres : float\\n        column width\\n    yres : float\\n        row  height\\n    z : float\\n        vertical exaggeration factor\\n    scale : float\\n        scale factor of pixel size units versus height units (insert 112000\\n        when having elevation values in meters in a geodetic projection)\\n\\n    Returns\\n    -------\\n    slope shade : array', 'Return hillshaded numpy array.\\n\\n    Parameters\\n    ----------\\n    elevation : array\\n        input elevation data\\n    tile : Tile\\n        tile covering the array\\n    z : float\\n        vertical exaggeration factor\\n    scale : float\\n        scale factor of pixel size units versus height units (insert 112000\\n        when having elevation values in meters in a geodetic projection)', 'Return ``BufferedTile`` object of this ``BufferedTilePyramid``.\\n\\n        Parameters\\n        ----------\\n        zoom : integer\\n            zoom level\\n        row : integer\\n            tile matrix row\\n        col : integer\\n            tile matrix column\\n\\n        Returns\\n        -------\\n        buffered tile : ``BufferedTile``', 'Return all tiles intersecting with bounds.\\n\\n        Bounds values will be cleaned if they cross the antimeridian or are\\n        outside of the Northern or Southern tile pyramid bounds.\\n\\n        Parameters\\n        ----------\\n        bounds : tuple\\n            (left, bottom, right, top) bounding values in tile pyramid CRS\\n        zoom : integer\\n            zoom level\\n\\n        Yields\\n        ------\\n        intersecting tiles : generator\\n            generates ``BufferedTiles``', 'All metatiles intersecting with given bounding box.\\n\\n        Parameters\\n        ----------\\n        geometry : ``shapely.geometry``\\n        zoom : integer\\n            zoom level\\n\\n        Yields\\n        ------\\n        intersecting tiles : generator\\n            generates ``BufferedTiles``', 'Return all tiles intersecting with input geometry.\\n\\n        Parameters\\n        ----------\\n        geometry : ``shapely.geometry``\\n        zoom : integer\\n            zoom level\\n\\n        Yields\\n        ------\\n        intersecting tiles : ``BufferedTile``', 'Return all BufferedTiles intersecting with tile.\\n\\n        Parameters\\n        ----------\\n        tile : ``BufferedTile``\\n            another tile', 'Return dictionary representation of pyramid parameters.', 'Return tile neighbors.\\n\\n        Tile neighbors are unique, i.e. in some edge cases, where both the left\\n        and right neighbor wrapped around the antimeridian is the same. Also,\\n        neighbors ouside the northern and southern TilePyramid boundaries are\\n        excluded, because they are invalid.\\n\\n        -------------\\n        | 8 | 1 | 5 |\\n        -------------\\n        | 4 | x | 2 |\\n        -------------\\n        | 7 | 3 | 6 |\\n        -------------\\n\\n        Parameters\\n        ----------\\n        connectedness : int\\n            [4 or 8] return four direct neighbors or all eight.\\n\\n        Returns\\n        -------\\n        list of BufferedTiles', 'Determine whether tile touches or goes over pyramid edge.', 'Read, stretch and return raster data.\\n\\n    Inputs:\\n    -------\\n    raster\\n        raster file\\n\\n    Parameters:\\n    -----------\\n    resampling : str\\n        rasterio.Resampling method\\n    scale_method : str\\n        - dtype_scale: use dtype minimum and maximum values\\n        - minmax_scale: use dataset bands minimum and maximum values\\n        - crop: clip data to output dtype\\n    scales_minmax : tuple\\n        tuple of band specific scale values\\n\\n    Output:\\n    -------\\n    np.ndarray', 'Open process output as input for other process.\\n\\n        Parameters\\n        ----------\\n        tile : ``Tile``\\n        process : ``MapcheteProcess``\\n        kwargs : keyword arguments', 'Convert data to web output.\\n\\n        Parameters\\n        ----------\\n        data : array\\n\\n        Returns\\n        -------\\n        web data : array', 'Serve a Mapchete process.\\n\\n    Creates the Mapchete host and serves both web page with OpenLayers and the\\n    WMTS simple REST endpoint.', 'Configure and create Flask app.', 'Return NumPy arrays from an input raster.\\n\\n    NumPy arrays are reprojected and resampled to tile properties from input\\n    raster. If tile boundaries cross the antimeridian, data on the other side\\n    of the antimeridian will be read and concatenated to the numpy array\\n    accordingly.\\n\\n    Parameters\\n    ----------\\n    input_files : string or list\\n        path to a raster file or list of paths to multiple raster files readable by\\n        rasterio.\\n    tile : Tile\\n        a Tile object\\n    indexes : list or int\\n        a list of band numbers; None will read all.\\n    resampling : string\\n        one of \"nearest\", \"average\", \"bilinear\" or \"lanczos\"\\n    src_nodata : int or float, optional\\n        if not set, the nodata value from the source dataset will be used\\n    dst_nodata : int or float, optional\\n        if not set, the nodata value from the source dataset will be used\\n    gdal_opts : dict\\n        GDAL options passed on to rasterio.Env()\\n\\n    Returns\\n    -------\\n    raster : MaskedArray', 'Extract a numpy array from a raster file.', 'Write a window from a numpy array to an output file.\\n\\n    Parameters\\n    ----------\\n    in_tile : ``BufferedTile``\\n        ``BufferedTile`` with a data attribute holding NumPy data\\n    in_data : array\\n    out_profile : dictionary\\n        metadata dictionary for rasterio\\n    out_tile : ``Tile``\\n        provides output boundaries; if None, in_tile is used\\n    out_path : string\\n        output path to write to\\n    tags : optional tags to be added to GeoTIFF file\\n    bucket_resource : boto3 bucket resource to write to in case of S3 output', 'Extract raster data window array.\\n\\n    Parameters\\n    ----------\\n    in_raster : array or ReferencedRaster\\n    in_affine : ``Affine`` required if in_raster is an array\\n    out_tile : ``BufferedTile``\\n\\n    Returns\\n    -------\\n    extracted array : array'], 'code': ['def area_at_zoom(self, zoom=None):\\n        \"\"\"\\n        Return process bounding box for zoom level.\\n\\n        Parameters\\n        ----------\\n        zoom : int or None\\n            if None, the union of all zoom level areas is returned\\n\\n        Returns\\n        -------\\n        process area : shapely geometry\\n        \"\"\"\\n        if zoom is None:\\n            if not self._cache_full_process_area:\\n                logger.debug(\"calculate process area ...\")\\n                self._cache_full_process_area = cascaded_union([\\n                    self._area_at_zoom(z) for z in self.init_zoom_levels]\\n                ).buffer(0)\\n            return self._cache_full_process_area\\n        else:\\n            if zoom not in self.init_zoom_levels:\\n                raise ValueError(\\n                    \"zoom level not available with current configuration\")\\n            return self._area_at_zoom(zoom)', 'def bounds_at_zoom(self, zoom=None):\\n        \"\"\"\\n        Return process bounds for zoom level.\\n\\n        Parameters\\n        ----------\\n        zoom : integer or list\\n\\n        Returns\\n        -------\\n        process bounds : tuple\\n            left, bottom, right, top\\n        \"\"\"\\n        return () if self.area_at_zoom(zoom).is_empty else Bounds(\\n            *self.area_at_zoom(zoom).bounds)', 'def zoom_index_gen(\\n    mp=None,\\n    out_dir=None,\\n    zoom=None,\\n    geojson=False,\\n    gpkg=False,\\n    shapefile=False,\\n    txt=False,\\n    vrt=False,\\n    fieldname=\"location\",\\n    basepath=None,\\n    for_gdal=True,\\n    threading=False,\\n):\\n    \"\"\"\\n    Generate indexes for given zoom level.\\n\\n    Parameters\\n    ----------\\n    mp : Mapchete object\\n        process output to be indexed\\n    out_dir : path\\n        optionally override process output directory\\n    zoom : int\\n        zoom level to be processed\\n    geojson : bool\\n        generate GeoJSON index (default: False)\\n    gpkg : bool\\n        generate GeoPackage index (default: False)\\n    shapefile : bool\\n        generate Shapefile index (default: False)\\n    txt : bool\\n        generate tile path list textfile (default: False)\\n    vrt : bool\\n        GDAL-style VRT file (default: False)\\n    fieldname : str\\n        field name which contains paths of tiles (default: \"location\")\\n    basepath : str\\n        if set, use custom base path instead of output path\\n    for_gdal : bool\\n        use GDAL compatible remote paths, i.e. add \"/vsicurl/\" before path\\n        (default: True)\\n    \"\"\"\\n    for zoom in get_zoom_levels(process_zoom_levels=zoom):\\n        with ExitStack() as es:\\n            # get index writers for all enabled formats\\n            index_writers = []\\n            if geojson:\\n                index_writers.append(\\n                    es.enter_context(\\n                        VectorFileWriter(\\n                            driver=\"GeoJSON\",\\n                            out_path=_index_file_path(out_dir, zoom, \"geojson\"),\\n                            crs=mp.config.output_pyramid.crs,\\n                            fieldname=fieldname\\n                        )\\n                    )\\n                )\\n            if gpkg:\\n                index_writers.append(\\n                    es.enter_context(\\n                        VectorFileWriter(\\n                            driver=\"GPKG\",\\n                            out_path=_index_file_path(out_dir, zoom, \"gpkg\"),\\n                            crs=mp.config.output_pyramid.crs,\\n                            fieldname=fieldname\\n                        )\\n                    )\\n                )\\n            if shapefile:\\n                index_writers.append(\\n                    es.enter_context(\\n                        VectorFileWriter(\\n                            driver=\"ESRI Shapefile\",\\n                            out_path=_index_file_path(out_dir, zoom, \"shp\"),\\n                            crs=mp.config.output_pyramid.crs,\\n                            fieldname=fieldname\\n                        )\\n                    )\\n                )\\n            if txt:\\n                index_writers.append(\\n                    es.enter_context(\\n                        TextFileWriter(out_path=_index_file_path(out_dir, zoom, \"txt\"))\\n                    )\\n                )\\n            if vrt:\\n                index_writers.append(\\n                    es.enter_context(\\n                        VRTFileWriter(\\n                            out_path=_index_file_path(out_dir, zoom, \"vrt\"),\\n                            output=mp.config.output,\\n                            out_pyramid=mp.config.output_pyramid\\n                        )\\n                    )\\n                )\\n\\n            logger.debug(\"use the following index writers: %s\", index_writers)\\n\\n            def _worker(tile):\\n                # if there are indexes to write to, check if output exists\\n                tile_path = _tile_path(\\n                    orig_path=mp.config.output.get_path(tile),\\n                    basepath=basepath,\\n                    for_gdal=for_gdal\\n                )\\n                indexes = [\\n                    i for i in index_writers\\n                    if not i.entry_exists(tile=tile, path=tile_path)\\n                ]\\n                if indexes:\\n                    output_exists = mp.config.output.tiles_exist(output_tile=tile)\\n                else:\\n                    output_exists = None\\n                return tile, tile_path, indexes, output_exists\\n\\n            with concurrent.futures.ThreadPoolExecutor() as executor:\\n                for task in concurrent.futures.as_completed(\\n                    (\\n                        executor.submit(_worker, i)\\n                        for i in mp.config.output_pyramid.tiles_from_geom(\\n                            mp.config.area_at_zoom(zoom), zoom\\n                        )\\n                    )\\n                ):\\n                    tile, tile_path, indexes, output_exists = task.result()\\n                    # only write entries if there are indexes to write to and output\\n                    # exists\\n                    if indexes and output_exists:\\n                        logger.debug(\"%s exists\", tile_path)\\n                        logger.debug(\"write to %s indexes\" % len(indexes))\\n                        for index in indexes:\\n                            index.write(tile, tile_path)\\n                    # yield tile for progress information\\n                    yield tile', 'def profile(self):\\n        \"\"\"Return raster metadata.\"\"\"\\n        with rasterio.open(self.path, \"r\") as src:\\n            return deepcopy(src.meta)', 'def execute(mp):\\n    \"\"\"\\n    Example process for testing.\\n\\n    Inputs:\\n    -------\\n    file1\\n        raster file\\n\\n    Parameters:\\n    -----------\\n\\n    Output:\\n    -------\\n    np.ndarray\\n    \"\"\"\\n    # Reading and writing data works like this:\\n    with mp.open(\"file1\", resampling=\"bilinear\") as raster_file:\\n        if raster_file.is_empty():\\n            return \"empty\"\\n            # This assures a transparent tile instead of a pink error tile\\n            # is returned when using mapchete serve.\\n        dem = raster_file.read()\\n    return dem', 'def is_valid_with_config(self, config):\\n        \"\"\"\\n        Check if output format is valid with other process parameters.\\n\\n        Parameters\\n        ----------\\n        config : dictionary\\n            output configuration parameters\\n\\n        Returns\\n        -------\\n        is_valid : bool\\n        \"\"\"\\n        validate_values(config, [(\"schema\", dict), (\"path\", str)])\\n        validate_values(config[\"schema\"], [(\"properties\", dict), (\"geometry\", str)])\\n        if config[\"schema\"][\"geometry\"] not in [\\n            \"Geometry\", \"Point\", \"MultiPoint\", \"Line\", \"MultiLine\",\\n            \"Polygon\", \"MultiPolygon\"\\n        ]:\\n            raise TypeError(\"invalid geometry type\")\\n        return True', 'def read(self, validity_check=True, no_neighbors=False, **kwargs):\\n        \"\"\"\\n        Read data from process output.\\n\\n        Parameters\\n        ----------\\n        validity_check : bool\\n            run geometry validity check (default: True)\\n        no_neighbors : bool\\n            don\\'t include neighbor tiles if there is a pixelbuffer (default:\\n            False)\\n\\n        Returns\\n        -------\\n        features : list\\n            GeoJSON-like list of features\\n        \"\"\"\\n        if no_neighbors:\\n            raise NotImplementedError()\\n        return self._from_cache(validity_check=validity_check)', 'def available_output_formats():\\n    \"\"\"\\n    Return all available output formats.\\n\\n    Returns\\n    -------\\n    formats : list\\n        all available output formats\\n    \"\"\"\\n    output_formats = []\\n    for v in pkg_resources.iter_entry_points(DRIVERS_ENTRY_POINT):\\n        driver_ = v.load()\\n        if hasattr(driver_, \"METADATA\") and (\\n            driver_.METADATA[\"mode\"] in [\"w\", \"rw\"]\\n        ):\\n            output_formats.append(driver_.METADATA[\"driver_name\"])\\n    return output_formats', 'def available_input_formats():\\n    \"\"\"\\n    Return all available input formats.\\n\\n    Returns\\n    -------\\n    formats : list\\n        all available input formats\\n    \"\"\"\\n    input_formats = []\\n    for v in pkg_resources.iter_entry_points(DRIVERS_ENTRY_POINT):\\n        logger.debug(\"driver found: %s\", v)\\n        driver_ = v.load()\\n        if hasattr(driver_, \"METADATA\") and (driver_.METADATA[\"mode\"] in [\"r\", \"rw\"]):\\n            input_formats.append(driver_.METADATA[\"driver_name\"])\\n    return input_formats', 'def load_output_writer(output_params, readonly=False):\\n    \"\"\"\\n    Return output class of driver.\\n\\n    Returns\\n    -------\\n    output : ``OutputData``\\n        output writer object\\n    \"\"\"\\n    if not isinstance(output_params, dict):\\n        raise TypeError(\"output_params must be a dictionary\")\\n    driver_name = output_params[\"format\"]\\n    for v in pkg_resources.iter_entry_points(DRIVERS_ENTRY_POINT):\\n        _driver = v.load()\\n        if all(\\n            [hasattr(_driver, attr) for attr in [\"OutputData\", \"METADATA\"]]\\n            ) and (\\n            _driver.METADATA[\"driver_name\"] == driver_name\\n        ):\\n            return _driver.OutputData(output_params, readonly=readonly)\\n    raise MapcheteDriverError(\"no loader for driver \\'%s\\' could be found.\" % driver_name)', 'def load_input_reader(input_params, readonly=False):\\n    \"\"\"\\n    Return input class of driver.\\n\\n    Returns\\n    -------\\n    input_params : ``InputData``\\n        input parameters\\n    \"\"\"\\n    logger.debug(\"find input reader with params %s\", input_params)\\n    if not isinstance(input_params, dict):\\n        raise TypeError(\"input_params must be a dictionary\")\\n    if \"abstract\" in input_params:\\n        driver_name = input_params[\"abstract\"][\"format\"]\\n    elif \"path\" in input_params:\\n        if os.path.splitext(input_params[\"path\"])[1]:\\n            input_file = input_params[\"path\"]\\n            driver_name = driver_from_file(input_file)\\n        else:\\n            logger.debug(\"%s is a directory\", input_params[\"path\"])\\n            driver_name = \"TileDirectory\"\\n    else:\\n        raise MapcheteDriverError(\"invalid input parameters %s\" % input_params)\\n    for v in pkg_resources.iter_entry_points(DRIVERS_ENTRY_POINT):\\n        driver_ = v.load()\\n        if hasattr(driver_, \"METADATA\") and (\\n            driver_.METADATA[\"driver_name\"] == driver_name\\n        ):\\n            return v.load().InputData(input_params, readonly=readonly)\\n    raise MapcheteDriverError(\"no loader for driver \\'%s\\' could be found.\" % driver_name)', 'def driver_from_file(input_file):\\n    \"\"\"\\n    Guess driver from file extension.\\n\\n    Returns\\n    -------\\n    driver : string\\n        driver name\\n    \"\"\"\\n    file_ext = os.path.splitext(input_file)[1].split(\".\")[1]\\n    if file_ext not in _file_ext_to_driver():\\n        raise MapcheteDriverError(\\n            \"no driver could be found for file extension %s\" % file_ext\\n        )\\n    driver = _file_ext_to_driver()[file_ext]\\n    if len(driver) > 1:\\n        warnings.warn(\\n            DeprecationWarning(\\n                \"more than one driver for file found, taking %s\" % driver[0]\\n            )\\n        )\\n    return driver[0]', 'def write_output_metadata(output_params):\\n    \"\"\"Dump output JSON and verify parameters if output metadata exist.\"\"\"\\n    if \"path\" in output_params:\\n        metadata_path = os.path.join(output_params[\"path\"], \"metadata.json\")\\n        logger.debug(\"check for output %s\", metadata_path)\\n        try:\\n            existing_params = read_output_metadata(metadata_path)\\n            logger.debug(\"%s exists\", metadata_path)\\n            logger.debug(\"existing output parameters: %s\", pformat(existing_params))\\n            existing_tp = existing_params[\"pyramid\"]\\n            current_params = params_to_dump(output_params)\\n            logger.debug(\"current output parameters: %s\", pformat(current_params))\\n            current_tp = BufferedTilePyramid(**current_params[\"pyramid\"])\\n            if existing_tp != current_tp:\\n                raise MapcheteConfigError(\\n                    \"pyramid definitions between existing and new output do not match: \"\\n                    \"%s != %s\" % (existing_tp, current_tp)\\n                )\\n            existing_format = existing_params[\"driver\"][\"format\"]\\n            current_format = current_params[\"driver\"][\"format\"]\\n            if existing_format != current_format:\\n                raise MapcheteConfigError(\\n                    \"existing output format does not match new output format: \"\\n                    \"%s != %s\" % (\\n                        (existing_format, current_format)\\n                    )\\n                )\\n        except FileNotFoundError:\\n            logger.debug(\"%s does not exist\", metadata_path)\\n            dump_params = params_to_dump(output_params)\\n            # dump output metadata\\n            write_json(metadata_path, dump_params)\\n    else:\\n        logger.debug(\"no path parameter found\")', 'def extract_contours(array, tile, interval=100, field=\\'elev\\', base=0):\\n    \"\"\"\\n    Extract contour lines from an array.\\n\\n    Parameters\\n    ----------\\n    array : array\\n        input elevation data\\n    tile : Tile\\n        tile covering the array\\n    interval : integer\\n        elevation value interval when drawing contour lines\\n    field : string\\n        output field name containing elevation value\\n    base : integer\\n        elevation base value the intervals are computed from\\n\\n    Returns\\n    -------\\n    contours : iterable\\n        contours as GeoJSON-like pairs of properties and geometry\\n    \"\"\"\\n    import matplotlib.pyplot as plt\\n    levels = _get_contour_values(\\n        array.min(), array.max(), interval=interval, base=base)\\n    if not levels:\\n        return []\\n    contours = plt.contour(array, levels)\\n    index = 0\\n    out_contours = []\\n    for level in range(len(contours.collections)):\\n        elevation = levels[index]\\n        index += 1\\n        paths = contours.collections[level].get_paths()\\n        for path in paths:\\n            out_coords = [\\n                (\\n                    tile.left + (y * tile.pixel_x_size),\\n                    tile.top - (x * tile.pixel_y_size),\\n                )\\n                for x, y in zip(path.vertices[:, 1], path.vertices[:, 0])\\n            ]\\n            if len(out_coords) >= 2:\\n                out_contours.append(\\n                    dict(\\n                        properties={field: elevation},\\n                        geometry=mapping(LineString(out_coords))\\n                    )\\n                )\\n    return out_contours', 'def _get_contour_values(min_val, max_val, base=0, interval=100):\\n    \"\"\"Return a list of values between min and max within an interval.\"\"\"\\n    i = base\\n    out = []\\n    if min_val < base:\\n        while i >= min_val:\\n            i -= interval\\n    while i <= max_val:\\n        if i >= min_val:\\n            out.append(i)\\n        i += interval\\n    return out', 'def create(\\n    mapchete_file,\\n    process_file,\\n    out_format,\\n    out_path=None,\\n    pyramid_type=None,\\n    force=False\\n):\\n    \"\"\"Create an empty Mapchete and process file in a given directory.\"\"\"\\n    if os.path.isfile(process_file) or os.path.isfile(mapchete_file):\\n        if not force:\\n            raise IOError(\"file(s) already exists\")\\n\\n    out_path = out_path if out_path else os.path.join(os.getcwd(), \"output\")\\n\\n    # copy file template to target directory\\n    process_template = pkg_resources.resource_filename(\\n        \"mapchete.static\", \"process_template.py\"\\n    )\\n    process_file = os.path.join(os.getcwd(), process_file)\\n    copyfile(process_template, process_file)\\n\\n    # modify and copy mapchete file template to target directory\\n    mapchete_template = pkg_resources.resource_filename(\\n        \"mapchete.static\", \"mapchete_template.mapchete\"\\n    )\\n\\n    output_options = dict(\\n        format=out_format, path=out_path, **FORMAT_MANDATORY[out_format]\\n    )\\n\\n    pyramid_options = {\\'grid\\': pyramid_type}\\n\\n    substitute_elements = {\\n        \\'process_file\\': process_file,\\n        \\'output\\': dump({\\'output\\': output_options}, default_flow_style=False),\\n        \\'pyramid\\': dump({\\'pyramid\\': pyramid_options}, default_flow_style=False)\\n    }\\n    with open(mapchete_template, \\'r\\') as config_template:\\n        config = Template(config_template.read())\\n        customized_config = config.substitute(substitute_elements)\\n    with open(mapchete_file, \\'w\\') as target_config:\\n        target_config.write(customized_config)', 'def get_path(self, tile):\\n        \"\"\"\\n        Determine target file path.\\n\\n        Parameters\\n        ----------\\n        tile : ``BufferedTile``\\n            must be member of output ``TilePyramid``\\n\\n        Returns\\n        -------\\n        path : string\\n        \"\"\"\\n        return os.path.join(*[\\n            self.path,\\n            str(tile.zoom),\\n            str(tile.row),\\n            str(tile.col) + self.file_extension\\n        ])', 'def prepare_path(self, tile):\\n        \"\"\"\\n        Create directory and subdirectory if necessary.\\n\\n        Parameters\\n        ----------\\n        tile : ``BufferedTile``\\n            must be member of output ``TilePyramid``\\n        \"\"\"\\n        makedirs(os.path.dirname(self.get_path(tile)))', 'def output_is_valid(self, process_data):\\n        \"\"\"\\n        Check whether process output is allowed with output driver.\\n\\n        Parameters\\n        ----------\\n        process_data : raw process output\\n\\n        Returns\\n        -------\\n        True or False\\n        \"\"\"\\n        if self.METADATA[\"data_type\"] == \"raster\":\\n            return (\\n                is_numpy_or_masked_array(process_data) or\\n                is_numpy_or_masked_array_with_tags(process_data)\\n            )\\n        elif self.METADATA[\"data_type\"] == \"vector\":\\n            return is_feature_list(process_data)', 'def output_cleaned(self, process_data):\\n        \"\"\"\\n        Return verified and cleaned output.\\n\\n        Parameters\\n        ----------\\n        process_data : raw process output\\n\\n        Returns\\n        -------\\n        NumPy array or list of features.\\n        \"\"\"\\n        if self.METADATA[\"data_type\"] == \"raster\":\\n            if is_numpy_or_masked_array(process_data):\\n                return process_data\\n            elif is_numpy_or_masked_array_with_tags(process_data):\\n                data, tags = process_data\\n                return self.output_cleaned(data), tags\\n        elif self.METADATA[\"data_type\"] == \"vector\":\\n            return list(process_data)', 'def extract_subset(self, input_data_tiles=None, out_tile=None):\\n        \"\"\"\\n        Extract subset from multiple tiles.\\n\\n        input_data_tiles : list of (``Tile``, process data) tuples\\n        out_tile : ``Tile``\\n\\n        Returns\\n        -------\\n        NumPy array or list of features.\\n        \"\"\"\\n        if self.METADATA[\"data_type\"] == \"raster\":\\n            mosaic = create_mosaic(input_data_tiles)\\n            return extract_from_array(\\n                in_raster=prepare_array(\\n                    mosaic.data,\\n                    nodata=self.nodata,\\n                    dtype=self.output_params[\"dtype\"]\\n                ),\\n                in_affine=mosaic.affine,\\n                out_tile=out_tile\\n            )\\n        elif self.METADATA[\"data_type\"] == \"vector\":\\n            return [\\n                feature for feature in list(\\n                    chain.from_iterable([features for _, features in input_data_tiles])\\n                )\\n                if shape(feature[\"geometry\"]).intersects(out_tile.bbox)\\n            ]', 'def calculate_slope_aspect(elevation, xres, yres, z=1.0, scale=1.0):\\n    \"\"\"\\n    Calculate slope and aspect map.\\n\\n    Return a pair of arrays 2 pixels smaller than the input elevation array.\\n\\n    Slope is returned in radians, from 0 for sheer face to pi/2 for\\n    flat ground. Aspect is returned in radians, counterclockwise from -pi\\n    at north around to pi.\\n\\n    Logic here is borrowed from hillshade.cpp:\\n    http://www.perrygeo.net/wordpress/?p=7\\n\\n    Parameters\\n    ----------\\n    elevation : array\\n        input elevation data\\n    xres : float\\n        column width\\n    yres : float\\n        row  height\\n    z : float\\n        vertical exaggeration factor\\n    scale : float\\n        scale factor of pixel size units versus height units (insert 112000\\n        when having elevation values in meters in a geodetic projection)\\n\\n    Returns\\n    -------\\n    slope shade : array\\n    \"\"\"\\n    z = float(z)\\n    scale = float(scale)\\n    height, width = elevation.shape[0] - 2, elevation.shape[1] - 2\\n    window = [\\n        z * elevation[row:(row + height), col:(col + width)]\\n        for (row, col) in product(range(3), range(3))\\n    ]\\n    x = (\\n        (window[0] + window[3] + window[3] + window[6])\\n        - (window[2] + window[5] + window[5] + window[8])\\n        ) / (8.0 * xres * scale)\\n    y = (\\n        (window[6] + window[7] + window[7] + window[8])\\n        - (window[0] + window[1] + window[1] + window[2])\\n        ) / (8.0 * yres * scale)\\n    # in radians, from 0 to pi/2\\n    slope = math.pi/2 - np.arctan(np.sqrt(x*x + y*y))\\n    # in radians counterclockwise, from -pi at north back to pi\\n    aspect = np.arctan2(x, y)\\n    return slope, aspect', 'def hillshade(elevation, tile, azimuth=315.0, altitude=45.0, z=1.0, scale=1.0):\\n    \"\"\"\\n    Return hillshaded numpy array.\\n\\n    Parameters\\n    ----------\\n    elevation : array\\n        input elevation data\\n    tile : Tile\\n        tile covering the array\\n    z : float\\n        vertical exaggeration factor\\n    scale : float\\n        scale factor of pixel size units versus height units (insert 112000\\n        when having elevation values in meters in a geodetic projection)\\n    \"\"\"\\n    azimuth = float(azimuth)\\n    altitude = float(altitude)\\n    z = float(z)\\n    scale = float(scale)\\n    xres = tile.tile.pixel_x_size\\n    yres = -tile.tile.pixel_y_size\\n    slope, aspect = calculate_slope_aspect(\\n        elevation, xres, yres, z=z, scale=scale)\\n    deg2rad = math.pi / 180.0\\n    shaded = np.sin(altitude * deg2rad) * np.sin(slope) \\\\\\n        + np.cos(altitude * deg2rad) * np.cos(slope) \\\\\\n        * np.cos((azimuth - 90.0) * deg2rad - aspect)\\n    # shaded now has values between -1.0 and +1.0\\n    # stretch to 0 - 255 and invert\\n    shaded = (((shaded+1.0)/2)*-255.0).astype(\"uint8\")\\n    # add one pixel padding using the edge values\\n    return ma.masked_array(\\n        data=np.pad(shaded, 1, mode=\\'edge\\'), mask=elevation.mask\\n    )', 'def tile(self, zoom, row, col):\\n        \"\"\"\\n        Return ``BufferedTile`` object of this ``BufferedTilePyramid``.\\n\\n        Parameters\\n        ----------\\n        zoom : integer\\n            zoom level\\n        row : integer\\n            tile matrix row\\n        col : integer\\n            tile matrix column\\n\\n        Returns\\n        -------\\n        buffered tile : ``BufferedTile``\\n        \"\"\"\\n        tile = self.tile_pyramid.tile(zoom, row, col)\\n        return BufferedTile(tile, pixelbuffer=self.pixelbuffer)', 'def tiles_from_bounds(self, bounds, zoom):\\n        \"\"\"\\n        Return all tiles intersecting with bounds.\\n\\n        Bounds values will be cleaned if they cross the antimeridian or are\\n        outside of the Northern or Southern tile pyramid bounds.\\n\\n        Parameters\\n        ----------\\n        bounds : tuple\\n            (left, bottom, right, top) bounding values in tile pyramid CRS\\n        zoom : integer\\n            zoom level\\n\\n        Yields\\n        ------\\n        intersecting tiles : generator\\n            generates ``BufferedTiles``\\n        \"\"\"\\n        for tile in self.tiles_from_bbox(box(*bounds), zoom):\\n            yield self.tile(*tile.id)', 'def tiles_from_bbox(self, geometry, zoom):\\n        \"\"\"\\n        All metatiles intersecting with given bounding box.\\n\\n        Parameters\\n        ----------\\n        geometry : ``shapely.geometry``\\n        zoom : integer\\n            zoom level\\n\\n        Yields\\n        ------\\n        intersecting tiles : generator\\n            generates ``BufferedTiles``\\n        \"\"\"\\n        for tile in self.tile_pyramid.tiles_from_bbox(geometry, zoom):\\n            yield self.tile(*tile.id)', 'def tiles_from_geom(self, geometry, zoom):\\n        \"\"\"\\n        Return all tiles intersecting with input geometry.\\n\\n        Parameters\\n        ----------\\n        geometry : ``shapely.geometry``\\n        zoom : integer\\n            zoom level\\n\\n        Yields\\n        ------\\n        intersecting tiles : ``BufferedTile``\\n        \"\"\"\\n        for tile in self.tile_pyramid.tiles_from_geom(geometry, zoom):\\n            yield self.tile(*tile.id)', 'def intersecting(self, tile):\\n        \"\"\"\\n        Return all BufferedTiles intersecting with tile.\\n\\n        Parameters\\n        ----------\\n        tile : ``BufferedTile``\\n            another tile\\n        \"\"\"\\n        return [\\n            self.tile(*intersecting_tile.id)\\n            for intersecting_tile in self.tile_pyramid.intersecting(tile)\\n        ]', 'def to_dict(self):\\n        \"\"\"\\n        Return dictionary representation of pyramid parameters.\\n        \"\"\"\\n        return dict(\\n            grid=self.grid.to_dict(),\\n            metatiling=self.metatiling,\\n            tile_size=self.tile_size,\\n            pixelbuffer=self.pixelbuffer\\n        )', 'def get_neighbors(self, connectedness=8):\\n        \"\"\"\\n        Return tile neighbors.\\n\\n        Tile neighbors are unique, i.e. in some edge cases, where both the left\\n        and right neighbor wrapped around the antimeridian is the same. Also,\\n        neighbors ouside the northern and southern TilePyramid boundaries are\\n        excluded, because they are invalid.\\n\\n        -------------\\n        | 8 | 1 | 5 |\\n        -------------\\n        | 4 | x | 2 |\\n        -------------\\n        | 7 | 3 | 6 |\\n        -------------\\n\\n        Parameters\\n        ----------\\n        connectedness : int\\n            [4 or 8] return four direct neighbors or all eight.\\n\\n        Returns\\n        -------\\n        list of BufferedTiles\\n        \"\"\"\\n        return [\\n            BufferedTile(t, self.pixelbuffer)\\n            for t in self._tile.get_neighbors(connectedness=connectedness)\\n        ]', 'def is_on_edge(self):\\n        \"\"\"Determine whether tile touches or goes over pyramid edge.\"\"\"\\n        return (\\n            self.left <= self.tile_pyramid.left or      # touches_left\\n            self.bottom <= self.tile_pyramid.bottom or  # touches_bottom\\n            self.right >= self.tile_pyramid.right or    # touches_right\\n            self.top >= self.tile_pyramid.top           # touches_top\\n        )', 'def execute(\\n    mp,\\n    resampling=\"nearest\",\\n    scale_method=None,\\n    scales_minmax=None\\n):\\n    \"\"\"\\n    Read, stretch and return raster data.\\n\\n    Inputs:\\n    -------\\n    raster\\n        raster file\\n\\n    Parameters:\\n    -----------\\n    resampling : str\\n        rasterio.Resampling method\\n    scale_method : str\\n        - dtype_scale: use dtype minimum and maximum values\\n        - minmax_scale: use dataset bands minimum and maximum values\\n        - crop: clip data to output dtype\\n    scales_minmax : tuple\\n        tuple of band specific scale values\\n\\n    Output:\\n    -------\\n    np.ndarray\\n    \"\"\"\\n    with mp.open(\"raster\", resampling=resampling) as raster_file:\\n\\n        # exit if input tile is empty\\n        if raster_file.is_empty():\\n            return \"empty\"\\n\\n        # actually read data and iterate through bands\\n        scaled = ()\\n        mask = ()\\n        raster_data = raster_file.read()\\n        if raster_data.ndim == 2:\\n            raster_data = ma.expand_dims(raster_data, axis=0)\\n        if not scale_method:\\n            scales_minmax = [(i, i) for i in range(len(raster_data))]\\n\\n        for band, (scale_min, scale_max) in zip(raster_data, scales_minmax):\\n            if scale_method in [\"dtype_scale\", \"minmax_scale\"]:\\n                scaled += (_stretch_array(band, scale_min, scale_max), )\\n            elif scale_method == \"crop\":\\n                scaled += (np.clip(band, scale_min, scale_max), )\\n            else:\\n                scaled += (band, )\\n            mask += (band.mask, )\\n\\n    return ma.masked_array(np.stack(scaled), np.stack(mask))', 'def open(self, tile, process, **kwargs):\\n        \"\"\"\\n        Open process output as input for other process.\\n\\n        Parameters\\n        ----------\\n        tile : ``Tile``\\n        process : ``MapcheteProcess``\\n        kwargs : keyword arguments\\n        \"\"\"\\n        return InputTile(tile, process, kwargs.get(\"resampling\", None))', 'def for_web(self, data):\\n        \"\"\"\\n        Convert data to web output.\\n\\n        Parameters\\n        ----------\\n        data : array\\n\\n        Returns\\n        -------\\n        web data : array\\n        \"\"\"\\n        rgba = self._prepare_array_for_png(data)\\n        data = ma.masked_where(rgba == self.nodata, rgba)\\n        return memory_file(data, self.profile()), \\'image/png\\'', 'def serve(\\n    mapchete_file,\\n    port=None,\\n    internal_cache=None,\\n    zoom=None,\\n    bounds=None,\\n    overwrite=False,\\n    readonly=False,\\n    memory=False,\\n    input_file=None,\\n    debug=False,\\n    logfile=None\\n):\\n    \"\"\"\\n    Serve a Mapchete process.\\n\\n    Creates the Mapchete host and serves both web page with OpenLayers and the\\n    WMTS simple REST endpoint.\\n    \"\"\"\\n    app = create_app(\\n        mapchete_files=[mapchete_file], zoom=zoom,\\n        bounds=bounds, single_input_file=input_file,\\n        mode=_get_mode(memory, readonly, overwrite), debug=debug\\n    )\\n    if os.environ.get(\"MAPCHETE_TEST\") == \"TRUE\":\\n        logger.debug(\"don\\'t run flask app, MAPCHETE_TEST environment detected\")\\n    else:\\n        app.run(\\n            threaded=True, debug=True, port=port, host=\\'0.0.0.0\\',\\n            extra_files=[mapchete_file]\\n        )', 'def create_app(\\n    mapchete_files=None, zoom=None, bounds=None, single_input_file=None,\\n    mode=\"continue\", debug=None\\n):\\n    \"\"\"Configure and create Flask app.\"\"\"\\n    from flask import Flask, render_template_string\\n    app = Flask(__name__)\\n    mapchete_processes = {\\n        os.path.splitext(os.path.basename(mapchete_file))[0]: mapchete.open(\\n            mapchete_file, zoom=zoom, bounds=bounds,\\n            single_input_file=single_input_file, mode=mode, with_cache=True,\\n            debug=debug)\\n        for mapchete_file in mapchete_files\\n    }\\n\\n    mp = next(iter(mapchete_processes.values()))\\n    pyramid_type = mp.config.process_pyramid.grid\\n    pyramid_srid = mp.config.process_pyramid.crs.to_epsg()\\n    process_bounds = \",\".join([str(i) for i in mp.config.bounds_at_zoom()])\\n    grid = \"g\" if pyramid_srid == 3857 else \"WGS84\"\\n    web_pyramid = BufferedTilePyramid(pyramid_type)\\n\\n    @app.route(\\'/\\', methods=[\\'GET\\'])\\n    def index():\\n        \"\"\"Render and hosts the appropriate OpenLayers instance.\"\"\"\\n        return render_template_string(\\n            pkgutil.get_data(\\n                \\'mapchete.static\\', \\'index.html\\').decode(\"utf-8\"),\\n            srid=pyramid_srid,\\n            process_bounds=process_bounds,\\n            is_mercator=(pyramid_srid == 3857),\\n            process_names=mapchete_processes.keys()\\n        )\\n\\n    @app.route(\\n        \"/\".join([\\n            \"\", \"wmts_simple\", \"1.0.0\", \"<string:mp_name>\", \"default\",\\n            grid, \"<int:zoom>\", \"<int:row>\", \"<int:col>.<string:file_ext>\"]),\\n        methods=[\\'GET\\'])\\n    def get(mp_name, zoom, row, col, file_ext):\\n        \"\"\"Return processed, empty or error (in pink color) tile.\"\"\"\\n        logger.debug(\\n            \"received tile (%s, %s, %s) for process %s\", zoom, row, col,\\n            mp_name)\\n        # convert zoom, row, col into tile object using web pyramid\\n        return _tile_response(\\n            mapchete_processes[mp_name], web_pyramid.tile(zoom, row, col),\\n            debug)\\n\\n    return app', 'def read_raster_window(\\n    input_files,\\n    tile,\\n    indexes=None,\\n    resampling=\"nearest\",\\n    src_nodata=None,\\n    dst_nodata=None,\\n    gdal_opts=None\\n):\\n    \"\"\"\\n    Return NumPy arrays from an input raster.\\n\\n    NumPy arrays are reprojected and resampled to tile properties from input\\n    raster. If tile boundaries cross the antimeridian, data on the other side\\n    of the antimeridian will be read and concatenated to the numpy array\\n    accordingly.\\n\\n    Parameters\\n    ----------\\n    input_files : string or list\\n        path to a raster file or list of paths to multiple raster files readable by\\n        rasterio.\\n    tile : Tile\\n        a Tile object\\n    indexes : list or int\\n        a list of band numbers; None will read all.\\n    resampling : string\\n        one of \"nearest\", \"average\", \"bilinear\" or \"lanczos\"\\n    src_nodata : int or float, optional\\n        if not set, the nodata value from the source dataset will be used\\n    dst_nodata : int or float, optional\\n        if not set, the nodata value from the source dataset will be used\\n    gdal_opts : dict\\n        GDAL options passed on to rasterio.Env()\\n\\n    Returns\\n    -------\\n    raster : MaskedArray\\n    \"\"\"\\n    with rasterio.Env(\\n        **get_gdal_options(\\n            gdal_opts,\\n            is_remote=path_is_remote(\\n                input_files[0] if isinstance(input_files, list) else input_files, s3=True\\n            )\\n        )\\n    ) as env:\\n        logger.debug(\"reading %s with GDAL options %s\", input_files, env.options)\\n        return _read_raster_window(\\n            input_files,\\n            tile,\\n            indexes=indexes,\\n            resampling=resampling,\\n            src_nodata=src_nodata,\\n            dst_nodata=dst_nodata\\n        )', 'def _get_warped_array(\\n    input_file=None,\\n    indexes=None,\\n    dst_bounds=None,\\n    dst_shape=None,\\n    dst_crs=None,\\n    resampling=None,\\n    src_nodata=None,\\n    dst_nodata=None\\n):\\n    \"\"\"Extract a numpy array from a raster file.\"\"\"\\n    try:\\n        return _rasterio_read(\\n            input_file=input_file,\\n            indexes=indexes,\\n            dst_bounds=dst_bounds,\\n            dst_shape=dst_shape,\\n            dst_crs=dst_crs,\\n            resampling=resampling,\\n            src_nodata=src_nodata,\\n            dst_nodata=dst_nodata\\n        )\\n    except Exception as e:\\n        logger.exception(\"error while reading file %s: %s\", input_file, e)\\n        raise', 'def write_raster_window(\\n    in_tile=None, in_data=None, out_profile=None, out_tile=None, out_path=None,\\n    tags=None, bucket_resource=None\\n):\\n    \"\"\"\\n    Write a window from a numpy array to an output file.\\n\\n    Parameters\\n    ----------\\n    in_tile : ``BufferedTile``\\n        ``BufferedTile`` with a data attribute holding NumPy data\\n    in_data : array\\n    out_profile : dictionary\\n        metadata dictionary for rasterio\\n    out_tile : ``Tile``\\n        provides output boundaries; if None, in_tile is used\\n    out_path : string\\n        output path to write to\\n    tags : optional tags to be added to GeoTIFF file\\n    bucket_resource : boto3 bucket resource to write to in case of S3 output\\n    \"\"\"\\n    if not isinstance(out_path, str):\\n        raise TypeError(\"out_path must be a string\")\\n    logger.debug(\"write %s\", out_path)\\n    if out_path == \"memoryfile\":\\n        raise DeprecationWarning(\\n            \"Writing to memoryfile with write_raster_window() is deprecated. \"\\n            \"Please use RasterWindowMemoryFile.\"\\n        )\\n    out_tile = in_tile if out_tile is None else out_tile\\n    _validate_write_window_params(in_tile, out_tile, in_data, out_profile)\\n\\n    # extract data\\n    window_data = extract_from_array(\\n        in_raster=in_data,\\n        in_affine=in_tile.affine,\\n        out_tile=out_tile\\n    ) if in_tile != out_tile else in_data\\n\\n    # use transform instead of affine\\n    if \"affine\" in out_profile:\\n        out_profile[\"transform\"] = out_profile.pop(\"affine\")\\n\\n    # write if there is any band with non-masked data\\n    if window_data.all() is not ma.masked:\\n\\n        try:\\n            if out_path.startswith(\"s3://\"):\\n                with RasterWindowMemoryFile(\\n                    in_tile=out_tile,\\n                    in_data=window_data,\\n                    out_profile=out_profile,\\n                    out_tile=out_tile,\\n                    tags=tags\\n                ) as memfile:\\n                    logger.debug((out_tile.id, \"upload tile\", out_path))\\n                    bucket_resource.put_object(\\n                        Key=\"/\".join(out_path.split(\"/\")[3:]),\\n                        Body=memfile\\n                    )\\n            else:\\n                with rasterio.open(out_path, \\'w\\', **out_profile) as dst:\\n                    logger.debug((out_tile.id, \"write tile\", out_path))\\n                    dst.write(window_data.astype(out_profile[\"dtype\"], copy=False))\\n                    _write_tags(dst, tags)\\n        except Exception as e:\\n            logger.exception(\"error while writing file %s: %s\", out_path, e)\\n            raise\\n    else:\\n        logger.debug((out_tile.id, \"array window empty\", out_path))', 'def extract_from_array(in_raster=None, in_affine=None, out_tile=None):\\n    \"\"\"\\n    Extract raster data window array.\\n\\n    Parameters\\n    ----------\\n    in_raster : array or ReferencedRaster\\n    in_affine : ``Affine`` required if in_raster is an array\\n    out_tile : ``BufferedTile``\\n\\n    Returns\\n    -------\\n    extracted array : array\\n    \"\"\"\\n    if isinstance(in_raster, ReferencedRaster):\\n        in_affine = in_raster.affine\\n        in_raster = in_raster.data\\n\\n    # get range within array\\n    minrow, maxrow, mincol, maxcol = bounds_to_ranges(\\n        out_bounds=out_tile.bounds, in_affine=in_affine, in_shape=in_raster.shape\\n    )\\n    # if output window is within input window\\n    if (\\n        minrow >= 0 and\\n        mincol >= 0 and\\n        maxrow <= in_raster.shape[-2] and\\n        maxcol <= in_raster.shape[-1]\\n    ):\\n        return in_raster[..., minrow:maxrow, mincol:maxcol]\\n    # raise error if output is not fully within input\\n    else:\\n        raise ValueError(\"extraction fails if output shape is not within input\")']}\n",
      "<class 'datasets.formatting.formatting.LazyBatch'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=5):   0%|          | 0/13788 [00:00<?, ? examples/s]\u001b[A\n",
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [8274, 8275, 8276, 8277, 8278, 8279, 8280, 8281, 8282, 8283, 8284, 8285, 8286, 8287, 8288, 8289, 8290, 8291, 8292, 8293, 8294, 8295, 8296, 8297, 8298, 8299, 8300, 8301, 8302, 8303, 8304, 8305, 8306, 8307, 8308, 8309, 8310, 8311, 8312, 8313], 'query_id': ['q260161', 'q260162', 'q260164', 'q260165', 'q260166', 'q260167', 'q260168', 'q260169', 'q260170', 'q260171', 'q260172', 'q260173', 'q260174', 'q260176', 'q260177', 'q260178', 'q260179', 'q260180', 'q260181', 'q260182', 'q260183', 'q260184', 'q260185', 'q260186', 'q260187', 'q260188', 'q260189', 'q260190', 'q260191', 'q260192', 'q260193', 'q260194', 'q260195', 'q260196', 'q260197', 'q260198', 'q260199', 'q260200', 'q260201', 'q260202'], 'corpus_id': ['c260094', 'c260095', 'c260096', 'c260097', 'c260098', 'c260099', 'c260100', 'c260101', 'c260102', 'c260103', 'c260104', 'c260105', 'c260106', 'c260107', 'c260108', 'c260109', 'c260110', 'c260111', 'c260112', 'c260113', 'c260114', 'c260115', 'c260116', 'c260117', 'c260118', 'c260119', 'c260120', 'c260121', 'c260122', 'c260123', 'c260124', 'c260125', 'c260126', 'c260127', 'c260128', 'c260129', 'c260130', 'c260131', 'c260132', 'c260133'], 'score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'doc': ['Cheap function to invert a hash.', 'Used in a call to re.sub to replace HTML, XML, and numeric\\n        entities with the appropriate Unicode characters. If HTML\\n        entities are being converted, any unrecognized entities are\\n        escaped.', 'Renders the contents of this tag as a string in the given\\n        encoding. If encoding is None, returns a Unicode string..', 'Return only the first child of this Tag matching the given\\n        criteria.', \"Extracts a list of Tag objects that match the given\\n        criteria.  You can specify the name of the Tag and any\\n        attributes you want the Tag to have.\\n\\n        The value of a key-value pair in the 'attrs' map can be a\\n        string, a list of strings, a regular expression object, or a\\n        callable that takes a string and returns whether or not the\\n        string matches for some custom definition of 'matches'. The\\n        same is true of the tag name.\", \"Initializes a map representation of this tag's attributes,\\n        if not already initialized.\", \"This method fixes a bug in Python's SGMLParser.\", 'Returns true iff the given string is the name of a\\n        self-closing tag according to this parser.', 'Adds a certain piece of text to the tree as a NavigableString\\n        subclass.', 'Handle a processing instruction as a ProcessingInstruction\\n        object, possibly one with a %SOUP-ENCODING% slot into which an\\n        encoding will be plugged later.', 'Handle character references as data.', 'Treat a bogus SGML declaration as raw data. Treat a CDATA\\n        declaration as a CData object.', 'Beautiful Soup can detect a charset included in a META tag,\\n        try to convert the document to that charset, and re-parse the\\n        document from the beginning.', 'Given a string and its encoding, decodes the string into Unicode.\\n        %encoding is a string recognized by encodings.aliases', 'Given a document, tries to detect its XML encoding.', 'Decorator to run some code in a bot instance.', 'Scale context based on difference between bot size and widget', 'Draw just the exposed part of the backing store, scaled to fit', 'Creates a recording surface for the bot to draw on\\n\\n        :param size: The width and height of bot', 'Return a JSON string representation of a Python data structure.\\n\\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\\n        \\'{\"foo\": [\"bar\", \"baz\"]}\\'', 'Return a dict in the form of\\n\\n        SHOEBOT_KEY_NAME, GTK_VALUE\\n\\n        Shoebot key names look like KEY_LEFT, whereas Gdk uses KEY_Left\\n        - Shoebot key names are derived from Nodebox 1, which was a mac\\n          app.', 'If filename was used output a filename, along with multifile\\n        numbered filenames will be used.\\n\\n        If buff was specified it is returned.\\n\\n        :return: Output buff or filename.', 'Called when CairoCanvas needs a cairo context to draw on', 'Called when CairoCanvas has rendered a bot', 'Function to output to a cairo surface\\n\\n        target is a cairo Context or filename\\n        if file_number is set, then files will be numbered\\n        (this is usually set to the current frame number)', 'Strips links from the definition and gathers them in a links property.', 'Create canvas and sink for attachment to a bot\\n\\n    canvas is what draws images, \\'sink\\' is the final consumer of the images\\n\\n    :param src: Defaults for title or outputfile if not specified.\\n\\n    :param format: CairoImageSink image format, if using buff instead of outputfile\\n    :param buff: CairoImageSink buffer object to send output to\\n\\n    :param outputfile: CairoImageSink output filename e.g. \"hello.svg\"\\n    :param multifile: CairoImageSink if True,\\n\\n    :param title: ShoebotWindow - set window title\\n    :param fullscreen: ShoebotWindow - set window title\\n    :param show_vars: ShoebotWindow - display variable window\\n\\n    Two kinds of sink are provided: CairoImageSink and ShoebotWindow\\n\\n    ShoebotWindow\\n\\n    Displays a window to draw shoebot inside.\\n\\n\\n    CairoImageSink\\n\\n    Output to a filename (or files if multifile is set), or a buffer object.', 'Create a canvas and a bot with the same canvas attached to it\\n\\n    bot parameters\\n    :param grammar: DRAWBOT or NODEBOX - decides what kind of bot is created\\n    :param vars: preset dictionary of vars from the called\\n\\n    canvas parameters:\\n    ... everything else ...\\n\\n    See create_canvas for details on those parameters.', 'Create and run a bot, the arguments all correspond to sanitized\\n    commandline options.\\n\\n    :param background_thread: If True then use a background thread.\\n\\n\\n    Other args are split into create_args and run_args\\n\\n    See create_bot for details on create_args\\n\\n    run_args are passed to bot.run - see Nodebot.run or Drawbot.run\\n\\n\\n\\n    Background thread:\\n\\n    readline in python is blocking, running the app in a background\\n    thread opens up the main thread for IO on stdin/stdout, which\\n    can be used for communication with shoebot when livecoding is\\n    enabled.\\n\\n    See shoebot.io for implementation of the shell, and the gedit\\n    plugin for an example of using livecoding.', 'Return True if the buffer was saved', 'Called when a slider is adjusted.', 'var was added in the bot while it ran, possibly\\n        by livecoding\\n\\n        :param v:\\n        :return:', 'var was added in the bot\\n\\n        :param v:\\n        :return:', 'Returns cached copies unless otherwise specified.', \"Returns XML element's attribute, or default if none.\", 'Expand the path with color information.\\n    \\n    Attempts to extract fill and stroke colors\\n    from the element and adds it to path attributes.', 'Returns a copy of the event handler, remembering the last node clicked.', 'Drags given node to mouse location.', 'Displays a popup when hovering over a node.', 'Returns a cached textpath of the given text in queue.'], 'code': ['def _invert(h):\\n        \"Cheap function to invert a hash.\"\\n        i = {}\\n        for k,v in h.items():\\n            i[v] = k\\n        return i', 'def _convertEntities(self, match):\\n        \"\"\"Used in a call to re.sub to replace HTML, XML, and numeric\\n        entities with the appropriate Unicode characters. If HTML\\n        entities are being converted, any unrecognized entities are\\n        escaped.\"\"\"\\n        x = match.group(1)\\n        if self.convertHTMLEntities and x in name2codepoint:\\n            return unichr(name2codepoint[x])\\n        elif x in self.XML_ENTITIES_TO_SPECIAL_CHARS:\\n            if self.convertXMLEntities:\\n                return self.XML_ENTITIES_TO_SPECIAL_CHARS[x]\\n            else:\\n                return u\\'&%s;\\' % x\\n        elif len(x) > 0 and x[0] == \\'#\\':\\n            # Handle numeric entities\\n            if len(x) > 1 and x[1] == \\'x\\':\\n                return unichr(int(x[2:], 16))\\n            else:\\n                return unichr(int(x[1:]))\\n\\n        elif self.escapeUnrecognizedEntities:\\n            return u\\'&amp;%s;\\' % x\\n        else:\\n            return u\\'&%s;\\' % x', 'def renderContents(self, encoding=DEFAULT_OUTPUT_ENCODING,\\n                       prettyPrint=False, indentLevel=0):\\n        \"\"\"Renders the contents of this tag as a string in the given\\n        encoding. If encoding is None, returns a Unicode string..\"\"\"\\n        s=[]\\n        for c in self:\\n            text = None\\n            if isinstance(c, NavigableString):\\n                text = c.__str__(encoding)\\n            elif isinstance(c, Tag):\\n                s.append(c.__str__(encoding, prettyPrint, indentLevel))\\n            if text and prettyPrint:\\n                text = text.strip()\\n            if text:\\n                if prettyPrint:\\n                    s.append(\" \" * (indentLevel-1))\\n                s.append(text)\\n                if prettyPrint:\\n                    s.append(\"\\\\n\")\\n        return \\'\\'.join(s)', 'def find(self, name=None, attrs={}, recursive=True, text=None,\\n             **kwargs):\\n        \"\"\"Return only the first child of this Tag matching the given\\n        criteria.\"\"\"\\n        r = None\\n        l = self.findAll(name, attrs, recursive, text, 1, **kwargs)\\n        if l:\\n            r = l[0]\\n        return r', 'def findAll(self, name=None, attrs={}, recursive=True, text=None,\\n                limit=None, **kwargs):\\n        \"\"\"Extracts a list of Tag objects that match the given\\n        criteria.  You can specify the name of the Tag and any\\n        attributes you want the Tag to have.\\n\\n        The value of a key-value pair in the \\'attrs\\' map can be a\\n        string, a list of strings, a regular expression object, or a\\n        callable that takes a string and returns whether or not the\\n        string matches for some custom definition of \\'matches\\'. The\\n        same is true of the tag name.\"\"\"\\n        generator = self.recursiveChildGenerator\\n        if not recursive:\\n            generator = self.childGenerator\\n        return self._findAll(name, attrs, text, limit, generator, **kwargs)', 'def _getAttrMap(self):\\n        \"\"\"Initializes a map representation of this tag\\'s attributes,\\n        if not already initialized.\"\"\"\\n        if not getattr(self, \\'attrMap\\'):\\n            self.attrMap = {}\\n            for (key, value) in self.attrs:\\n                self.attrMap[key] = value\\n        return self.attrMap', 'def convert_charref(self, name):\\n        \"\"\"This method fixes a bug in Python\\'s SGMLParser.\"\"\"\\n        try:\\n            n = int(name)\\n        except ValueError:\\n            return\\n        if not 0 <= n <= 127 : # ASCII ends at 127, not 255\\n            return\\n        return self.convert_codepoint(n)', 'def isSelfClosingTag(self, name):\\n        \"\"\"Returns true iff the given string is the name of a\\n        self-closing tag according to this parser.\"\"\"\\n        return self.SELF_CLOSING_TAGS.has_key(name) \\\\\\n               or self.instanceSelfClosingTags.has_key(name)', 'def _toStringSubclass(self, text, subclass):\\n        \"\"\"Adds a certain piece of text to the tree as a NavigableString\\n        subclass.\"\"\"\\n        self.endData()\\n        self.handle_data(text)\\n        self.endData(subclass)', 'def handle_pi(self, text):\\n        \"\"\"Handle a processing instruction as a ProcessingInstruction\\n        object, possibly one with a %SOUP-ENCODING% slot into which an\\n        encoding will be plugged later.\"\"\"\\n        if text[:3] == \"xml\":\\n            text = u\"xml version=\\'1.0\\' encoding=\\'%SOUP-ENCODING%\\'\"\\n        self._toStringSubclass(text, ProcessingInstruction)', 'def handle_charref(self, ref):\\n        \"Handle character references as data.\"\\n        if self.convertEntities:\\n            data = unichr(int(ref))\\n        else:\\n            data = \\'&#%s;\\' % ref\\n        self.handle_data(data)', 'def parse_declaration(self, i):\\n        \"\"\"Treat a bogus SGML declaration as raw data. Treat a CDATA\\n        declaration as a CData object.\"\"\"\\n        j = None\\n        if self.rawdata[i:i+9] == \\'<![CDATA[\\':\\n             k = self.rawdata.find(\\']]>\\', i)\\n             if k == -1:\\n                 k = len(self.rawdata)\\n             data = self.rawdata[i+9:k]\\n             j = k+3\\n             self._toStringSubclass(data, CData)\\n        else:\\n            try:\\n                j = SGMLParser.parse_declaration(self, i)\\n            except SGMLParseError:\\n                toHandle = self.rawdata[i:]\\n                self.handle_data(toHandle)\\n                j = i + len(toHandle)\\n        return j', 'def start_meta(self, attrs):\\n        \"\"\"Beautiful Soup can detect a charset included in a META tag,\\n        try to convert the document to that charset, and re-parse the\\n        document from the beginning.\"\"\"\\n        httpEquiv = None\\n        contentType = None\\n        contentTypeIndex = None\\n        tagNeedsEncodingSubstitution = False\\n\\n        for i in range(0, len(attrs)):\\n            key, value = attrs[i]\\n            key = key.lower()\\n            if key == \\'http-equiv\\':\\n                httpEquiv = value\\n            elif key == \\'content\\':\\n                contentType = value\\n                contentTypeIndex = i\\n\\n        if httpEquiv and contentType: # It\\'s an interesting meta tag.\\n            match = self.CHARSET_RE.search(contentType)\\n            if match:\\n                if (self.declaredHTMLEncoding is not None or\\n                    self.originalEncoding == self.fromEncoding):\\n                    # An HTML encoding was sniffed while converting\\n                    # the document to Unicode, or an HTML encoding was\\n                    # sniffed during a previous pass through the\\n                    # document, or an encoding was specified\\n                    # explicitly and it worked. Rewrite the meta tag.\\n                    def rewrite(match):\\n                        return match.group(1) + \"%SOUP-ENCODING%\"\\n                    newAttr = self.CHARSET_RE.sub(rewrite, contentType)\\n                    attrs[contentTypeIndex] = (attrs[contentTypeIndex][0],\\n                                               newAttr)\\n                    tagNeedsEncodingSubstitution = True\\n                else:\\n                    # This is our first pass through the document.\\n                    # Go through it again with the encoding information.\\n                    newCharset = match.group(3)\\n                    if newCharset and newCharset != self.originalEncoding:\\n                        self.declaredHTMLEncoding = newCharset\\n                        self._feed(self.declaredHTMLEncoding)\\n                        raise StopParsing\\n                    pass\\n        tag = self.unknown_starttag(\"meta\", attrs)\\n        if tag and tagNeedsEncodingSubstitution:\\n            tag.containsSubstitutions = True', \"def _toUnicode(self, data, encoding):\\n        '''Given a string and its encoding, decodes the string into Unicode.\\n        %encoding is a string recognized by encodings.aliases'''\\n\\n        # strip Byte Order Mark (if present)\\n        if (len(data) >= 4) and (data[:2] == '\\\\xfe\\\\xff') \\\\\\n               and (data[2:4] != '\\\\x00\\\\x00'):\\n            encoding = 'utf-16be'\\n            data = data[2:]\\n        elif (len(data) >= 4) and (data[:2] == '\\\\xff\\\\xfe') \\\\\\n                 and (data[2:4] != '\\\\x00\\\\x00'):\\n            encoding = 'utf-16le'\\n            data = data[2:]\\n        elif data[:3] == '\\\\xef\\\\xbb\\\\xbf':\\n            encoding = 'utf-8'\\n            data = data[3:]\\n        elif data[:4] == '\\\\x00\\\\x00\\\\xfe\\\\xff':\\n            encoding = 'utf-32be'\\n            data = data[4:]\\n        elif data[:4] == '\\\\xff\\\\xfe\\\\x00\\\\x00':\\n            encoding = 'utf-32le'\\n            data = data[4:]\\n        newdata = unicode(data, encoding)\\n        return newdata\", 'def _detectEncoding(self, xml_data, isHTML=False):\\n        \"\"\"Given a document, tries to detect its XML encoding.\"\"\"\\n        xml_encoding = sniffed_xml_encoding = None\\n        try:\\n            if xml_data[:4] == \\'\\\\x4c\\\\x6f\\\\xa7\\\\x94\\':\\n                # EBCDIC\\n                xml_data = self._ebcdic_to_ascii(xml_data)\\n            elif xml_data[:4] == \\'\\\\x00\\\\x3c\\\\x00\\\\x3f\\':\\n                # UTF-16BE\\n                sniffed_xml_encoding = \\'utf-16be\\'\\n                xml_data = unicode(xml_data, \\'utf-16be\\').encode(\\'utf-8\\')\\n            elif (len(xml_data) >= 4) and (xml_data[:2] == \\'\\\\xfe\\\\xff\\') \\\\\\n                     and (xml_data[2:4] != \\'\\\\x00\\\\x00\\'):\\n                # UTF-16BE with BOM\\n                sniffed_xml_encoding = \\'utf-16be\\'\\n                xml_data = unicode(xml_data[2:], \\'utf-16be\\').encode(\\'utf-8\\')\\n            elif xml_data[:4] == \\'\\\\x3c\\\\x00\\\\x3f\\\\x00\\':\\n                # UTF-16LE\\n                sniffed_xml_encoding = \\'utf-16le\\'\\n                xml_data = unicode(xml_data, \\'utf-16le\\').encode(\\'utf-8\\')\\n            elif (len(xml_data) >= 4) and (xml_data[:2] == \\'\\\\xff\\\\xfe\\') and \\\\\\n                     (xml_data[2:4] != \\'\\\\x00\\\\x00\\'):\\n                # UTF-16LE with BOM\\n                sniffed_xml_encoding = \\'utf-16le\\'\\n                xml_data = unicode(xml_data[2:], \\'utf-16le\\').encode(\\'utf-8\\')\\n            elif xml_data[:4] == \\'\\\\x00\\\\x00\\\\x00\\\\x3c\\':\\n                # UTF-32BE\\n                sniffed_xml_encoding = \\'utf-32be\\'\\n                xml_data = unicode(xml_data, \\'utf-32be\\').encode(\\'utf-8\\')\\n            elif xml_data[:4] == \\'\\\\x3c\\\\x00\\\\x00\\\\x00\\':\\n                # UTF-32LE\\n                sniffed_xml_encoding = \\'utf-32le\\'\\n                xml_data = unicode(xml_data, \\'utf-32le\\').encode(\\'utf-8\\')\\n            elif xml_data[:4] == \\'\\\\x00\\\\x00\\\\xfe\\\\xff\\':\\n                # UTF-32BE with BOM\\n                sniffed_xml_encoding = \\'utf-32be\\'\\n                xml_data = unicode(xml_data[4:], \\'utf-32be\\').encode(\\'utf-8\\')\\n            elif xml_data[:4] == \\'\\\\xff\\\\xfe\\\\x00\\\\x00\\':\\n                # UTF-32LE with BOM\\n                sniffed_xml_encoding = \\'utf-32le\\'\\n                xml_data = unicode(xml_data[4:], \\'utf-32le\\').encode(\\'utf-8\\')\\n            elif xml_data[:3] == \\'\\\\xef\\\\xbb\\\\xbf\\':\\n                # UTF-8 with BOM\\n                sniffed_xml_encoding = \\'utf-8\\'\\n                xml_data = unicode(xml_data[3:], \\'utf-8\\').encode(\\'utf-8\\')\\n            else:\\n                sniffed_xml_encoding = \\'ascii\\'\\n                pass\\n        except:\\n            xml_encoding_match = None\\n        xml_encoding_match = re.compile(\\n            \\'^<\\\\?.*encoding=[\\\\\\'\"](.*?)[\\\\\\'\"].*\\\\?>\\').match(xml_data)\\n        if not xml_encoding_match and isHTML:\\n            regexp = re.compile(\\'<\\\\s*meta[^>]+charset=([^>]*?)[;\\\\\\'\">]\\', re.I)\\n            xml_encoding_match = regexp.search(xml_data)\\n        if xml_encoding_match is not None:\\n            xml_encoding = xml_encoding_match.groups()[0].lower()\\n            if isHTML:\\n                self.declaredHTMLEncoding = xml_encoding\\n            if sniffed_xml_encoding and \\\\\\n               (xml_encoding in (\\'iso-10646-ucs-2\\', \\'ucs-2\\', \\'csunicode\\',\\n                                 \\'iso-10646-ucs-4\\', \\'ucs-4\\', \\'csucs4\\',\\n                                 \\'utf-16\\', \\'utf-32\\', \\'utf_16\\', \\'utf_32\\',\\n                                 \\'utf16\\', \\'u16\\')):\\n                xml_encoding = sniffed_xml_encoding\\n        return xml_data, xml_encoding, sniffed_xml_encoding', 'def shoebot_example(**shoebot_kwargs):\\n    \"\"\"\\n    Decorator to run some code in a bot instance.\\n    \"\"\"\\n\\n    def decorator(f):\\n        def run():\\n            from shoebot import ShoebotInstallError  # https://github.com/shoebot/shoebot/issues/206\\n            print(\"    Shoebot - %s:\" % f.__name__.replace(\"_\", \" \"))\\n            try:\\n                import shoebot\\n                outputfile = \"/tmp/shoebot-%s.png\" % f.__name__\\n                bot = shoebot.create_bot(outputfile=outputfile)\\n                f(bot)\\n                bot.finish()\\n                print(\\'        [passed] : %s\\' % outputfile)\\n                print(\\'\\')\\n            except ShoebotInstallError as e:\\n                print(\\'        [failed]\\', e.args[0])\\n                print(\\'\\')\\n            except Exception:\\n                print(\\'        [failed] - traceback:\\')\\n                for line in traceback.format_exc().splitlines():\\n                    print(\\'    %s\\' % line)\\n                print(\\'\\')\\n\\n        return run\\n\\n    return decorator', 'def scale_context_and_center(self, cr):\\n        \"\"\"\\n        Scale context based on difference between bot size and widget\\n        \"\"\"\\n        bot_width, bot_height = self.bot_size\\n        if self.width != bot_width or self.height != bot_height:\\n            # Scale up by largest dimension\\n            if self.width < self.height:\\n                scale_x = float(self.width) / float(bot_width)\\n                scale_y = scale_x\\n                cr.translate(0, (self.height - (bot_height * scale_y)) / 2.0)\\n            elif self.width > self.height:\\n                scale_y = float(self.height) / float(bot_height)\\n                scale_x = scale_y\\n                cr.translate((self.width - (bot_width * scale_x)) / 2.0, 0)\\n            else:\\n                scale_x = 1.0\\n                scale_y = 1.0\\n            cr.scale(scale_x, scale_y)\\n            self.input_device.scale_x = scale_y\\n            self.input_device.scale_y = scale_y', \"def draw(self, widget, cr):\\n        '''\\n        Draw just the exposed part of the backing store, scaled to fit\\n        '''\\n        if self.bot_size is None:\\n            # No bot to draw yet.\\n            self.draw_default_image(cr)\\n            return\\n\\n        cr = driver.ensure_pycairo_context(cr)\\n        \\n        surface = self.backing_store.surface\\n        cr.set_source_surface(surface)\\n        cr.paint()\", \"def create_rcontext(self, size, frame):\\n        '''\\n        Creates a recording surface for the bot to draw on\\n\\n        :param size: The width and height of bot\\n        '''\\n        self.frame = frame\\n        width, height = size\\n        meta_surface = cairo.RecordingSurface(cairo.CONTENT_COLOR_ALPHA, (0, 0, width, height))\\n\\n        ctx = cairo.Context(meta_surface)\\n        return ctx\", 'def encode(self, o):\\n        \"\"\"\\n        Return a JSON string representation of a Python data structure.\\n\\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\\n        \\'{\"foo\": [\"bar\", \"baz\"]}\\'\\n        \"\"\"\\n        # This is for extremely simple cases and benchmarks.\\n        if isinstance(o, basestring):\\n            if isinstance(o, str):\\n                _encoding = self.encoding\\n                if (_encoding is not None \\n                        and not (_encoding == \\'utf-8\\')):\\n                    o = o.decode(_encoding)\\n            if self.ensure_ascii:\\n                return encode_basestring_ascii(o)\\n            else:\\n                return encode_basestring(o)\\n        # This doesn\\'t pass the iterator directly to \\'\\'.join() because the\\n        # exceptions aren\\'t as detailed.  The list call should be roughly\\n        # equivalent to the PySequence_Fast that \\'\\'.join() would do.\\n        chunks = list(self.iterencode(o))\\n        return \\'\\'.join(chunks)', \"def get_key_map(self):\\n        '''\\n        Return a dict in the form of\\n\\n        SHOEBOT_KEY_NAME, GTK_VALUE\\n\\n        Shoebot key names look like KEY_LEFT, whereas Gdk uses KEY_Left\\n        - Shoebot key names are derived from Nodebox 1, which was a mac\\n          app.\\n        '''\\n        kdict = {}\\n        for gdk_name in dir(Gdk):\\n            nb_name = gdk_name.upper()\\n            kdict[nb_name] = getattr(Gdk, gdk_name)\\n        return kdict\", 'def _output_file(self, frame):\\n        \"\"\"\\n        If filename was used output a filename, along with multifile\\n        numbered filenames will be used.\\n\\n        If buff was specified it is returned.\\n\\n        :return: Output buff or filename.\\n        \"\"\"\\n        if self.buff:\\n            return self.buff\\n        elif self.multifile:\\n            return self.file_root + \"_%03d\" % frame + self.file_ext\\n        else:\\n            return self.filename', 'def create_rcontext(self, size, frame):\\n        \"\"\"\\n        Called when CairoCanvas needs a cairo context to draw on\\n        \"\"\"\\n        if self.format == \\'pdf\\':\\n            surface = cairo.PDFSurface(self._output_file(frame), *size)\\n        elif self.format in (\\'ps\\', \\'eps\\'):\\n            surface = cairo.PSSurface(self._output_file(frame), *size)\\n        elif self.format == \\'svg\\':\\n            surface = cairo.SVGSurface(self._output_file(frame), *size)\\n        elif self.format == \\'surface\\':\\n            surface = self.target\\n        else:\\n            surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, *size)\\n        return cairo.Context(surface)', 'def rendering_finished(self, size, frame, cairo_ctx):\\n        \"\"\"\\n        Called when CairoCanvas has rendered a bot\\n        \"\"\"\\n        surface = cairo_ctx.get_target()\\n        if self.format == \\'png\\':\\n            surface.write_to_png(self._output_file(frame))\\n        surface.finish()\\n        surface.flush()', \"def output_closure(self, target, file_number=None):\\n        '''\\n        Function to output to a cairo surface\\n\\n        target is a cairo Context or filename\\n        if file_number is set, then files will be numbered\\n        (this is usually set to the current frame number)\\n        '''\\n        def output_context(ctx):\\n            target_ctx = target\\n            target_ctx.set_source_surface(ctx.get_target())\\n            target_ctx.paint()\\n            return target_ctx\\n\\n        def output_surface(ctx):\\n            target_ctx = cairo.Context(target)\\n            target_ctx.set_source_surface(ctx.get_target())\\n            target_ctx.paint()\\n            return target_ctx\\n\\n        def output_file(ctx):\\n            root, extension = os.path.splitext(target)\\n            if file_number:\\n                filename = '%s_%04d%s' % (root, file_number, extension)\\n            else:\\n                filename = target\\n\\n            extension = extension.lower()\\n            if extension == '.png':\\n                surface = ctx.get_target()\\n                surface.write_to_png(target)\\n            elif extension == '.pdf':\\n                target_ctx = cairo.Context(cairo.PDFSurface(filename, *self.size_or_default()))\\n                target_ctx.set_source_surface(ctx.get_target())\\n                target_ctx.paint()\\n            elif extension in ('.ps', '.eps'):\\n                target_ctx = cairo.Context(cairo.PSSurface(filename, *self.size_or_default()))\\n                if extension == '.eps':\\n                    target_ctx.set_eps(extension='.eps')\\n                target_ctx.set_source_surface(ctx.get_target())\\n                target_ctx.paint()\\n            elif extension == '.svg':\\n                target_ctx = cairo.Context(cairo.SVGSurface(filename, *self.size_or_default()))\\n                target_ctx.set_source_surface(ctx.get_target())\\n                target_ctx.paint()\\n            return filename\\n\\n        if isinstance(target, cairo.Context):\\n            return output_context\\n        elif isinstance(target, cairo.Surface):\\n            return output_surface\\n        else:\\n            return output_file\", 'def _parse(self):\\n        \\n        \"\"\" Strips links from the definition and gathers them in a links property.\\n        \"\"\"\\n        \\n        p1 = \"\\\\[.*?\\\\](.*?)\\\\[\\\\/.*?\\\\]\"\\n        p2 = \"\\\\[(.*?)\\\\]\"\\n        self.links = []\\n        for p in (p1,p2):\\n            for link in re.findall(p, self.description):\\n                self.links.append(link)\\n            self.description = re.sub(p, \"\\\\\\\\1\", self.description)\\n            \\n        self.description = self.description.strip()', 'def create_canvas(src, format=None, outputfile=None, multifile=False, buff=None, window=False, title=None,\\n                  fullscreen=None, show_vars=False):\\n    \"\"\"\\n    Create canvas and sink for attachment to a bot\\n\\n    canvas is what draws images, \\'sink\\' is the final consumer of the images\\n\\n    :param src: Defaults for title or outputfile if not specified.\\n\\n    :param format: CairoImageSink image format, if using buff instead of outputfile\\n    :param buff: CairoImageSink buffer object to send output to\\n\\n    :param outputfile: CairoImageSink output filename e.g. \"hello.svg\"\\n    :param multifile: CairoImageSink if True,\\n\\n    :param title: ShoebotWindow - set window title\\n    :param fullscreen: ShoebotWindow - set window title\\n    :param show_vars: ShoebotWindow - display variable window\\n\\n    Two kinds of sink are provided: CairoImageSink and ShoebotWindow\\n\\n    ShoebotWindow\\n\\n    Displays a window to draw shoebot inside.\\n\\n\\n    CairoImageSink\\n\\n    Output to a filename (or files if multifile is set), or a buffer object.\\n    \"\"\"\\n    from core import CairoCanvas, CairoImageSink # https://github.com/shoebot/shoebot/issues/206\\n\\n    if outputfile:\\n        sink = CairoImageSink(outputfile, format, multifile, buff)\\n    elif window or show_vars:\\n        from gui import ShoebotWindow\\n        if not title:\\n            if src and os.path.isfile(src):\\n                title = os.path.splitext(os.path.basename(src))[0] + \\' - Shoebot\\'\\n            else:\\n                title = \\'Untitled - Shoebot\\'\\n        sink = ShoebotWindow(title, show_vars, fullscreen=fullscreen)\\n    else:\\n        if src and isinstance(src, cairo.Surface):\\n            outputfile = src\\n            format = \\'surface\\'\\n        elif src and os.path.isfile(src):\\n            outputfile = os.path.splitext(os.path.basename(src))[0] + \\'.\\' + (format or \\'svg\\')\\n        else:\\n            outputfile = \\'output.svg\\'\\n        sink = CairoImageSink(outputfile, format, multifile, buff)\\n    canvas = CairoCanvas(sink)\\n\\n    return canvas', 'def create_bot(src=None, grammar=NODEBOX, format=None, outputfile=None, iterations=1, buff=None, window=False,\\n               title=None, fullscreen=None, server=False, port=7777, show_vars=False, vars=None, namespace=None):\\n    \"\"\"\\n    Create a canvas and a bot with the same canvas attached to it\\n\\n    bot parameters\\n    :param grammar: DRAWBOT or NODEBOX - decides what kind of bot is created\\n    :param vars: preset dictionary of vars from the called\\n\\n    canvas parameters:\\n    ... everything else ...\\n\\n    See create_canvas for details on those parameters.\\n\\n    \"\"\"\\n    canvas = create_canvas(src, format, outputfile, iterations > 1, buff, window, title, fullscreen=fullscreen,\\n                           show_vars=show_vars)\\n\\n    if grammar == DRAWBOT:\\n        from shoebot.grammar import DrawBot\\n        bot = DrawBot(canvas, namespace=namespace, vars=vars)\\n    else:\\n        from shoebot.grammar import NodeBot\\n        bot = NodeBot(canvas, namespace=namespace, vars=vars)\\n\\n    if server:\\n        from shoebot.sbio import SocketServer\\n        socket_server = SocketServer(bot, \"\", port=port)\\n    return bot', 'def run(src,\\n        grammar=NODEBOX,\\n        format=None,\\n        outputfile=None,\\n        iterations=1,\\n        buff=None,\\n        window=True,\\n        title=None,\\n        fullscreen=None,\\n        close_window=False,\\n        server=False,\\n        port=7777,\\n        show_vars=False,\\n        vars=None,\\n        namespace=None,\\n        run_shell=False,\\n        args=[],\\n        verbose=False,\\n        background_thread=True):\\n    \"\"\"\\n    Create and run a bot, the arguments all correspond to sanitized\\n    commandline options.\\n\\n    :param background_thread: If True then use a background thread.\\n\\n\\n    Other args are split into create_args and run_args\\n\\n    See create_bot for details on create_args\\n\\n    run_args are passed to bot.run - see Nodebot.run or Drawbot.run\\n\\n\\n\\n    Background thread:\\n\\n    readline in python is blocking, running the app in a background\\n    thread opens up the main thread for IO on stdin/stdout, which\\n    can be used for communication with shoebot when livecoding is\\n    enabled.\\n\\n    See shoebot.io for implementation of the shell, and the gedit\\n    plugin for an example of using livecoding.\\n    \"\"\"\\n    # Munge shoebogt sys.argv\\n    sys.argv = [sys.argv[\\n                    0]] + args  # Remove shoebot parameters so sbot can be used in place of the python interpreter (e.g. for sphinx).\\n\\n    # arguments for create_bot\\n    create_args = [src,\\n                   grammar,\\n                   format,\\n                   outputfile,\\n                   iterations,\\n                   buff,\\n                   window,\\n                   title,\\n                   fullscreen,\\n                   server,\\n                   port,\\n                   show_vars]\\n    create_kwargs = dict(vars=vars, namespace=namespace)\\n    run_args = [src]\\n    run_kwargs = dict(\\n        iterations=iterations,\\n        frame_limiter=window,\\n        verbose=verbose,\\n        # run forever except 1. windowed mode is off 2. if --close-window was specified and\\n        # 3. if an output file was indicated\\n        run_forever=window and not (close_window or bool(outputfile)),\\n    )\\n\\n    # Run shoebot in a background thread so we can run a cmdline shell in the current thread\\n    if background_thread:\\n        sbot_thread = ShoebotThread(\\n            create_args=create_args,\\n            create_kwargs=create_kwargs,\\n            run_args=run_args,\\n            run_kwargs=run_kwargs,\\n            send_sigint=run_shell\\n        )\\n        sbot_thread.start()\\n        sbot = sbot_thread.sbot\\n    else:\\n        print(\\'background thread disabled\\')\\n        # This is a debug option, things should always work using the\\n        # background thread (crosses fingers)\\n        if run_shell:\\n            # python readline is blocking, so ui must run in a seperate\\n            # thread\\n            raise ValueError(\\'UI Must run in a separate thread to shell and shell needs main thread\\')\\n\\n        sbot_thread = None\\n        sbot = create_bot(*create_args, **create_kwargs)\\n        sbot.run(*run_args, **run_kwargs)\\n\\n    if run_shell:\\n        import shoebot.sbio.shell\\n        shell = shoebot.sbio.shell.ShoebotCmd(sbot, trusted=True)\\n        try:\\n            shell.cmdloop()\\n        except KeyboardInterrupt as e:\\n            publish_event(QUIT_EVENT)  # Handle Ctrl-C\\n            # KeyboardInterrupt is generated by os.kill from the other thread\\n            if verbose:\\n                raise\\n            else:\\n                return\\n    elif background_thread:\\n        try:\\n            while sbot_thread.is_alive():\\n                sleep(1)\\n        except KeyboardInterrupt:\\n            publish_event(QUIT_EVENT)\\n\\n    if all((background_thread, sbot_thread)):\\n        sbot_thread.join()\\n\\n    return sbot', 'def save_as(self):\\n        \"\"\"\\n        Return True if the buffer was saved\\n        \"\"\"\\n        chooser = ShoebotFileChooserDialog(_(\\'Save File\\'), None, Gtk.FileChooserAction.SAVE,\\n                                           (Gtk.STOCK_SAVE, Gtk.ResponseType.ACCEPT,\\n                                            Gtk.STOCK_CANCEL, Gtk.ResponseType.CANCEL))\\n        chooser.set_do_overwrite_confirmation(True)\\n        chooser.set_transient_for(self)\\n        saved = chooser.run() == Gtk.ResponseType.ACCEPT\\n        if saved:\\n            old_filename = self.filename\\n            self.source_buffer.filename = chooser.get_filename()\\n            if not self.save():\\n                self.filename = old_filename\\n        chooser.destroy()\\n        return saved', \"def widget_changed(self, widget, v):\\n        ''' Called when a slider is adjusted. '''\\n        # set the appropriate bot var\\n        if v.type is NUMBER:\\n            self.bot._namespace[v.name] = widget.get_value()\\n            self.bot._vars[v.name].value = widget.get_value()  ## Not sure if this is how to do this - stu\\n            publish_event(VARIABLE_UPDATED_EVENT, v)  # pretty dumb for now\\n        elif v.type is BOOLEAN:\\n            self.bot._namespace[v.name] = widget.get_active()\\n            self.bot._vars[v.name].value = widget.get_active()  ## Not sure if this is how to do this - stu\\n            publish_event(VARIABLE_UPDATED_EVENT, v)  # pretty dumb for now\\n        elif v.type is TEXT:\\n            self.bot._namespace[v.name] = widget.get_text()\\n            self.bot._vars[v.name].value = widget.get_text()  ## Not sure if this is how to do this - stu\\n            publish_event(VARIABLE_UPDATED_EVENT, v)\", 'def var_added(self, v):\\n        \"\"\"\\n        var was added in the bot while it ran, possibly\\n        by livecoding\\n\\n        :param v:\\n        :return:\\n        \"\"\"\\n        self.add_variable(v)\\n\\n        self.window.set_size_request(400, 35 * len(self.widgets.keys()))\\n        self.window.show_all()', 'def var_deleted(self, v):\\n        \"\"\"\\n        var was added in the bot\\n\\n        :param v:\\n        :return:\\n        \"\"\"\\n        widget = self.widgets[v.name]\\n\\n        # widgets are all in a single container ..\\n        parent = widget.get_parent()\\n        self.container.remove(parent)\\n        del self.widgets[v.name]\\n\\n        self.window.set_size_request(400, 35 * len(self.widgets.keys()))\\n        self.window.show_all()', 'def parse(svg, cached=False, _copy=True):\\n    \\n    \"\"\" Returns cached copies unless otherwise specified.\\n    \"\"\"\\n    \\n    if not cached:\\n        dom = parser.parseString(svg)\\n        paths = parse_node(dom, [])\\n    else:\\n        id = _cache.id(svg)\\n        if not _cache.has_key(id):\\n            dom = parser.parseString(svg)\\n            _cache.save(id, parse_node(dom, []))\\n        paths = _cache.load(id, _copy)\\n   \\n    return paths', 'def get_attribute(element, attribute, default=0):\\n    \\n    \"\"\" Returns XML element\\'s attribute, or default if none.\\n    \"\"\" \\n    \\n    a = element.getAttribute(attribute)\\n    if a == \"\": \\n        return default\\n    return a', 'def add_color_info(e, path):\\n    \\n    \"\"\" Expand the path with color information.\\n    \\n    Attempts to extract fill and stroke colors\\n    from the element and adds it to path attributes.\\n    \\n    \"\"\"\\n    \\n    _ctx.colormode(RGB, 1.0)\\n    \\n    def _color(hex, alpha=1.0):\\n        if hex == \"none\": return None\\n        n = int(hex[1:],16)\\n        r = (n>>16)&0xff\\n        g = (n>>8)&0xff\\n        b = n&0xff\\n        return _ctx.color(r/255.0, g/255.0, b/255.0, alpha)\\n\\n    path.fill = (0,0,0,0)\\n    path.stroke = (0,0,0,0)\\n    path.strokewidth = 0\\n\\n    # See if we can find an opacity attribute,\\n    # which is the color\\'s alpha.\\n    alpha = get_attribute(e, \"opacity\", default=\"\")\\n    if alpha == \"\":\\n        alpha = 1.0\\n    else:\\n        alpha = float(alpha)\\n    \\n    # Colors stored as fill=\"\" or stroke=\"\" attributes.\\n    try: path.fill = _color(get_attribute(e, \"fill\", default=\"#00000\"), alpha)\\n    except: \\n        pass\\n    try: path.stroke = _color(get_attribute(e, \"stroke\", default=\"none\"), alpha)\\n    except: \\n        pass\\n    try: path.strokewidth = float(get_attribute(e, \"stroke-width\", default=\"1\"))\\n    except: \\n        pass\\n    \\n    # Colors stored as a CSS style attribute, for example:\\n    # style=\"fill:#ff6600;stroke:#ffe600;stroke-width:0.06742057\"\\n    style = get_attribute(e, \"style\", default=\"\").split(\";\")\\n    for s in style:\\n        try:\\n            if s.startswith(\"fill:\"):\\n                path.fill = _color(s.replace(\"fill:\", \"\"))\\n            elif s.startswith(\"stroke:\"):\\n                path.stroke = _color(s.replace(\"stroke:\", \"\"))\\n            elif s.startswith(\"stroke-width:\"):\\n                path.strokewidth = float(s.replace(\"stroke-width:\", \"\"))\\n        except:\\n            pass    \\n\\n    # A path with beginning and ending coordinate\\n    # at the same location is considered closed.\\n    # Unless it contains a MOVETO somewhere in the middle.\\n    path.closed = False\\n    if path[0].x == path[len(path)-1].x and \\\\\\n       path[0].y == path[len(path)-1].y: \\n        path.closed = True\\n    for i in range(1,-1):\\n        if path[i].cmd == MOVETO:\\n            path.closed = False\\n        \\n    return path', 'def copy(self, graph):\\n    \\n        \"\"\" Returns a copy of the event handler, remembering the last node clicked.\\n        \"\"\"\\n    \\n        e = events(graph, self._ctx)\\n        e.clicked = self.clicked\\n        return e', 'def drag(self, node):\\n\\n        \"\"\" Drags given node to mouse location.\\n        \"\"\"\\n    \\n        dx = self.mouse.x - self.graph.x\\n        dy = self.mouse.y - self.graph.y\\n\\n        # A dashed line indicates the drag vector.\\n        s = self.graph.styles.default\\n        self._ctx.nofill()\\n        self._ctx.nostroke()\\n        if s.stroke: \\n            self._ctx.strokewidth(s.strokewidth)\\n            self._ctx.stroke(\\n                s.stroke.r, \\n                s.stroke.g, \\n                s.stroke.g, \\n                0.75\\n            )\\n        p = self._ctx.line(node.x, node.y, dx, dy, draw=False)\\n        try: p._nsBezierPath.setLineDash_count_phase_([2,4], 2, 50)\\n        except:\\n            pass\\n        self._ctx.drawpath(p)\\n        r = node.__class__(None).r * 0.75\\n        self._ctx.oval(dx-r/2, dy-r/2, r, r)\\n    \\n        node.vx = dx / self.graph.d\\n        node.vy = dy / self.graph.d', 'def hover(self, node):\\n        \\n        \"\"\" Displays a popup when hovering over a node.\\n        \"\"\"\\n        \\n        if self.popup == False: return\\n        if self.popup == True or self.popup.node != node:\\n            if self.popup_text.has_key(node.id):\\n                texts = self.popup_text[node.id]\\n            else:\\n                texts = None\\n            self.popup = popup(self._ctx, node, texts)\\n        self.popup.draw()', 'def textpath(self, i):\\n        \\n        \"\"\" Returns a cached textpath of the given text in queue.\\n        \"\"\"\\n        \\n        if len(self._textpaths) == i:\\n            self._ctx.font(self.font, self.fontsize)\\n            txt = self.q[i]\\n            if len(self.q) > 1:\\n                # Indicate current text (e.g. 5/13).\\n                txt += \" (\"+str(i+1)+\"/\" + str(len(self.q))+\")\"\\n            p = self._ctx.textpath(txt, 0, 0, width=self._w)\\n            h = self._ctx.textheight(txt, width=self._w)\\n            self._textpaths.append((p, h))\\n\\n        return self._textpaths[i]']}"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'datasets.formatting.formatting.LazyBatch'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Unnamed: 0': [11031, 11032, 11033, 11034, 11035, 11036, 11037, 11038, 11039, 11040, 11041, 11042, 11043, 11044, 11045, 11046, 11047, 11048, 11049, 11050, 11051, 11052, 11053, 11054, 11055, 11056, 11057, 11058, 11059, 11060, 11061, 11062, 11063, 11064, 11065, 11066, 11067, 11068, 11069, 11070], 'query_id': ['q262952', 'q262953', 'q262954', 'q262955', 'q262956', 'q262957', 'q262958', 'q262959', 'q262960', 'q262961', 'q262962', 'q262963', 'q262964', 'q262965', 'q262966', 'q262967', 'q262968', 'q262969', 'q262970', 'q262971', 'q262972', 'q262973', 'q262974', 'q262975', 'q262976', 'q262977', 'q262978', 'q262979', 'q262980', 'q262981', 'q262982', 'q262983', 'q262984', 'q262985', 'q262986', 'q262987', 'q262988', 'q262989', 'q262990', 'q262991'], 'corpus_id': ['c262851', 'c262852', 'c262853', 'c262854', 'c262855', 'c262856', 'c262857', 'c262858', 'c262859', 'c262860', 'c262861', 'c262862', 'c262863', 'c262864', 'c262865', 'c262866', 'c262867', 'c262868', 'c262869', 'c262870', 'c262871', 'c262872', 'c262873', 'c262874', 'c262875', 'c262876', 'c262877', 'c262878', 'c262879', 'c262880', 'c262881', 'c262882', 'c262883', 'c262884', 'c262885', 'c262886', 'c262887', 'c262888', 'c262889', 'c262890'], 'score': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'doc': ['Called when a file in the monitored directory has been moved.\\n\\n        Breaks move down into a delete and a create (which it is sometimes detected as!).\\n        :param event: the file system event', 'Tears down all temp files and directories.', 'Test whether a file target is not exists or it exists but allow\\n        overwrite.', 'Copy this file to other place.', 'Clients a Docker client.\\n\\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\\n    :return: the Docker client', 'Decorate methods when repository path is required.', 'clean repository given before and after states', 'Get repository descriptive stats\\n\\n        :Returns:\\n            #. numberOfDirectories (integer): Number of diretories in repository\\n            #. numberOfFiles (integer): Number of files in repository', 'Reset repository instance.', \"Load repository from a directory path and update the current instance.\\n        First, new repository still will be loaded. If failed, then old\\n        style repository load will be tried.\\n\\n        :Parameters:\\n            #. path (string): The path of the directory from where to load\\n               the repository from. If '.' or an empty string is passed,\\n               the current working directory will be used.\\n            #. verbose (boolean): Whether to be verbose about abnormalities\\n            #. ntrials (int): After aquiring all locks, ntrials is the maximum\\n               number of trials allowed before failing.\\n               In rare cases, when multiple processes\\n               are accessing the same repository components, different processes\\n               can alter repository components between successive lock releases\\n               of some other process. Bigger number of trials lowers the\\n               likelyhood of failure due to multiple processes same time\\n               alteration.\\n\\n        :Returns:\\n             #. repository (pyrep.Repository): returns self repository with loaded data.\", 'Remove all repository from path along with all repository tracked files.\\n\\n        :Parameters:\\n            #. path (None, string): The path the repository to remove.\\n            #. removeEmptyDirs (boolean): Whether to remove remaining empty\\n               directories.', 'Get whether creating a file or a directory from the basenane of the given\\n        path is allowed\\n\\n        :Parameters:\\n            #. path (str): The absolute or relative path or simply the file\\n               or directory name.\\n\\n        :Returns:\\n            #. allowed (bool): Whether name is allowed.\\n            #. message (None, str): Reason for the name to be forbidden.', 'Given a path, return relative path to diretory\\n\\n        :Parameters:\\n            #. path (str): Path as a string\\n            #. split (boolean): Whether to split path to its components\\n\\n        :Returns:\\n            #. relativePath (str, list): Relative path as a string or as a list\\n               of components if split is True', \"Get a list representation of repository state along with useful\\n        information. List state is ordered relativeley to directories level\\n\\n        :Parameters:\\n            #. relaPath (None, str): relative directory path from where to\\n               start. If None all repository representation is returned.\\n\\n        :Returns:\\n            #. state (list): List representation of the repository.\\n               List items are all dictionaries. Every dictionary has a single\\n               key which is the file or the directory name and the value is a\\n               dictionary of information including:\\n\\n                   * 'type': the type of the tracked whether it's file, dir, or objectdir\\n                   * 'exists': whether file or directory actually exists on disk\\n                   * 'pyrepfileinfo': In case of a file or an objectdir whether .%s_pyrepfileinfo exists\\n                   * 'pyrepdirinfo': In case of a directory whether .pyrepdirinfo exists\", 'Get file information dict from the repository given its relative path.\\n\\n        :Parameters:\\n            #. relativePath (string): The relative to the repository path of\\n               the file.\\n\\n        :Returns:\\n            #. info (None, dictionary): The file information dictionary.\\n               If None, it means an error has occurred.\\n            #. errorMessage (string): The error message if any error occurred.', 'Check whether a given relative path is a repository file path\\n\\n        :Parameters:\\n            #. relativePath (string): File relative path\\n\\n        :Returns:\\n            #. isRepoFile (boolean): Whether file is a repository file.\\n            #. isFileOnDisk (boolean): Whether file is found on disk.\\n            #. isFileInfoOnDisk (boolean): Whether file info is found on disk.\\n            #. isFileClassOnDisk (boolean): Whether file class is found on disk.', \"Create a tar file package of all the repository files and directories.\\n        Only files and directories that are tracked in the repository\\n        are stored in the package tar file.\\n\\n        **N.B. On some systems packaging requires root permissions.**\\n\\n        :Parameters:\\n            #. path (None, string): The real absolute path where to create the\\n               package. If None, it will be created in the same directory as\\n               the repository. If '.' or an empty string is passed, the current\\n               working directory will be used.\\n            #. name (None, string): The name to give to the package file\\n               If None, the package directory name will be used with the\\n               appropriate extension added.\\n            #. mode (None, string): The writing mode of the tarfile.\\n               If None, automatically the best compression mode will be chose.\\n               Available modes are ('w', 'w:', 'w:gz', 'w:bz2')\", 'Renames an item in this collection as a transaction.\\n\\n        Will override if new key name already exists.\\n        :param key: the current name of the item\\n        :param new_key: the new name that the item should have', \"Use default hash method to return hash value of a piece of string\\n    default setting use 'utf-8' encoding.\", 'Return md5 hash value of a piece of a file\\n\\n    Estimate processing time on:\\n\\n    :param abspath: the absolute path to the file\\n    :param nbytes: only has first N bytes of the file. if 0 or None,\\n      hash all file\\n\\n    CPU = i7-4600U 2.10GHz - 2.70GHz, RAM = 8.00 GB\\n    1 second can process 0.25GB data\\n\\n    - 0.59G - 2.43 sec\\n    - 1.3G - 5.68 sec\\n    - 1.9G - 7.72 sec\\n    - 2.5G - 10.32 sec\\n    - 3.9G - 16.0 sec', 'Return sha256 hash value of a piece of a file\\n\\n    Estimate processing time on:\\n\\n    :param abspath: the absolute path to the file\\n    :param nbytes: only has first N bytes of the file. if 0 or None,\\n      hash all file', 'Return sha512 hash value of a piece of a file\\n\\n    Estimate processing time on:\\n\\n    :param abspath: the absolute path to the file\\n    :param nbytes: only has first N bytes of the file. if 0 or None,\\n      hash all file', 'A command line auto complete similar behavior. Find all item with same\\n        prefix of this one.\\n\\n        :param case_sensitive: toggle if it is case sensitive.\\n        :return: list of :class:`pathlib_mate.pathlib2.Path`.', 'Print ``top_n`` big dir in this dir.', 'Print ``top_n`` big file in this dir.', 'Print ``top_n`` big dir and ``top_n`` big file in each dir.', \"Create a new folder having exactly same structure with this directory.\\n        However, all files are just empty file with same file name.\\n\\n        :param dst: destination directory. The directory can't exists before\\n        you execute this.\\n\\n        **中文文档**\\n\\n        创建一个目录的镜像拷贝, 与拷贝操作不同的是, 文件的副本只是在文件名上\\n        与原件一致, 但是是空文件, 完全没有内容, 文件大小为0。\", 'Execute every ``.py`` file as main script.\\n\\n        :param py_exe: str, python command or python executable path.\\n\\n        **中文文档**\\n\\n        将目录下的所有Python文件作为主脚本用当前解释器运行。', 'Trail white space at end of each line for every ``.py`` file.\\n\\n        **中文文档**\\n\\n        将目录下的所有被选择的文件中行末的空格删除。', 'Auto convert your python code in a directory to pep8 styled code.\\n\\n        :param kwargs: arguments for ``autopep8.fix_code`` method.\\n\\n        **中文文档**\\n\\n        将目录下的所有Python文件用pep8风格格式化。增加其可读性和规范性。', 'File size in bytes.', 'Get most recent modify time in timestamp.', 'Get most recent access time in timestamp.', 'Get most recent create time in timestamp.', 'Lists options that have not been used to format other values in \\n        their sections. \\n        \\n        Good for finding out if the user has misspelled any of the options.', 'List names of options and positional arguments.', 'Add an Option object to the user interface.', 'Append a positional argument to the user interface.\\n\\n        Optional positional arguments must be added after the required ones. \\n        The user interface can have at most one recurring positional argument, \\n        and if present, that argument must be the last one.', 'Read program documentation from a DocParser compatible file.\\n\\n        docsfiles is a list of paths to potential docsfiles: parse if present.\\n        A string is taken as a list of one item.', 'Return user friendly help on program options.'], 'code': ['def _on_file_moved(self, event: FileSystemMovedEvent):\\n        \"\"\"\\n        Called when a file in the monitored directory has been moved.\\n\\n        Breaks move down into a delete and a create (which it is sometimes detected as!).\\n        :param event: the file system event\\n        \"\"\"\\n        if not event.is_directory and self.is_data_file(event.src_path):\\n            delete_event = FileSystemEvent(event.src_path)\\n            delete_event.event_type = EVENT_TYPE_DELETED\\n            self._on_file_deleted(delete_event)\\n\\n            create_event = FileSystemEvent(event.dest_path)\\n            create_event.event_type = EVENT_TYPE_CREATED\\n            self._on_file_created(create_event)', 'def tear_down(self):\\n        \"\"\"\\n        Tears down all temp files and directories.\\n        \"\"\"\\n        while len(self._temp_directories) > 0:\\n            directory = self._temp_directories.pop()\\n            shutil.rmtree(directory, ignore_errors=True)\\n        while len(self._temp_files) > 0:\\n            file = self._temp_files.pop()\\n            try:\\n                os.remove(file)\\n            except OSError:\\n                pass', 'def is_not_exist_or_allow_overwrite(self, overwrite=False):\\n        \"\"\"\\n        Test whether a file target is not exists or it exists but allow\\n        overwrite.\\n        \"\"\"\\n        if self.exists() and overwrite is False:\\n            return False\\n        else:  # pragma: no cover\\n            return True', 'def copyto(self,\\n               new_abspath=None,\\n               new_dirpath=None,\\n               new_dirname=None,\\n               new_basename=None,\\n               new_fname=None,\\n               new_ext=None,\\n               overwrite=False,\\n               makedirs=False):\\n        \"\"\"\\n        Copy this file to other place.\\n        \"\"\"\\n        self.assert_exists()\\n\\n        p = self.change(\\n            new_abspath=new_abspath,\\n            new_dirpath=new_dirpath,\\n            new_dirname=new_dirname,\\n            new_basename=new_basename,\\n            new_fname=new_fname,\\n            new_ext=new_ext,\\n        )\\n\\n        if p.is_not_exist_or_allow_overwrite(overwrite=overwrite):\\n            # 如果两个路径不同, 才进行copy\\n            if self.abspath != p.abspath:\\n                try:\\n                    shutil.copy(self.abspath, p.abspath)\\n                except IOError as e:\\n                    if makedirs:\\n                        os.makedirs(p.parent.abspath)\\n                        shutil.copy(self.abspath, p.abspath)\\n                    else:\\n                        raise e\\n        return p', 'def create_client() -> APIClient:\\n    \"\"\"\\n    Clients a Docker client.\\n\\n    Will raise a `ConnectionError` if the Docker daemon is not accessible.\\n    :return: the Docker client\\n    \"\"\"\\n    global _client\\n    client = _client()\\n    if client is None:\\n        # First try looking at the environment variables for specification of the daemon\\'s location\\n        docker_environment = kwargs_from_env(assert_hostname=False)\\n        if \"base_url\" in docker_environment:\\n            client = _create_client(docker_environment.get(\"base_url\"), docker_environment.get(\"tls\"))\\n            if client is None:\\n                raise ConnectionError(\\n                    \"Could not connect to the Docker daemon specified by the `DOCKER_X` environment variables: %s\"\\n                    % docker_environment)\\n            else:\\n                logging.info(\"Connected to Docker daemon specified by the environment variables\")\\n        else:\\n            # Let\\'s see if the Docker daemon is accessible via the UNIX socket\\n            client = _create_client(\"unix://var/run/docker.sock\")\\n            if client is not None:\\n                logging.info(\"Connected to Docker daemon running on UNIX socket\")\\n            else:\\n                raise ConnectionError(\\n                    \"Cannot connect to Docker - is the Docker daemon running? `$DOCKER_HOST` should be set or the \"\\n                    \"daemon should be accessible via the standard UNIX socket.\")\\n        _client = weakref.ref(client)\\n    assert isinstance(client, APIClient)\\n    return client', 'def path_required(func):\\n    \"\"\"Decorate methods when repository path is required.\"\"\"\\n    @wraps(func)\\n    def wrapper(self, *args, **kwargs):\\n        if self.path is None:\\n            warnings.warn(\\'Must load (Repository.load_repository) or initialize (Repository.create_repository) the repository first !\\')\\n            return\\n        return func(self, *args, **kwargs)\\n    return wrapper', 'def __clean_before_after(self, stateBefore, stateAfter, keepNoneEmptyDirectory=True):\\n        \"\"\"clean repository given before and after states\"\"\"\\n        # prepare after for faster search\\n        errors    = []\\n        afterDict = {}\\n        [afterDict.setdefault(list(aitem)[0],[]).append(aitem) for aitem in stateAfter]\\n        # loop before\\n        for bitem in reversed(stateBefore):\\n            relaPath = list(bitem)[0]\\n            basename = os.path.basename(relaPath)\\n            btype    = bitem[relaPath][\\'type\\']\\n            alist    = afterDict.get(relaPath, [])\\n            aitem    = [a for a in alist if a[relaPath][\\'type\\']==btype]\\n            if len(aitem)>1:\\n                errors.append(\"Multiple \\'%s\\' of type \\'%s\\' where found in \\'%s\\', this should never had happened. Please report issue\"%(basename,btype,relaPath))\\n                continue\\n            if not len(aitem):\\n                removeDirs  = []\\n                removeFiles = []\\n                if btype == \\'dir\\':\\n                    if not len(relaPath):\\n                        errors.append(\"Removing main repository directory is not allowed\")\\n                        continue\\n                    removeDirs.append(os.path.join(self.__path,relaPath))\\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__dirInfo))\\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__dirLock))\\n                elif btype == \\'file\\':\\n                    removeFiles.append(os.path.join(self.__path,relaPath))\\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__fileInfo%basename))\\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__fileLock%basename))\\n                else:\\n                    ### MUST VERIFY THAT ONCE pyrepobjectdir IS IMPLEMENTED\\n                    removeDirs.append(os.path.join(self.__path,relaPath))\\n                    removeFiles.append(os.path.join(self.__path,relaPath,self.__fileInfo%basename))\\n                # remove files\\n                for fpath in removeFiles:\\n                    if os.path.isfile(fpath):\\n                        try:\\n                            os.remove(fpath)\\n                        except Exception as err:\\n                            errors.append(\"Unable to clean file \\'%s\\' (%s)\"%(fpath, str(err)))\\n                # remove directories\\n                for dpath in removeDirs:\\n                    if os.path.isdir(dpath):\\n                        if keepNoneEmptyDirectory or not len(os.listdir(dpath)):\\n                            try:\\n                                shutil.rmtree(dpath)\\n                            except Exception as err:\\n                                errors.append(\"Unable to clean directory \\'%s\\' (%s)\"%(fpath, str(err)))\\n        # return result and errors list\\n        return len(errors)==0, errors', 'def get_stats(self):\\n        \"\"\"\\n        Get repository descriptive stats\\n\\n        :Returns:\\n            #. numberOfDirectories (integer): Number of diretories in repository\\n            #. numberOfFiles (integer): Number of files in repository\\n        \"\"\"\\n        if self.__path is None:\\n            return 0,0\\n        nfiles = 0\\n        ndirs  = 0\\n        for fdict in self.get_repository_state():\\n            fdname = list(fdict)[0]\\n            if fdname == \\'\\':\\n                continue\\n            if fdict[fdname].get(\\'pyrepfileinfo\\', False):\\n                nfiles += 1\\n            elif fdict[fdname].get(\\'pyrepdirinfo\\', False):\\n                ndirs += 1\\n            else:\\n                raise Exception(\\'Not sure what to do next. Please report issue\\')\\n        return ndirs,nfiles', 'def reset(self):\\n        \"\"\"Reset repository instance.\\n        \"\"\"\\n        self.__path   = None\\n        self.__repo   = {\\'repository_unique_name\\': str(uuid.uuid1()),\\n                         \\'create_utctime\\': time.time(),\\n                         \\'last_update_utctime\\': None,\\n                         \\'pyrep_version\\': str(__version__),\\n                         \\'repository_information\\': \\'\\',\\n                         \\'walk_repo\\': []}', 'def load_repository(self, path, verbose=True, ntrials=3):\\n        \"\"\"\\n        Load repository from a directory path and update the current instance.\\n        First, new repository still will be loaded. If failed, then old\\n        style repository load will be tried.\\n\\n        :Parameters:\\n            #. path (string): The path of the directory from where to load\\n               the repository from. If \\'.\\' or an empty string is passed,\\n               the current working directory will be used.\\n            #. verbose (boolean): Whether to be verbose about abnormalities\\n            #. ntrials (int): After aquiring all locks, ntrials is the maximum\\n               number of trials allowed before failing.\\n               In rare cases, when multiple processes\\n               are accessing the same repository components, different processes\\n               can alter repository components between successive lock releases\\n               of some other process. Bigger number of trials lowers the\\n               likelyhood of failure due to multiple processes same time\\n               alteration.\\n\\n        :Returns:\\n             #. repository (pyrep.Repository): returns self repository with loaded data.\\n        \"\"\"\\n        assert isinstance(ntrials, int), \"ntrials must be integer\"\\n        assert ntrials>0, \"ntrials must be >0\"\\n        repo = None\\n        for _trial in range(ntrials):\\n            try:\\n                self.__load_repository(path=path, verbose=True)\\n            except Exception as err1:\\n                try:\\n                    from .OldRepository import Repository\\n                    REP = Repository(path)\\n                except Exception as err2:\\n                    #traceback.print_exc()\\n                    error = \"Unable to load repository using neiher new style (%s) nor old style (%s)\"%(err1, err2)\\n                    if self.DEBUG_PRINT_FAILED_TRIALS: print(\"Trial %i failed in Repository.%s (%s). Set Repository.DEBUG_PRINT_FAILED_TRIALS to False to mute\"%(_trial, inspect.stack()[1][3], str(error)))\\n                else:\\n                    error = None\\n                    repo  = REP\\n                    break\\n            else:\\n                error = None\\n                repo  = self\\n                break\\n        # check and return\\n        assert error is None, error\\n        return repo', 'def remove_repository(self, path=None, removeEmptyDirs=True):\\n        \"\"\"\\n        Remove all repository from path along with all repository tracked files.\\n\\n        :Parameters:\\n            #. path (None, string): The path the repository to remove.\\n            #. removeEmptyDirs (boolean): Whether to remove remaining empty\\n               directories.\\n        \"\"\"\\n        assert isinstance(removeEmptyDirs, bool), \"removeEmptyDirs must be boolean\"\\n        if path is not None:\\n            if path != self.__path:\\n                repo = Repository()\\n                repo.load_repository(path)\\n            else:\\n                repo = self\\n        else:\\n            repo = self\\n        assert repo.path is not None, \"path is not given and repository is not initialized\"\\n        # remove repo files and directories\\n        for fdict in reversed(repo.get_repository_state()):\\n            relaPath   = list(fdict)[0]\\n            realPath   = os.path.join(repo.path, relaPath)\\n            path, name = os.path.split(realPath)\\n            if fdict[relaPath][\\'type\\'] == \\'file\\':\\n                if os.path.isfile(realPath):\\n                    os.remove(realPath)\\n                if os.path.isfile(os.path.join(repo.path,path,self.__fileInfo%name)):\\n                    os.remove(os.path.join(repo.path,path,self.__fileInfo%name))\\n                if os.path.isfile(os.path.join(repo.path,path,self.__fileLock%name)):\\n                    os.remove(os.path.join(repo.path,path,self.__fileLock%name))\\n                if os.path.isfile(os.path.join(repo.path,path,self.__fileClass%name)):\\n                    os.remove(os.path.join(repo.path,path,self.__fileClass%name))\\n            elif fdict[relaPath][\\'type\\'] == \\'dir\\':\\n                if os.path.isfile(os.path.join(realPath,self.__dirInfo)):\\n                    os.remove(os.path.join(realPath,self.__dirInfo))\\n                if os.path.isfile(os.path.join(realPath,self.__dirLock)):\\n                    os.remove(os.path.join(realPath,self.__dirLock))\\n                if not len(os.listdir(realPath)) and removeEmptyDirs:\\n                    shutil.rmtree( realPath )\\n        # remove repo information file\\n        if os.path.isfile(os.path.join(repo.path,self.__repoFile)):\\n            os.remove(os.path.join(repo.path,self.__repoFile))\\n        if os.path.isfile(os.path.join(repo.path,self.__repoLock)):\\n            os.remove(os.path.join(repo.path,self.__repoLock))', 'def is_name_allowed(self, path):\\n        \"\"\"\\n        Get whether creating a file or a directory from the basenane of the given\\n        path is allowed\\n\\n        :Parameters:\\n            #. path (str): The absolute or relative path or simply the file\\n               or directory name.\\n\\n        :Returns:\\n            #. allowed (bool): Whether name is allowed.\\n            #. message (None, str): Reason for the name to be forbidden.\\n        \"\"\"\\n        assert isinstance(path, basestring), \"given path must be a string\"\\n        name = os.path.basename(path)\\n        if not len(name):\\n            return False, \"empty name is not allowed\"\\n        # exact match\\n        for em in [self.__repoLock,self.__repoFile,self.__dirInfo,self.__dirLock]:\\n            if name == em:\\n                return False, \"name \\'%s\\' is reserved for pyrep internal usage\"%em\\n        # pattern match\\n        for pm in [self.__fileInfo,self.__fileLock]:#,self.__objectDir]:\\n            if name == pm or (name.endswith(pm[3:]) and name.startswith(\\'.\\')):\\n                return False, \"name pattern \\'%s\\' is not allowed as result may be reserved for pyrep internal usage\"%pm\\n        # name is ok\\n        return True, None', 'def to_repo_relative_path(self, path, split=False):\\n        \"\"\"\\n        Given a path, return relative path to diretory\\n\\n        :Parameters:\\n            #. path (str): Path as a string\\n            #. split (boolean): Whether to split path to its components\\n\\n        :Returns:\\n            #. relativePath (str, list): Relative path as a string or as a list\\n               of components if split is True\\n        \"\"\"\\n        path = os.path.normpath(path)\\n        if path == \\'.\\':\\n            path = \\'\\'\\n        path = path.split(self.__path)[-1].strip(os.sep)\\n        if split:\\n            return path.split(os.sep)\\n        else:\\n            return path', 'def get_repository_state(self, relaPath=None):\\n        \"\"\"\\n        Get a list representation of repository state along with useful\\n        information. List state is ordered relativeley to directories level\\n\\n        :Parameters:\\n            #. relaPath (None, str): relative directory path from where to\\n               start. If None all repository representation is returned.\\n\\n        :Returns:\\n            #. state (list): List representation of the repository.\\n               List items are all dictionaries. Every dictionary has a single\\n               key which is the file or the directory name and the value is a\\n               dictionary of information including:\\n\\n                   * \\'type\\': the type of the tracked whether it\\'s file, dir, or objectdir\\n                   * \\'exists\\': whether file or directory actually exists on disk\\n                   * \\'pyrepfileinfo\\': In case of a file or an objectdir whether .%s_pyrepfileinfo exists\\n                   * \\'pyrepdirinfo\\': In case of a directory whether .pyrepdirinfo exists\\n        \"\"\"\\n        state = []\\n        def _walk_dir(relaPath, dirList):\\n            dirDict = {\\'type\\':\\'dir\\',\\n                       \\'exists\\':os.path.isdir(os.path.join(self.__path,relaPath)),\\n                       \\'pyrepdirinfo\\':os.path.isfile(os.path.join(self.__path,relaPath,self.__dirInfo)),\\n                      }\\n            state.append({relaPath:dirDict})\\n            # loop files and dirobjects\\n            for fname in sorted([f for f in dirList if isinstance(f, basestring)]):\\n                relaFilePath = os.path.join(relaPath,fname)\\n                realFilePath = os.path.join(self.__path,relaFilePath)\\n                #if os.path.isdir(realFilePath) and df.startswith(\\'.\\') and df.endswith(self.__objectDir[3:]):\\n                #    fileDict = {\\'type\\':\\'objectdir\\',\\n                #                \\'exists\\':True,\\n                #                \\'pyrepfileinfo\\':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)),\\n                #               }\\n                #else:\\n                #    fileDict = {\\'type\\':\\'file\\',\\n                #                \\'exists\\':os.path.isfile(realFilePath),\\n                #                \\'pyrepfileinfo\\':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)),\\n                #               }\\n                fileDict = {\\'type\\':\\'file\\',\\n                            \\'exists\\':os.path.isfile(realFilePath),\\n                            \\'pyrepfileinfo\\':os.path.isfile(os.path.join(self.__path,relaPath,self.__fileInfo%fname)),\\n                           }\\n                state.append({relaFilePath:fileDict})\\n            # loop directories\\n            #for ddict in sorted([d for d in dirList if isinstance(d, dict) and len(d)], key=lambda k: list(k)[0]):\\n            for ddict in sorted([d for d in dirList if isinstance(d, dict)], key=lambda k: list(k)[0]):\\n                dirname = list(ddict)[0]\\n                _walk_dir(relaPath=os.path.join(relaPath,dirname), dirList=ddict[dirname])\\n        # call recursive _walk_dir\\n        if relaPath is None:\\n            _walk_dir(relaPath=\\'\\', dirList=self.__repo[\\'walk_repo\\'])\\n        else:\\n            assert isinstance(relaPath, basestring), \"relaPath must be None or a str\"\\n            relaPath = self.to_repo_relative_path(path=relaPath, split=False)\\n            spath    = relaPath.split(os.sep)\\n            dirList  = self.__repo[\\'walk_repo\\']\\n            while len(spath):\\n                dirname = spath.pop(0)\\n                dList   = [d for d in dirList if isinstance(d, dict)]\\n                if not len(dList):\\n                    dirList = None\\n                    break\\n                cDict = [d for d in dList if dirname in d]\\n                if not len(cDict):\\n                    dirList = None\\n                    break\\n                dirList = cDict[0][dirname]\\n            if dirList is not None:\\n                _walk_dir(relaPath=relaPath, dirList=dirList)\\n        # return state list\\n        return state', 'def get_file_info(self, relativePath):\\n        \"\"\"\\n        Get file information dict from the repository given its relative path.\\n\\n        :Parameters:\\n            #. relativePath (string): The relative to the repository path of\\n               the file.\\n\\n        :Returns:\\n            #. info (None, dictionary): The file information dictionary.\\n               If None, it means an error has occurred.\\n            #. errorMessage (string): The error message if any error occurred.\\n        \"\"\"\\n        relativePath = self.to_repo_relative_path(path=relativePath, split=False)\\n        fileName     = os.path.basename(relativePath)\\n        isRepoFile,fileOnDisk, infoOnDisk, classOnDisk = self.is_repository_file(relativePath)\\n        if not isRepoFile:\\n            return None, \"file is not a registered repository file.\"\\n        if not infoOnDisk:\\n            return None, \"file is a registered repository file but info file missing\"\\n        fileInfoPath = os.path.join(self.__path,os.path.dirname(relativePath),self.__fileInfo%fileName)\\n        try:\\n            with open(fileInfoPath, \\'rb\\') as fd:\\n                info = pickle.load(fd)\\n        except Exception as err:\\n            return None, \"Unable to read file info from disk (%s)\"%str(err)\\n        return info, \\'\\'', 'def is_repository_file(self, relativePath):\\n        \"\"\"\\n        Check whether a given relative path is a repository file path\\n\\n        :Parameters:\\n            #. relativePath (string): File relative path\\n\\n        :Returns:\\n            #. isRepoFile (boolean): Whether file is a repository file.\\n            #. isFileOnDisk (boolean): Whether file is found on disk.\\n            #. isFileInfoOnDisk (boolean): Whether file info is found on disk.\\n            #. isFileClassOnDisk (boolean): Whether file class is found on disk.\\n        \"\"\"\\n        relativePath  = self.to_repo_relative_path(path=relativePath, split=False)\\n        if relativePath == \\'\\':\\n            return False, False, False, False\\n        relaDir, name = os.path.split(relativePath)\\n        fileOnDisk    = os.path.isfile(os.path.join(self.__path, relativePath))\\n        infoOnDisk    = os.path.isfile(os.path.join(self.__path,os.path.dirname(relativePath),self.__fileInfo%name))\\n        classOnDisk   = os.path.isfile(os.path.join(self.__path,os.path.dirname(relativePath),self.__fileClass%name))\\n        cDir          = self.__repo[\\'walk_repo\\']\\n        if len(relaDir):\\n            for dirname in relaDir.split(os.sep):\\n                dList = [d for d in cDir if isinstance(d, dict)]\\n                if not len(dList):\\n                    cDir = None\\n                    break\\n                cDict = [d for d in dList if dirname in d]\\n                if not len(cDict):\\n                    cDir = None\\n                    break\\n                cDir = cDict[0][dirname]\\n        if cDir is None:\\n            return False, fileOnDisk, infoOnDisk, classOnDisk\\n        #if name not in cDir:\\n        if str(name) not in [str(i) for i in cDir]:\\n            return False, fileOnDisk, infoOnDisk, classOnDisk\\n        # this is a repository registered file. check whether all is on disk\\n        return True, fileOnDisk, infoOnDisk, classOnDisk', 'def create_package(self, path=None, name=None, mode=None):\\n        \"\"\"\\n        Create a tar file package of all the repository files and directories.\\n        Only files and directories that are tracked in the repository\\n        are stored in the package tar file.\\n\\n        **N.B. On some systems packaging requires root permissions.**\\n\\n        :Parameters:\\n            #. path (None, string): The real absolute path where to create the\\n               package. If None, it will be created in the same directory as\\n               the repository. If \\'.\\' or an empty string is passed, the current\\n               working directory will be used.\\n            #. name (None, string): The name to give to the package file\\n               If None, the package directory name will be used with the\\n               appropriate extension added.\\n            #. mode (None, string): The writing mode of the tarfile.\\n               If None, automatically the best compression mode will be chose.\\n               Available modes are (\\'w\\', \\'w:\\', \\'w:gz\\', \\'w:bz2\\')\\n        \"\"\"\\n        # check mode\\n        assert mode in (None, \\'w\\', \\'w:\\', \\'w:gz\\', \\'w:bz2\\'), \\'unkown archive mode %s\\'%str(mode)\\n        if mode is None:\\n            #mode = \\'w:bz2\\'\\n            mode = \\'w:\\'\\n        # get root\\n        if path is None:\\n            root = os.path.split(self.__path)[0]\\n        elif path.strip() in (\\'\\',\\'.\\'):\\n            root = os.getcwd()\\n        else:\\n            root = os.path.realpath( os.path.expanduser(path) )\\n        assert os.path.isdir(root), \\'absolute path %s is not a valid directory\\'%path\\n        # get name\\n        if name is None:\\n            ext = mode.split(\":\")\\n            if len(ext) == 2:\\n                if len(ext[1]):\\n                    ext = \".\"+ext[1]\\n                else:\\n                    ext = \\'.tar\\'\\n            else:\\n                ext = \\'.tar\\'\\n            name = os.path.split(self.__path)[1]+ext\\n        # create tar file\\n        tarfilePath = os.path.join(root, name)\\n        try:\\n            tarHandler = tarfile.TarFile.open(tarfilePath, mode=mode)\\n        except Exception as e:\\n            raise Exception(\"Unable to create package (%s)\"%e)\\n        # walk directory and create empty directories\\n        for dpath in sorted(list(self.walk_directories_path(recursive=True))):\\n            t = tarfile.TarInfo( dpath )\\n            t.type = tarfile.DIRTYPE\\n            tarHandler.addfile(t)\\n            tarHandler.add(os.path.join(self.__path,dpath,self.__dirInfo), arcname=self.__dirInfo)\\n        # walk files and add to tar\\n        for fpath in self.walk_files_path(recursive=True):\\n            relaPath, fname = os.path.split(fpath)\\n            tarHandler.add(os.path.join(self.__path,fpath), arcname=fname)\\n            tarHandler.add(os.path.join(self.__path,relaPath,self.__fileInfo%fname), arcname=self.__fileInfo%fname)\\n            tarHandler.add(os.path.join(self.__path,relaPath,self.__fileClass%fname), arcname=self.__fileClass%fname)\\n        # save repository .pyrepinfo\\n        tarHandler.add(os.path.join(self.__path,self.__repoFile), arcname=\".pyrepinfo\")\\n        # close tar file\\n        tarHandler.close()', 'def rename(self, key: Any, new_key: Any):\\n        \"\"\"\\n        Renames an item in this collection as a transaction.\\n\\n        Will override if new key name already exists.\\n        :param key: the current name of the item\\n        :param new_key: the new name that the item should have\\n        \"\"\"\\n        if new_key == key:\\n            return\\n\\n        required_locks = [self._key_locks[key], self._key_locks[new_key]]\\n        ordered_required_locks = sorted(required_locks, key=lambda x: id(x))\\n        for lock in ordered_required_locks:\\n            lock.acquire()\\n\\n        try:\\n            if key not in self._data:\\n                raise KeyError(\"Attribute to rename \\\\\"%s\\\\\" does not exist\" % key)\\n            self._data[new_key] = self[key]\\n            del self._data[key]\\n        finally:\\n            for lock in required_locks:\\n                lock.release()', 'def get_text_fingerprint(text, hash_meth, encoding=\"utf-8\"):  # pragma: no cover\\n    \"\"\"\\n    Use default hash method to return hash value of a piece of string\\n    default setting use \\'utf-8\\' encoding.\\n    \"\"\"\\n    m = hash_meth()\\n    m.update(text.encode(encoding))\\n    return m.hexdigest()', 'def md5file(abspath, nbytes=0, chunk_size=DEFAULT_CHUNK_SIZE):\\n    \"\"\"\\n    Return md5 hash value of a piece of a file\\n\\n    Estimate processing time on:\\n\\n    :param abspath: the absolute path to the file\\n    :param nbytes: only has first N bytes of the file. if 0 or None,\\n      hash all file\\n\\n    CPU = i7-4600U 2.10GHz - 2.70GHz, RAM = 8.00 GB\\n    1 second can process 0.25GB data\\n\\n    - 0.59G - 2.43 sec\\n    - 1.3G - 5.68 sec\\n    - 1.9G - 7.72 sec\\n    - 2.5G - 10.32 sec\\n    - 3.9G - 16.0 sec\\n    \"\"\"\\n    return get_file_fingerprint(abspath, hashlib.md5, nbytes=nbytes, chunk_size=chunk_size)', 'def sha256file(abspath, nbytes=0, chunk_size=DEFAULT_CHUNK_SIZE):\\n    \"\"\"\\n    Return sha256 hash value of a piece of a file\\n\\n    Estimate processing time on:\\n\\n    :param abspath: the absolute path to the file\\n    :param nbytes: only has first N bytes of the file. if 0 or None,\\n      hash all file\\n    \"\"\"\\n    return get_file_fingerprint(abspath, hashlib.sha256, nbytes=nbytes, chunk_size=chunk_size)', 'def sha512file(abspath, nbytes=0, chunk_size=DEFAULT_CHUNK_SIZE):\\n    \"\"\"\\n    Return sha512 hash value of a piece of a file\\n\\n    Estimate processing time on:\\n\\n    :param abspath: the absolute path to the file\\n    :param nbytes: only has first N bytes of the file. if 0 or None,\\n      hash all file\\n    \"\"\"\\n    return get_file_fingerprint(abspath, hashlib.sha512, nbytes=nbytes, chunk_size=chunk_size)', 'def auto_complete_choices(self, case_sensitive=False):\\n        \"\"\"\\n        A command line auto complete similar behavior. Find all item with same\\n        prefix of this one.\\n\\n        :param case_sensitive: toggle if it is case sensitive.\\n        :return: list of :class:`pathlib_mate.pathlib2.Path`.\\n        \"\"\"\\n        self_basename = self.basename\\n        self_basename_lower = self.basename.lower()\\n        if case_sensitive:  # pragma: no cover\\n            def match(basename):\\n                return basename.startswith(self_basename)\\n        else:\\n            def match(basename):\\n                return basename.lower().startswith(self_basename_lower)\\n\\n        choices = list()\\n        if self.is_dir():\\n            choices.append(self)\\n            for p in self.sort_by_abspath(self.select(recursive=False)):\\n                choices.append(p)\\n        else:\\n            p_parent = self.parent\\n            if p_parent.is_dir():\\n                for p in self.sort_by_abspath(p_parent.select(recursive=False)):\\n                    if match(p.basename):\\n                        choices.append(p)\\n            else:  # pragma: no cover\\n                raise ValueError(\"\\'%s\\' directory does not exist!\" % p_parent)\\n        return choices', 'def print_big_dir(self, top_n=5):\\n        \"\"\"\\n        Print ``top_n`` big dir in this dir.\\n        \"\"\"\\n        self.assert_is_dir_and_exists()\\n\\n        size_table = sorted(\\n            [(p, p.dirsize) for p in self.select_dir(recursive=False)],\\n            key=lambda x: x[1],\\n            reverse=True,\\n        )\\n        for p, size in size_table[:top_n]:\\n            print(\"{:<9}    {:<9}\".format(repr_data_size(size), p.abspath))', 'def print_big_file(self, top_n=5):\\n        \"\"\"\\n        Print ``top_n`` big file in this dir.\\n        \"\"\"\\n        self.assert_is_dir_and_exists()\\n\\n        size_table = sorted(\\n            [(p, p.size) for p in self.select_file(recursive=True)],\\n            key=lambda x: x[1],\\n            reverse=True,\\n        )\\n        for p, size in size_table[:top_n]:\\n            print(\"{:<9}    {:<9}\".format(repr_data_size(size), p.abspath))', 'def print_big_dir_and_big_file(self, top_n=5):\\n        \"\"\"Print ``top_n`` big dir and ``top_n`` big file in each dir.\\n        \"\"\"\\n        self.assert_is_dir_and_exists()\\n\\n        size_table1 = sorted(\\n            [(p, p.dirsize) for p in self.select_dir(recursive=False)],\\n            key=lambda x: x[1],\\n            reverse=True,\\n        )\\n        for p1, size1 in size_table1[:top_n]:\\n            print(\"{:<9}    {:<9}\".format(repr_data_size(size1), p1.abspath))\\n            size_table2 = sorted(\\n                [(p, p.size) for p in p1.select_file(recursive=True)],\\n                key=lambda x: x[1],\\n                reverse=True,\\n            )\\n            for p2, size2 in size_table2[:top_n]:\\n                print(\"    {:<9}    {:<9}\".format(\\n                    repr_data_size(size2), p2.abspath))', 'def mirror_to(self, dst):  # pragma: no cover\\n        \"\"\"\\n        Create a new folder having exactly same structure with this directory.\\n        However, all files are just empty file with same file name.\\n\\n        :param dst: destination directory. The directory can\\'t exists before\\n        you execute this.\\n\\n        **中文文档**\\n\\n        创建一个目录的镜像拷贝, 与拷贝操作不同的是, 文件的副本只是在文件名上\\n        与原件一致, 但是是空文件, 完全没有内容, 文件大小为0。\\n        \"\"\"\\n        self.assert_is_dir_and_exists()\\n\\n        src = self.abspath\\n        dst = os.path.abspath(dst)\\n        if os.path.exists(dst):  # pragma: no cover\\n            raise Exception(\"distination already exist!\")\\n\\n        folder_to_create = list()\\n        file_to_create = list()\\n\\n        for current_folder, _, file_list in os.walk(self.abspath):\\n            current_folder = current_folder.replace(src, dst)\\n            try:\\n                os.mkdir(current_folder)\\n            except:  # pragma: no cover\\n                pass\\n            for basename in file_list:\\n                abspath = os.path.join(current_folder, basename)\\n                with open(abspath, \"wb\") as _:\\n                    pass', 'def execute_pyfile(self, py_exe=None):  # pragma: no cover\\n        \"\"\"\\n        Execute every ``.py`` file as main script.\\n\\n        :param py_exe: str, python command or python executable path.\\n\\n        **中文文档**\\n\\n        将目录下的所有Python文件作为主脚本用当前解释器运行。\\n        \"\"\"\\n        import subprocess\\n\\n        self.assert_is_dir_and_exists()\\n\\n        if py_exe is None:\\n            if six.PY2:\\n                py_exe = \"python2\"\\n            elif six.PY3:\\n                py_exe = \"python3\"\\n\\n        for p in self.select_by_ext(\".py\"):\\n            subprocess.Popen(\\'%s \"%s\"\\' % (py_exe, p.abspath))', 'def trail_space(self, filters=lambda p: p.ext == \".py\"):  # pragma: no cover\\n        \"\"\"\\n        Trail white space at end of each line for every ``.py`` file.\\n\\n        **中文文档**\\n\\n        将目录下的所有被选择的文件中行末的空格删除。\\n        \"\"\"\\n        self.assert_is_dir_and_exists()\\n\\n        for p in self.select_file(filters):\\n            try:\\n                with open(p.abspath, \"rb\") as f:\\n                    lines = list()\\n                    for line in f:\\n                        lines.append(line.decode(\"utf-8\").rstrip())\\n\\n                with open(p.abspath, \"wb\") as f:\\n                    f.write(\"\\\\n\".join(lines).encode(\"utf-8\"))\\n\\n            except Exception as e:  # pragma: no cover\\n                raise e', 'def autopep8(self, **kwargs):  # pragma: no cover\\n        \"\"\"\\n        Auto convert your python code in a directory to pep8 styled code.\\n\\n        :param kwargs: arguments for ``autopep8.fix_code`` method.\\n\\n        **中文文档**\\n\\n        将目录下的所有Python文件用pep8风格格式化。增加其可读性和规范性。\\n        \"\"\"\\n        self.assert_is_dir_and_exists()\\n\\n        for p in self.select_by_ext(\".py\"):\\n            with open(p.abspath, \"rb\") as f:\\n                code = f.read().decode(\"utf-8\")\\n\\n            formatted_code = autopep8.fix_code(code, **kwargs)\\n\\n            with open(p.abspath, \"wb\") as f:\\n                f.write(formatted_code.encode(\"utf-8\"))', 'def size(self):\\n        \"\"\"\\n        File size in bytes.\\n        \"\"\"\\n        try:\\n            return self._stat.st_size\\n        except:  # pragma: no cover\\n            self._stat = self.stat()\\n            return self.size', 'def mtime(self):\\n        \"\"\"\\n        Get most recent modify time in timestamp.\\n        \"\"\"\\n        try:\\n            return self._stat.st_mtime\\n        except:  # pragma: no cover\\n            self._stat = self.stat()\\n            return self.mtime', 'def atime(self):\\n        \"\"\"\\n        Get most recent access time in timestamp.\\n        \"\"\"\\n        try:\\n            return self._stat.st_atime\\n        except:  # pragma: no cover\\n            self._stat = self.stat()\\n            return self.atime', 'def ctime(self):\\n        \"\"\"\\n        Get most recent create time in timestamp.\\n        \"\"\"\\n        try:\\n            return self._stat.st_ctime\\n        except:  # pragma: no cover\\n            self._stat = self.stat()\\n            return self.ctime', 'def unusedoptions(self, sections):\\n        \"\"\"Lists options that have not been used to format other values in \\n        their sections. \\n        \\n        Good for finding out if the user has misspelled any of the options.\\n        \"\"\"\\n        unused = set([])\\n        for section in _list(sections):\\n            if not self.has_section(section):\\n                continue\\n            options = self.options(section)\\n            raw_values = [self.get(section, option, raw=True) for option in options]\\n            for option in options:\\n                formatter = \"%(\" + option + \")s\"\\n                for raw_value in raw_values:\\n                    if formatter in raw_value:\\n                        break\\n                else:\\n                    unused.add(option) \\n            return list(unused)', 'def keys(self):\\n        \"\"\"List names of options and positional arguments.\"\"\"\\n        return self.options.keys() + [p.name for p in self.positional_args]', 'def _add_option(self, option):\\n        \"\"\"Add an Option object to the user interface.\"\"\"\\n        if option.name in self.options:\\n            raise ValueError(\\'name already in use\\')\\n        if option.abbreviation in self.abbreviations:\\n            raise ValueError(\\'abbreviation already in use\\')\\n        if option.name in [arg.name for arg in self.positional_args]:\\n            raise ValueError(\\'name already in use by a positional argument\\')\\n        self.options[option.name] = option\\n        if option.abbreviation:\\n            self.abbreviations[option.abbreviation] = option\\n        self.option_order.append(option.name)', 'def _add_positional_argument(self, posarg):\\n        \"\"\"Append a positional argument to the user interface.\\n\\n        Optional positional arguments must be added after the required ones. \\n        The user interface can have at most one recurring positional argument, \\n        and if present, that argument must be the last one.\\n        \"\"\"\\n        if self.positional_args:\\n            if self.positional_args[-1].recurring:\\n                raise ValueError(\"recurring positional arguments must be last\")\\n            if self.positional_args[-1].optional and not posarg.optional:\\n                raise ValueError(\"required positional arguments must precede optional ones\")\\n        self.positional_args.append(posarg)', 'def read_docs(self, docsfiles):\\n        \"\"\"Read program documentation from a DocParser compatible file.\\n\\n        docsfiles is a list of paths to potential docsfiles: parse if present.\\n        A string is taken as a list of one item.\\n        \"\"\"\\n        updates = DocParser()\\n        for docsfile in _list(docsfiles):\\n            if os.path.isfile(docsfile):\\n                updates.parse(docsfile)\\n        self.docs.update((k, _docs(updates[k], self.docvars)) for k in self.docs if updates.blocks[k])\\n        for name, text in updates[\\'parameters\\'].items():\\n            if name in self:\\n                self.getparam(name).docs = text[0] % self.docvars\\n            elif name not in self.ignore:\\n                raise ValueError(\"parameter %r does not exist\" % name)', 'def optionhelp(self, indent=0, maxindent=25, width=79):\\n        \"\"\"Return user friendly help on program options.\"\"\"\\n        def makelabels(option):\\n            labels = \\'%*s--%s\\' % (indent, \\' \\', option.name)\\n            if option.abbreviation:\\n                labels += \\', -\\' + option.abbreviation\\n            return labels + \\': \\'\\n        docs = []\\n        helpindent = _autoindent([makelabels(o) for o in self.options.values()], indent, maxindent)\\n        for name in self.option_order:\\n            option = self.options[name]\\n            labels = makelabels(option)\\n            helpstring = \"%s(%s). %s\" % (option.formatname, option.strvalue, option.docs)\\n            wrapped = self._wrap_labelled(labels, helpstring, helpindent, width)\\n            docs.extend(wrapped)\\n        return \\'\\\\n\\'.join(docs)']}\n",
      "<class 'datasets.formatting.formatting.LazyBatch'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/200 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\n",
      "Map (num_proc=5):   0%|          | 0/13788 [00:00<?, ? examples/s]\n",
      "  0%|          | 0/1 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3519, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3469, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3392, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n  File \"/tmp/ipykernel_1667297/3535550556.py\", line 29, in process_batch\n    process_and_save(batch, generator, model_key, output_path, progress_dict)\n  File \"/tmp/ipykernel_1667297/1953078382.py\", line 45, in process_and_save\n    explanations = generator.generate_explanations_batch(batch)\n  File \"/tmp/ipykernel_1667297/1953078382.py\", line 37, in generate_explanations_batch\n    results = self.llm.generate(all_prompts, self.sampling_params)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py\", line 1057, in inner\n    return fn(*args, **kwargs)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/entrypoints/llm.py\", line 469, in generate\n    outputs = self._run_engine(use_tqdm=use_tqdm)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/entrypoints/llm.py\", line 1397, in _run_engine\n    step_outputs = self.llm_engine.step()\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/engine/llm_engine.py\", line 1391, in step\n    outputs = self.model_executor.execute_model(\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/executor/executor_base.py\", line 139, in execute_model\n    output = self.collective_rpc(\"execute_model\",\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n    answer = run_method(self.driver_worker, method, args, kwargs)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/utils.py\", line 2196, in run_method\n    return func(*args, **kwargs)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker_base.py\", line 394, in execute_model\n    inputs = self.prepare_input(execute_model_req)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker_base.py\", line 379, in prepare_input\n    return self._get_driver_input_and_broadcast(execute_model_req)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker_base.py\", line 338, in _get_driver_input_and_broadcast\n    worker_input: WorkerInput = self.prepare_worker_input(\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/vllm/worker/worker.py\", line 367, in prepare_worker_input\n    blocks_to_copy = torch.tensor(execute_model_req.blocks_to_copy,\n  File \"/home/anamikaghosh_umass_edu/.conda/envs/gpu-env/lib/python3.9/site-packages/torch/cuda/__init__.py\", line 305, in _lazy_init\n    raise RuntimeError(\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     process_and_save(batch, generator, model_key, output_path, progress_dict)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch  \u001b[38;5;66;03m# Return unmodified batch for `map`'s sake\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprocess_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generator\n\u001b[1;32m     41\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    560\u001b[0m }\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/arrow_dataset.py:3171\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3165\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpawning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m processes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3166\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3167\u001b[0m     unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3168\u001b[0m     total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3169\u001b[0m     desc\u001b[38;5;241m=\u001b[39m(desc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (num_proc=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_proc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3170\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3171\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m iflatmap_unordered(\n\u001b[1;32m   3172\u001b[0m         pool, Dataset\u001b[38;5;241m.\u001b[39m_map_single, kwargs_iterable\u001b[38;5;241m=\u001b[39mkwargs_per_job\n\u001b[1;32m   3173\u001b[0m     ):\n\u001b[1;32m   3174\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3175\u001b[0m             shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/utils/py_utils.py:718\u001b[0m, in \u001b[0;36miflatmap_unordered\u001b[0;34m(pool, func, kwargs_iterable)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m         [async_result\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/datasets/utils/py_utils.py:718\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pool_changed:\n\u001b[1;32m    717\u001b[0m         \u001b[38;5;66;03m# we get the result in case there's an error to raise\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m         [\u001b[43masync_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m async_result \u001b[38;5;129;01min\u001b[39;00m async_results]\n",
      "File \u001b[0;32m~/.conda/envs/gpu-env/lib/python3.9/site-packages/multiprocess/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    mp.set_start_method(\"spawn\", force=True)\n",
    "    start = time.time()\n",
    "    csv_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/data_preprocessing/CodeSearchNet_Python_valid.csv\"\n",
    "    output_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/explanations/CodeSearchNet_Python_valid_vllm_new.csv\"\n",
    "    \n",
    "    dataset = Dataset.from_pandas(pd.read_csv(csv_path))\n",
    "\n",
    "    models_dict = {\n",
    "        \"deepseek\": \"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "        # \"granite\": \"/datasets/ai/ibm-granite/hub/models--ibm-granite--granite-3.0-2b-instruct/snapshots/69e41fe735f54cec1792de2ac4f124b6cc84638f\"\n",
    "    }\n",
    "\n",
    "    batch_size = 40  # effective batch is 5 * 40 = 200\n",
    "\n",
    "    progress_dict = load_progress()\n",
    "\n",
    "    for model_key, model_path in tqdm(models_dict.items()):\n",
    "        print(f\"\\nProcessing model {model_key}\")\n",
    "        generator = ExplanationGeneratorLama(model_path)\n",
    "\n",
    "        # Determine where to resume\n",
    "        last_id = progress_dict.get(model_key)\n",
    "        if last_id:\n",
    "            idx = next((i for i, ex in enumerate(dataset) if ex['corpus_id'] == last_id), -1)\n",
    "            if idx != -1:\n",
    "                dataset = dataset.select(range(idx + 1, len(dataset)))\n",
    "\n",
    "        def process_batch(batch):\n",
    "            process_and_save(batch, generator, model_key, output_path, progress_dict)\n",
    "            return batch  # Return unmodified batch for `map`'s sake\n",
    "\n",
    "        dataset.map(\n",
    "            process_batch,\n",
    "            batched=True,\n",
    "            batch_size=batch_size,\n",
    "            num_proc=5,\n",
    "            load_from_cache_file=False\n",
    "        )\n",
    "\n",
    "        del generator\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"All model generations written to {output_path}\")\n",
    "    print(f'Overall time taken = {(end-start)} seconds')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
