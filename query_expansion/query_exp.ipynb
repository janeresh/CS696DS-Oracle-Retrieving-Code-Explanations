{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>query_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>corpus_id</th>\n",
       "      <th>code</th>\n",
       "      <th>explanation_granite_1</th>\n",
       "      <th>explanation_granite_2</th>\n",
       "      <th>explanation_granite_3</th>\n",
       "      <th>explanation_granite_4</th>\n",
       "      <th>explanation_deepseek_1_cleaned</th>\n",
       "      <th>explanation_deepseek_2_cleaned</th>\n",
       "      <th>explanation_deepseek_3_cleaned</th>\n",
       "      <th>explanation_deepseek_4_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>q1</td>\n",
       "      <td>python code to write bool value 1</td>\n",
       "      <td>d1</td>\n",
       "      <td>def writeBoolean(self, n):\\n        \"\"\"\\n     ...</td>\n",
       "      <td>\\nThe provided code is a Python method named `...</td>\n",
       "      <td>\\n1. `def writeBoolean(self, n):` - This line ...</td>\n",
       "      <td>\\nThis code snippet is a function that writes ...</td>\n",
       "      <td>\\nThe code snippet provided is a method named ...</td>\n",
       "      <td>The doc string and code both write the boolean...</td>\n",
       "      <td>Alright, I need to break down this Python code...</td>\n",
       "      <td>The code snippet writes a boolean value to a s...</td>\n",
       "      <td>The code snippet defines a method called write...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>q2</td>\n",
       "      <td>\"python how to manipulate clipboard\"</td>\n",
       "      <td>d2</td>\n",
       "      <td>def paste(xsel=False):\\n    \"\"\"Returns system ...</td>\n",
       "      <td>\\nThe provided Python code snippet defines a f...</td>\n",
       "      <td>\\n1. `def paste(xsel=False):` - This line defi...</td>\n",
       "      <td>\\nThis code snippet is a function in Python th...</td>\n",
       "      <td>\\nThe code snippet provided is a Python functi...</td>\n",
       "      <td>The paste function pastes data from the clipbo...</td>\n",
       "      <td>The code snippet begins by defining a function...</td>\n",
       "      <td>The code snippet is a function called paste th...</td>\n",
       "      <td>The code snippet defines a function called pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>q3</td>\n",
       "      <td>python colored output to html</td>\n",
       "      <td>d3</td>\n",
       "      <td>def _format_json(data, theme):\\n    \"\"\"Pretty ...</td>\n",
       "      <td>\\nThis Python function, `_format_json`, takes ...</td>\n",
       "      <td>\\n1. `def _format_json(data, theme):` - This l...</td>\n",
       "      <td>\\nThis code snippet, named `_format_json`, is ...</td>\n",
       "      <td>\\n```python\\nimport json\\nimport sys\\nimport p...</td>\n",
       "      <td>The docstring explains that the function _form...</td>\n",
       "      <td>```pythondef _format_json(data, theme):    \"\"\"...</td>\n",
       "      <td>This code snippet is a function called _format...</td>\n",
       "      <td>To explain the code snippet in a way that can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>q4</td>\n",
       "      <td>python \"create directory\" using \"relative path\"</td>\n",
       "      <td>d4</td>\n",
       "      <td>def create_path(path):\\n    \"\"\"Creates a absol...</td>\n",
       "      <td>\\nThe provided Python function, `create_path(p...</td>\n",
       "      <td>\\n1. The code snippet defines a function named...</td>\n",
       "      <td>\\nThis code snippet helps you create a new dir...</td>\n",
       "      <td>\\nThe provided code snippet is a Python functi...</td>\n",
       "      <td>The doc and code create an absolute path from ...</td>\n",
       "      <td>Okay, I need to explain this Python code snipp...</td>\n",
       "      <td>This code creates a file system where any path...</td>\n",
       "      <td>The code snippet defines a function called cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>q5</td>\n",
       "      <td>python column of an array</td>\n",
       "      <td>d5</td>\n",
       "      <td>def _vector_or_scalar(x, type='row'):\\n    \"\"\"...</td>\n",
       "      <td>\\nThe provided Python code defines a function ...</td>\n",
       "      <td>\\nThis code snippet defines a function called ...</td>\n",
       "      <td>\\nThis code snippet is a function that convert...</td>\n",
       "      <td>\\nThe code snippet provided is a function defi...</td>\n",
       "      <td>The code checks if an object `x` is an array o...</td>\n",
       "      <td>Alright, so I've got this Python code snippet ...</td>\n",
       "      <td>The code snippet is a Python function called _...</td>\n",
       "      <td>Okay, I'm going to try to figure out how to ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 query_id                                              doc  \\\n",
       "0           0       q1                python code to write bool value 1   \n",
       "1           1       q2             \"python how to manipulate clipboard\"   \n",
       "2           2       q3                    python colored output to html   \n",
       "3           3       q4  python \"create directory\" using \"relative path\"   \n",
       "4           4       q5                        python column of an array   \n",
       "\n",
       "  corpus_id                                               code  \\\n",
       "0        d1  def writeBoolean(self, n):\\n        \"\"\"\\n     ...   \n",
       "1        d2  def paste(xsel=False):\\n    \"\"\"Returns system ...   \n",
       "2        d3  def _format_json(data, theme):\\n    \"\"\"Pretty ...   \n",
       "3        d4  def create_path(path):\\n    \"\"\"Creates a absol...   \n",
       "4        d5  def _vector_or_scalar(x, type='row'):\\n    \"\"\"...   \n",
       "\n",
       "                               explanation_granite_1  \\\n",
       "0  \\nThe provided code is a Python method named `...   \n",
       "1  \\nThe provided Python code snippet defines a f...   \n",
       "2  \\nThis Python function, `_format_json`, takes ...   \n",
       "3  \\nThe provided Python function, `create_path(p...   \n",
       "4  \\nThe provided Python code defines a function ...   \n",
       "\n",
       "                               explanation_granite_2  \\\n",
       "0  \\n1. `def writeBoolean(self, n):` - This line ...   \n",
       "1  \\n1. `def paste(xsel=False):` - This line defi...   \n",
       "2  \\n1. `def _format_json(data, theme):` - This l...   \n",
       "3  \\n1. The code snippet defines a function named...   \n",
       "4  \\nThis code snippet defines a function called ...   \n",
       "\n",
       "                               explanation_granite_3  \\\n",
       "0  \\nThis code snippet is a function that writes ...   \n",
       "1  \\nThis code snippet is a function in Python th...   \n",
       "2  \\nThis code snippet, named `_format_json`, is ...   \n",
       "3  \\nThis code snippet helps you create a new dir...   \n",
       "4  \\nThis code snippet is a function that convert...   \n",
       "\n",
       "                               explanation_granite_4  \\\n",
       "0  \\nThe code snippet provided is a method named ...   \n",
       "1  \\nThe code snippet provided is a Python functi...   \n",
       "2  \\n```python\\nimport json\\nimport sys\\nimport p...   \n",
       "3  \\nThe provided code snippet is a Python functi...   \n",
       "4  \\nThe code snippet provided is a function defi...   \n",
       "\n",
       "                      explanation_deepseek_1_cleaned  \\\n",
       "0  The doc string and code both write the boolean...   \n",
       "1  The paste function pastes data from the clipbo...   \n",
       "2  The docstring explains that the function _form...   \n",
       "3  The doc and code create an absolute path from ...   \n",
       "4  The code checks if an object `x` is an array o...   \n",
       "\n",
       "                      explanation_deepseek_2_cleaned  \\\n",
       "0  Alright, I need to break down this Python code...   \n",
       "1  The code snippet begins by defining a function...   \n",
       "2  ```pythondef _format_json(data, theme):    \"\"\"...   \n",
       "3  Okay, I need to explain this Python code snipp...   \n",
       "4  Alright, so I've got this Python code snippet ...   \n",
       "\n",
       "                      explanation_deepseek_3_cleaned  \\\n",
       "0  The code snippet writes a boolean value to a s...   \n",
       "1  The code snippet is a function called paste th...   \n",
       "2  This code snippet is a function called _format...   \n",
       "3  This code creates a file system where any path...   \n",
       "4  The code snippet is a Python function called _...   \n",
       "\n",
       "                      explanation_deepseek_4_cleaned  \n",
       "0  The code snippet defines a method called write...  \n",
       "1  The code snippet defines a function called pas...  \n",
       "2  To explain the code snippet in a way that can ...  \n",
       "3  The code snippet defines a function called cre...  \n",
       "4  Okay, I'm going to try to figure out how to ex...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"/work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/data/CoSQA_explanations_query_code.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# === Set path to your manually extracted nltk_data ===\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')  # Change if needed\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# === Load your CSV ===\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# === NLP Functions ===\n",
    "def get_synonyms(word):\n",
    "    return {lemma.name().replace(\"_\", \" \") for syn in wordnet.synsets(word) for lemma in syn.lemmas()}\n",
    "\n",
    "def get_related_terms(word):\n",
    "    related = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for hyper in syn.hypernyms():\n",
    "            related |= {lemma.name().replace(\"_\", \" \") for lemma in hyper.lemmas()}\n",
    "        for hypo in syn.hyponyms():\n",
    "            related |= {lemma.name().replace(\"_\", \" \") for lemma in hypo.lemmas()}\n",
    "    return related\n",
    "\n",
    "def get_normal_forms(word):\n",
    "    return {stemmer.stem(word), lemmatizer.lemmatize(word)}\n",
    "\n",
    "def get_query_permutations(query):\n",
    "    words = query.split()\n",
    "    return {' '.join(p) for p in set(itertools.permutations(words)) if p != tuple(words)}\n",
    "\n",
    "def expand_query(query, max_syns=5, max_rel=5, use_permutations=False):\n",
    "    query = query.lower()\n",
    "    words = query.split()\n",
    "    expanded_queries = set([query])\n",
    "\n",
    "    for word in words:\n",
    "        # Limit synonyms and related terms for speed\n",
    "        synonyms = list(get_synonyms(word))[:max_syns]\n",
    "        related = list(get_related_terms(word))[:max_rel]\n",
    "        normal_forms = get_normal_forms(word)\n",
    "\n",
    "        for term in set(synonyms + related + list(normal_forms)):\n",
    "            if term != word:\n",
    "                new_query = query.replace(word, term)\n",
    "                expanded_queries.add(new_query)\n",
    "\n",
    "    if use_permutations and len(words) <= 5:\n",
    "        expanded_queries |= get_query_permutations(query)\n",
    "\n",
    "    return list(expanded_queries)\n",
    "\n",
    "\n",
    "def rank_queries(expanded_queries, original_query, top_k=10):\n",
    "    if len(expanded_queries) > 2000:\n",
    "        expanded_queries = expanded_queries[:2000]  # cap for safety\n",
    "\n",
    "    corpus = [original_query] + expanded_queries\n",
    "    tfidf = TfidfVectorizer().fit_transform(corpus)\n",
    "    orig_vec = tfidf[0]\n",
    "    similarity_scores = cosine_similarity(orig_vec, tfidf[1:])[0]\n",
    "    ranked = sorted(zip(expanded_queries, similarity_scores), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "\n",
    "# === Main ===\n",
    "if __name__ == \"__main__\":\n",
    "    query = df.iloc[0][\"doc\"]  # You can loop through all rows if needed\n",
    "    try:\n",
    "        top_k = int(input(\"How many top results do you want? (Top K): \").strip())\n",
    "    except ValueError:\n",
    "        top_k = 10\n",
    "\n",
    "    expanded = expand_query(query)\n",
    "    ranked_expansions = rank_queries(expanded, query, top_k=top_k)\n",
    "\n",
    "    print(f\"\\n🔍 Original Query: {query}\")\n",
    "    print(f\"\\n✨ Top {top_k} Expanded Queries (ranked by semantic similarity):\")\n",
    "    for i, (exp, score) in enumerate(ranked_expansions, 1):\n",
    "        print(f\"{i:2d}. {exp} (similarity: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted to: nltk_data/corpora/omw-1.4\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = os.path.expanduser('nltk_data/corpora/omw-1.4.zip')\n",
    "extract_to = os.path.expanduser('nltk_data/corpora/omw-1.4')\n",
    "\n",
    "# Create target folder if it doesn't exist\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# Extract manually\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"✅ Extracted to:\", extract_to)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /work/pi_wenlongzhao_umass_ed\n",
      "[nltk_data]     u/27/janet/query_expansion/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /work/pi_wenlongzhao_umas\n",
      "[nltk_data]     s_edu/27/janet/query_expansion/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /work/pi_wenlongzhao_umass_\n",
      "[nltk_data]     edu/27/janet/query_expansion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "\n",
    "# Set and create your custom nltk_data path\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# 🔁 Force download of punkt again to your path\n",
    "nltk.download('punkt', download_dir=nltk_data_path)\n",
    "nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "nltk.data.path.clear()  # Clear any old paths\n",
    "nltk.data.path.append(nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /work/pi_wenlongzhao_umass_\n",
      "[nltk_data]     edu/27/janet/query_expansion/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "nltk.data.path.clear()\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# Re-download WordNet\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'test', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.data.path = [\"nltk_data\"]  # ✅ Point to your custom path\n",
    "\n",
    "print(word_tokenize(\"This is a test sentence.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet.n.01:noun.event:\t\t\ta weak chirping sound as of a small bird\n",
      "tweet.v.01:verb.perception:\t\t\tmake a weak, chirping sound\n",
      "pinch.v.01:verb.contact:\t\t\tsqueeze tightly between the fingers\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "for syn in wn.synsets(\"tweet\"): \n",
    "    print (\"{0}:{1}:\\t\\t\\t{2}\".format(syn.name(), syn.lexname(), syn.definition()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Expanded queries saved to /work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/results/queries_expanded_nltk.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# === Set custom NLTK data path ===\n",
    "nltk_data_path = os.path.join(os.getcwd(), 'nltk_data')\n",
    "os.makedirs(nltk_data_path, exist_ok=True)\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# === Download necessary resources to that path ===\n",
    "# nltk.download('punkt', download_dir=nltk_data_path)\n",
    "# nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "# nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "\n",
    "# === Load stopwords ===\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# === Load input CSV ===\n",
    "input_path = \"/work/pi_wenlongzhao_umass_edu/27/anamikaghosh/CoSQA_explanations_query_code.csv\"  # Update this if needed\n",
    "df = pd.read_csv(input_path).iloc[:2]\n",
    "\n",
    "# === Function to get WordNet synonyms (limit 3 per word) ===\n",
    "def get_synonyms(word, limit=3):\n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            name = lemma.name().replace('_', ' ')\n",
    "            if name != word:\n",
    "                synonyms.add(name)\n",
    "            if len(synonyms) >= limit:\n",
    "                return list(synonyms)\n",
    "    return list(synonyms)\n",
    "\n",
    "# === Expand queries ===\n",
    "expanded_data = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    query_id = row['query_id']\n",
    "    query = row['doc'].lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    words = word_tokenize(query)\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    synonyms = []\n",
    "    for word in words:\n",
    "        synonyms += get_synonyms(word, limit=3)\n",
    "\n",
    "    expanded_data.append({\n",
    "        \"id\": query_id,\n",
    "        \"original_query\": row['doc'],\n",
    "        \"expanded_query\": ' '.join(synonyms)\n",
    "    })\n",
    "\n",
    "# === Save to output CSV ===\n",
    "output_path = \"/work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/results/queries_expanded_nltk.csv\"\n",
    "pd.DataFrame(expanded_data).to_csv(output_path, index=False)\n",
    "print(f\"✅ Expanded queries saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1606488/1111135649.py:10: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "/tmp/ipykernel_1606488/1111135649.py:65: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm([system_msg, user_msg])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! Results saved to /work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/results/code_queries_deepseek_expanded_10.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "\n",
    "# === LangChain + DeepSeek ===\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",  # Unity-hosted endpoint\n",
    "    openai_api_key=\"dummy-key\",  # required but not used by vLLM\n",
    "    temperature=0.5\n",
    ")\n",
    "def clean_lines(lines):\n",
    "    return [\n",
    "        line.strip().lstrip(\"0123456789).:- \").strip('\" ')\n",
    "        for line in lines if line.strip()\n",
    "    ]\n",
    "\n",
    "def expand_query_with_deepseek(query, llm, num_expansions=5):\n",
    "    # prompt = (\n",
    "    #     f\"Expand the following programming-related query into {num_expansions} semantically similar variants. \"\n",
    "    #     f\"Keep technical keywords like 'python', 'list', 'json' unchanged.\\n\\n\"\n",
    "    #     f\"Original query: {query}\\n\\n\"\n",
    "    #     f\"Expanded queries:\"\n",
    "    # )\n",
    "    # response = llm.invoke(prompt)\n",
    "    # lines = response.content.strip().split(\"\\n\")\n",
    "    # expansions = [line.strip(\"1234567890. \").strip() for line in lines if line.strip()]\n",
    "    # system_message = SystemMessage(content=(\n",
    "    # \"You are a query expansion engine for programming questions. \"\n",
    "    # \"Only return a clean list of semantically similar queries. \"\n",
    "    # \"Do not explain, do not chat. \"\n",
    "    # \"Preserve technical terms such as 'python', 'list', 'json', etc.\"\n",
    "    # ))\n",
    "\n",
    "    # user_prompt = (\n",
    "    #     f\"Expand the following programming-related query into 10 semantically similar variants. \"\n",
    "    #     f\"Keep technical keywords like 'python', 'list', 'json' unchanged.\\n\\n\"\n",
    "    #     f\"Original query: {query}\\n\\n\"\n",
    "    #     f\"Expanded queries:\"\n",
    "    # )\n",
    "\n",
    "    # response = llm([system_message, HumanMessage(content=user_prompt)])\n",
    "    # lines = response.content.splitlines()\n",
    "    # expansions = [line.strip(\"1234567890. \").strip() for line in lines if line.strip()]\n",
    "    # return expansions\n",
    "    system_msg = SystemMessage(content=(\n",
    "        \"You are a query expansion engine specialized in programming queries. \"\n",
    "        \"Return a list of clean, short, semantically similar variants of the input query. \"\n",
    "        \"Do not include any explanation, preambles, or list numbers. \"\n",
    "        \"Keep programming terms like 'python', 'list', 'json', etc. unchanged. \"\n",
    "        \"Each expansion must be a standalone query.\"\n",
    "    ))\n",
    "\n",
    "    user_msg = HumanMessage(content=(\n",
    "        f\"Expand this query into {num_expansions} semantically similar variants:\\n\"\n",
    "        f\"{query}\\n\\n\"\n",
    "        \"Only return the expanded queries, one per line, no numbering or bullet points.\"\n",
    "    ))\n",
    "\n",
    "    try:\n",
    "        response = llm([system_msg, user_msg])\n",
    "        expanded = clean_lines(response.content.splitlines())\n",
    "        return expanded\n",
    "    except Exception as e:\n",
    "        print(f\"Error expanding query '{query}': {e}\")\n",
    "        return []\n",
    "\n",
    "def rank_queries(expanded_queries, original_query, top_k=5):\n",
    "    corpus = [original_query] + expanded_queries\n",
    "    tfidf = TfidfVectorizer().fit_transform(corpus)\n",
    "    sims = cosine_similarity(tfidf[0:1], tfidf[1:])[0]\n",
    "    ranked = sorted(zip(expanded_queries, sims), key=lambda x: x[1], reverse=True)\n",
    "    return ranked[:top_k]\n",
    "\n",
    "# === File paths ===\n",
    "input_csv = \"/work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/data/CoSQA_explanations_query_code.csv\"\n",
    "output_csv = \"/work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/results/code_queries_deepseek_expanded_10.csv\"\n",
    "\n",
    "df = pd.read_csv(input_csv).iloc[:10]\n",
    "results = []\n",
    "\n",
    "# === Main Loop ===\n",
    "for _, row in df.iterrows():\n",
    "    qid = row['query_id']\n",
    "    query = row['doc']\n",
    "\n",
    "    try:\n",
    "        expanded = expand_query_with_deepseek(query, llm, num_expansions=10)\n",
    "        ranked = rank_queries(expanded, query, top_k=5)\n",
    "\n",
    "        for rank, (exp_q, score) in enumerate(ranked, 1):\n",
    "            results.append({\n",
    "                \"id\": qid,\n",
    "                \"original_query\": query,\n",
    "                \"rank\": rank,\n",
    "                \"expanded_query\": exp_q,\n",
    "                \"similarity\": round(score, 4)\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error expanding query ID {qid}: {e}\")\n",
    "\n",
    "# === Save to CSV ===\n",
    "pd.DataFrame(results).to_csv(output_csv, index=False)\n",
    "print(f\"Done! Results saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Original Query:\n",
      "python code to write bool value 1\n",
      "\n",
      "✨ Expanded Variants:\n",
      "1. First, I should understand what the original query is asking. It seems like someone wants a Python code snippet that creates a boolean value of 1. So, the goal is to generate code that evaluates to True when 1 is assigned to a boolean variable.\n",
      "2. I should consider different ways to structure the code. Maybe using if statements, elifs, else blocks, or even try-except for error handling. Also, using logical operators like and, or, not, or boolean functions like bool(), int(), etc.\n",
      "3. I should also think about variable names. It's better to use descriptive names like bool_value or bool_val. Maybe even include comments to explain what the code does.\n",
      "4. Another thought: perhaps the user wants to test the code or use it in a larger program. So, including examples or explanations of how to use the code would be helpful.\n",
      "5. I should also consider different programming paradigms, like object-oriented or functional approaches, but since the query is about a boolean value, maybe sticking to simple control structures is better.\n",
      "6. Using if statement: if bool_value: print(\"True\")\n",
      "7. Using elif: if bool_value: print(\"True\") else: print(\"False\")\n",
      "8. Using else block: if bool_value: print(\"True\") else: print(\"False\")\n",
      "9. Using try-except: try: if bool_value: print(\"True\") except: print(\"False\")\n",
      "10. Using logical operators: bool_value and True or False\n",
      "11. Using boolean function: bool(bool_value)\n",
      "12. Using int conversion: bool(int(bool_value))\n",
      "13. Using not operator: not bool_value\n",
      "14. Using logical and: bool_value and True\n",
      "15. Using logical or: bool_value or False\n",
      "16. Wait, some of these might be redundant or not necessary. For example, using bool(int(bool_value)) is similar to bool(bool_value). Maybe I can simplify some of them.\n",
      "17. Also, I should make sure each variant is clean and semantically similar but slightly different in structure. That way, the user can choose the one that best fits their needs.\n",
      "18. I should also ensure that each variant is on its own line and doesn't include any explanations or additional text. Just the code.\n",
      "19. For example, adding a comment after the code to explain what it does. That might make the variants more useful for the user.\n",
      "20. I should also consider using different variable names, like bool_val instead of bool_value, to make it more readable.\n",
      "21. </think>\n",
      "22. bool_value = 1\n",
      "23. if bool_value:\n",
      "24. print(\"True\")\n",
      "25. bool_value = 1\n",
      "26. elif bool_value:\n",
      "27. print(\"True\")\n",
      "28. else:\n",
      "29. print(\"False\")\n",
      "30. bool_value = 1\n",
      "31. else:\n",
      "32. print(\"False\")\n",
      "33. bool_value = 1\n",
      "34. bool_value and True or False\n",
      "35. bool_value = 1\n",
      "36. bool(int(bool_value))\n",
      "37. bool_value = 1\n",
      "38. not bool_value\n",
      "39. bool_value = 1\n",
      "40. bool(bool_value)\n",
      "41. bool_value = 1\n",
      "42. bool(int(bool_value))\n",
      "43. bool_value = 1\n",
      "44. bool_value and True\n",
      "45. bool_value = 1\n",
      "46. bool_value or False\n",
      "47. bool_value = 1\n",
      "48. print(\"True\" if bool_value else \"False\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "\n",
    "# === Connect to local DeepSeek model via OpenAI-compatible vLLM server ===\n",
    "llm = ChatOpenAI(\n",
    "    model_name=\"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\",\n",
    "    openai_api_base=\"http://localhost:8000/v1\",\n",
    "    openai_api_key=\"dummy-key\",  # required by LangChain\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# === Define a strict system instruction ===\n",
    "system_prompt = SystemMessage(content=(\n",
    "    \"You are a query expansion engine. \"\n",
    "    \"ONLY return clean, short, semantically similar variants of the given programming query. \"\n",
    "    \"NO explanation. NO chat. NO thoughts. \"\n",
    "    \"Return each expanded query on its own line. No bullets, no numbers, no quotes.\"\n",
    "))\n",
    "\n",
    "# === Define user query ===\n",
    "query = \"python code to write bool value 1\"\n",
    "user_prompt = HumanMessage(content=f\"Expand this query into 10 variants:\\n{query}\")\n",
    "\n",
    "# === Run the model ===\n",
    "response = llm([system_prompt, user_prompt])\n",
    "\n",
    "# === Process result ===\n",
    "expanded_queries = [line.strip().lstrip(\"0123456789).:- \").strip('\" ')\n",
    "                    for line in response.content.splitlines()\n",
    "                    if line.strip() and \"let me\" not in line.lower()]\n",
    "\n",
    "# === Print results ===\n",
    "print(\"\\n🔍 Original Query:\")\n",
    "print(query)\n",
    "print(\"\\n✨ Expanded Variants:\")\n",
    "for i, q in enumerate(expanded_queries, 1):\n",
    "    print(f\"{i}. {q}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# # === Config ===\n",
    "# MODEL_NAME=\"/datasets/ai/deepseek/hub/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-1.5B/snapshots/530ca3e1ad39d440e182c2e4317aa40f012512fa\"\n",
    "# API_BASE = \"http://localhost:8000/v1\"  # change port if needed\n",
    "# API_KEY = \"dummy-key\"\n",
    "# INPUT_CSV = \"/work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/data/CoSQA_explanations_query_code.csv\"\n",
    "# OUTPUT_CSV = \"/work/pi_wenlongzhao_umass_edu/27/janet/query_expansion/results/code_queries_deepseek_expanded_clean.csv\"\n",
    "# TOP_K = 5\n",
    "# NUM_EXPANSIONS = 10\n",
    "\n",
    "# # === Initialize LangChain LLM ===\n",
    "# llm = ChatOpenAI(\n",
    "#     model_name=MODEL_NAME,\n",
    "#     openai_api_base=API_BASE,\n",
    "#     openai_api_key=API_KEY,\n",
    "#     temperature=0.0\n",
    "# )\n",
    "\n",
    "# # === Clean lines (basic only) ===\n",
    "# def clean_lines_basic(lines):\n",
    "#     return [\n",
    "#         line.strip().lstrip(\"0123456789).:- \").strip('\" ')\n",
    "#         for line in lines if line.strip()\n",
    "#     ]\n",
    "\n",
    "# def expand_query_llm(query, num_expansions=10):\n",
    "#     system_msg = SystemMessage(content=(\n",
    "#         \"You are a query expansion engine specialized in programming queries. \"\n",
    "#         \"Return a list of semantically similar variants of the input query. \"\n",
    "#         \"Only return one query per line. No explanations or list formatting.\"\n",
    "#     ))\n",
    "\n",
    "#     user_msg = HumanMessage(content=(\n",
    "#         f\"Expand this query into {num_expansions} semantically similar variants:\\n\"\n",
    "#         f\"{query}\\n\\n\"\n",
    "#         \"Only return the expanded queries, one per line.\"\n",
    "#     ))\n",
    "\n",
    "#     try:\n",
    "#         response = llm([system_msg, user_msg])\n",
    "#         return clean_lines_basic(response.content.splitlines())\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error expanding query '{query}': {e}\")\n",
    "#         return []\n",
    "\n",
    "# def rank_queries(expanded_queries, original_query, top_k=5):\n",
    "#     corpus = [original_query] + expanded_queries\n",
    "#     tfidf = TfidfVectorizer().fit_transform(corpus)\n",
    "#     orig_vec = tfidf[0]\n",
    "#     sims = cosine_similarity(orig_vec, tfidf[1:])[0]\n",
    "#     ranked = sorted(zip(expanded_queries, sims), key=lambda x: x[1], reverse=True)\n",
    "#     return ranked[:top_k]\n",
    "\n",
    "# # === Run Query Expansion ===\n",
    "# df = pd.read_csv(INPUT_CSV).iloc[:1]\n",
    "# results = []\n",
    "\n",
    "# for _, row in df.iterrows():\n",
    "#     query_id = row['query_id']\n",
    "#     query = row['doc']\n",
    "#     expansions = expand_query_llm(query, num_expansions=NUM_EXPANSIONS)\n",
    "\n",
    "#     if not expansions:\n",
    "#         continue\n",
    "\n",
    "#     ranked = rank_queries(expansions, query, top_k=TOP_K)\n",
    "#     for rank, (q, score) in enumerate(ranked, 1):\n",
    "#         results.append({\n",
    "#             \"id\": query_id,\n",
    "#             \"original_query\": query,\n",
    "#             \"rank\": rank,\n",
    "#             \"expanded_query\": q,\n",
    "#             \"similarity\": round(score, 4)\n",
    "#         })\n",
    "\n",
    "# # Save to CSV\n",
    "# pd.DataFrame(results).to_csv(OUTPUT_CSV, index=False)\n",
    "# print(f\"✅ Saved top {TOP_K} expanded queries per input to: {OUTPUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
