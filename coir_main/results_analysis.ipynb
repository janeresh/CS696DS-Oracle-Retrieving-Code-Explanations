{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from coir.data_loader import get_tasks\n",
    "from coir.evaluation import COIR\n",
    "from coir.models import YourCustomDEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since CoIR-Retrieval/cosqa-queries-corpus couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/aabedeen_umass_edu/.cache/huggingface/datasets/CoIR-Retrieval___cosqa-queries-corpus/default/0.0.0/d56676dfbe7cd137229c33bd1e7dd96c688d2126 (last modified on Thu Mar 27 20:03:00 2025).\n",
      "Using the latest cached version of the dataset since CoIR-Retrieval/cosqa-qrels couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/aabedeen_umass_edu/.cache/huggingface/datasets/CoIR-Retrieval___cosqa-qrels/default/0.0.0/c70cfe89508993ed4707e31be1f83908f1fd6d38 (last modified on Thu Mar 27 20:03:01 2025).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5cd2393abc4ff0916b4fa8f7da3564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasks = get_tasks(tasks=[\"cosqa\"])\n",
    "corpus, queries, qrels = tasks['cosqa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"colbert_results.pkl\", \"rb\") as f:\n",
    "    expl_results = pickle.load(f)\n",
    "with open(\"baseline_results.pkl\", \"rb\") as f:\n",
    "    baseline_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q20105': {'d20105': 1}, 'q20106': {'d20106': 1}, 'q20107': {'d20107': 1}, 'q20108': {'d20108': 1}, 'q20109': {'d20109': 1}, 'q20110': {'d20110': 1}, 'q20111': {'d20111': 1}, 'q20112': {'d20112': 1}, 'q20113': {'d20113': 1}, 'q20114': {'d20114': 1}, 'q20115': {'d20115': 1}, 'q20116': {'d20116': 1}, 'q20117': {'d20117': 1}, 'q20118': {'d20118': 1}, 'q20119': {'d20119': 1}, 'q20120': {'d20120': 1}, 'q20121': {'d20121': 1}, 'q20122': {'d20122': 1}, 'q20123': {'d20123': 1}, 'q20124': {'d20124': 1}, 'q20125': {'d20125': 1}, 'q20126': {'d20126': 1}, 'q20127': {'d20127': 1}, 'q20128': {'d20128': 1}, 'q20129': {'d20129': 1}, 'q20130': {'d20130': 1}, 'q20131': {'d20131': 1}, 'q20132': {'d20132': 1}, 'q20133': {'d20133': 1}, 'q20134': {'d20134': 1}, 'q20135': {'d20135': 1}, 'q20136': {'d20136': 1}, 'q20137': {'d20137': 1}, 'q20138': {'d20138': 1}, 'q20139': {'d20139': 1}, 'q20140': {'d20140': 1}, 'q20141': {'d20141': 1}, 'q20142': {'d20142': 1}, 'q20143': {'d20143': 1}, 'q20144': {'d20144': 1}, 'q20145': {'d20145': 1}, 'q20146': {'d20146': 1}, 'q20147': {'d20147': 1}, 'q20148': {'d20148': 1}, 'q20149': {'d20149': 1}, 'q20150': {'d20150': 1}, 'q20151': {'d20151': 1}, 'q20152': {'d20152': 1}, 'q20153': {'d20153': 1}, 'q20154': {'d20154': 1}, 'q20155': {'d20155': 1}, 'q20156': {'d20156': 1}, 'q20157': {'d20157': 1}, 'q20158': {'d20158': 1}, 'q20159': {'d20159': 1}, 'q20160': {'d20160': 1}, 'q20161': {'d20161': 1}, 'q20162': {'d20162': 1}, 'q20163': {'d20163': 1}, 'q20164': {'d20164': 1}, 'q20165': {'d20165': 1}, 'q20166': {'d20166': 1}, 'q20167': {'d20167': 1}, 'q20168': {'d20168': 1}, 'q20169': {'d20169': 1}, 'q20170': {'d20170': 1}, 'q20171': {'d20171': 1}, 'q20172': {'d20172': 1}, 'q20173': {'d20173': 1}, 'q20174': {'d20174': 1}, 'q20175': {'d20175': 1}, 'q20176': {'d20176': 1}, 'q20177': {'d20177': 1}, 'q20178': {'d20178': 1}, 'q20179': {'d20179': 1}, 'q20180': {'d20180': 1}, 'q20181': {'d20181': 1}, 'q20182': {'d20182': 1}, 'q20183': {'d20183': 1}, 'q20184': {'d20184': 1}, 'q20185': {'d20185': 1}, 'q20186': {'d20186': 1}, 'q20187': {'d20187': 1}, 'q20188': {'d20188': 1}, 'q20189': {'d20189': 1}, 'q20190': {'d20190': 1}, 'q20191': {'d20191': 1}, 'q20192': {'d20192': 1}, 'q20193': {'d20193': 1}, 'q20194': {'d20194': 1}, 'q20195': {'d20195': 1}, 'q20196': {'d20196': 1}, 'q20197': {'d20197': 1}, 'q20198': {'d20198': 1}, 'q20199': {'d20199': 1}, 'q20200': {'d20200': 1}, 'q20201': {'d20201': 1}, 'q20202': {'d20202': 1}, 'q20203': {'d20203': 1}, 'q20204': {'d20204': 1}, 'q20205': {'d20205': 1}, 'q20206': {'d20206': 1}, 'q20207': {'d20207': 1}, 'q20208': {'d20208': 1}, 'q20209': {'d20209': 1}, 'q20210': {'d20210': 1}, 'q20211': {'d20211': 1}, 'q20212': {'d20212': 1}, 'q20213': {'d20213': 1}, 'q20214': {'d20214': 1}, 'q20215': {'d20215': 1}, 'q20216': {'d20216': 1}, 'q20217': {'d20217': 1}, 'q20218': {'d20218': 1}, 'q20219': {'d20219': 1}, 'q20220': {'d20220': 1}, 'q20221': {'d20221': 1}, 'q20222': {'d20222': 1}, 'q20223': {'d20223': 1}, 'q20224': {'d20224': 1}, 'q20225': {'d20225': 1}, 'q20226': {'d20226': 1}, 'q20227': {'d20227': 1}, 'q20228': {'d20228': 1}, 'q20229': {'d20229': 1}, 'q20230': {'d20230': 1}, 'q20231': {'d20231': 1}, 'q20232': {'d20232': 1}, 'q20233': {'d20233': 1}, 'q20234': {'d20234': 1}, 'q20235': {'d20235': 1}, 'q20236': {'d20236': 1}, 'q20237': {'d20237': 1}, 'q20238': {'d20238': 1}, 'q20239': {'d20239': 1}, 'q20240': {'d20240': 1}, 'q20241': {'d20241': 1}, 'q20242': {'d20242': 1}, 'q20243': {'d20243': 1}, 'q20244': {'d20244': 1}, 'q20245': {'d20245': 1}, 'q20246': {'d20246': 1}, 'q20247': {'d20247': 1}, 'q20248': {'d20248': 1}, 'q20249': {'d20249': 1}, 'q20250': {'d20250': 1}, 'q20251': {'d20251': 1}, 'q20252': {'d20252': 1}, 'q20253': {'d20253': 1}, 'q20254': {'d20254': 1}, 'q20255': {'d20255': 1}, 'q20256': {'d20256': 1}, 'q20257': {'d20257': 1}, 'q20258': {'d20258': 1}, 'q20259': {'d20259': 1}, 'q20260': {'d20260': 1}, 'q20261': {'d20261': 1}, 'q20262': {'d20262': 1}, 'q20263': {'d20263': 1}, 'q20264': {'d20264': 1}, 'q20265': {'d20265': 1}, 'q20266': {'d20266': 1}, 'q20267': {'d20267': 1}, 'q20268': {'d20268': 1}, 'q20269': {'d20269': 1}, 'q20270': {'d20270': 1}, 'q20271': {'d20271': 1}, 'q20272': {'d20272': 1}, 'q20273': {'d20273': 1}, 'q20274': {'d20274': 1}, 'q20275': {'d20275': 1}, 'q20276': {'d20276': 1}, 'q20277': {'d20277': 1}, 'q20278': {'d20278': 1}, 'q20279': {'d20279': 1}, 'q20280': {'d20280': 1}, 'q20281': {'d20281': 1}, 'q20282': {'d20282': 1}, 'q20283': {'d20283': 1}, 'q20284': {'d20284': 1}, 'q20285': {'d20285': 1}, 'q20286': {'d20286': 1}, 'q20287': {'d20287': 1}, 'q20288': {'d20288': 1}, 'q20289': {'d20289': 1}, 'q20290': {'d20290': 1}, 'q20291': {'d20291': 1}, 'q20292': {'d20292': 1}, 'q20293': {'d20293': 1}, 'q20294': {'d20294': 1}, 'q20295': {'d20295': 1}, 'q20296': {'d20296': 1}, 'q20297': {'d20297': 1}, 'q20298': {'d20298': 1}, 'q20299': {'d20299': 1}, 'q20300': {'d20300': 1}, 'q20301': {'d20301': 1}, 'q20302': {'d20302': 1}, 'q20303': {'d20303': 1}, 'q20304': {'d20304': 1}, 'q20305': {'d20305': 1}, 'q20306': {'d20306': 1}, 'q20307': {'d20307': 1}, 'q20308': {'d20308': 1}, 'q20309': {'d20309': 1}, 'q20310': {'d20310': 1}, 'q20311': {'d20311': 1}, 'q20312': {'d20312': 1}, 'q20313': {'d20313': 1}, 'q20314': {'d20314': 1}, 'q20315': {'d20315': 1}, 'q20316': {'d20316': 1}, 'q20317': {'d20317': 1}, 'q20318': {'d20318': 1}, 'q20319': {'d20319': 1}, 'q20320': {'d20320': 1}, 'q20321': {'d20321': 1}, 'q20322': {'d20322': 1}, 'q20323': {'d20323': 1}, 'q20324': {'d20324': 1}, 'q20325': {'d20325': 1}, 'q20326': {'d20326': 1}, 'q20327': {'d20327': 1}, 'q20328': {'d20328': 1}, 'q20329': {'d20329': 1}, 'q20330': {'d20330': 1}, 'q20331': {'d20331': 1}, 'q20332': {'d20332': 1}, 'q20333': {'d20333': 1}, 'q20334': {'d20334': 1}, 'q20335': {'d20335': 1}, 'q20336': {'d20336': 1}, 'q20337': {'d20337': 1}, 'q20338': {'d20338': 1}, 'q20339': {'d20339': 1}, 'q20340': {'d20340': 1}, 'q20341': {'d20341': 1}, 'q20342': {'d20342': 1}, 'q20343': {'d20343': 1}, 'q20344': {'d20344': 1}, 'q20345': {'d20345': 1}, 'q20346': {'d20346': 1}, 'q20347': {'d20347': 1}, 'q20348': {'d20348': 1}, 'q20349': {'d20349': 1}, 'q20350': {'d20350': 1}, 'q20351': {'d20351': 1}, 'q20352': {'d20352': 1}, 'q20353': {'d20353': 1}, 'q20354': {'d20354': 1}, 'q20355': {'d20355': 1}, 'q20356': {'d20356': 1}, 'q20357': {'d20357': 1}, 'q20358': {'d20358': 1}, 'q20359': {'d20359': 1}, 'q20360': {'d20360': 1}, 'q20361': {'d20361': 1}, 'q20362': {'d20362': 1}, 'q20363': {'d20363': 1}, 'q20364': {'d20364': 1}, 'q20365': {'d20365': 1}, 'q20366': {'d20366': 1}, 'q20367': {'d20367': 1}, 'q20368': {'d20368': 1}, 'q20369': {'d20369': 1}, 'q20370': {'d20370': 1}, 'q20371': {'d20371': 1}, 'q20372': {'d20372': 1}, 'q20373': {'d20373': 1}, 'q20374': {'d20374': 1}, 'q20375': {'d20375': 1}, 'q20376': {'d20376': 1}, 'q20377': {'d20377': 1}, 'q20378': {'d20378': 1}, 'q20379': {'d20379': 1}, 'q20380': {'d20380': 1}, 'q20381': {'d20381': 1}, 'q20382': {'d20382': 1}, 'q20383': {'d20383': 1}, 'q20384': {'d20384': 1}, 'q20385': {'d20385': 1}, 'q20386': {'d20386': 1}, 'q20387': {'d20387': 1}, 'q20388': {'d20388': 1}, 'q20389': {'d20389': 1}, 'q20390': {'d20390': 1}, 'q20391': {'d20391': 1}, 'q20392': {'d20392': 1}, 'q20393': {'d20393': 1}, 'q20394': {'d20394': 1}, 'q20395': {'d20395': 1}, 'q20396': {'d20396': 1}, 'q20397': {'d20397': 1}, 'q20398': {'d20398': 1}, 'q20399': {'d20399': 1}, 'q20400': {'d20400': 1}, 'q20401': {'d20401': 1}, 'q20402': {'d20402': 1}, 'q20403': {'d20403': 1}, 'q20404': {'d20404': 1}, 'q20405': {'d20405': 1}, 'q20406': {'d20406': 1}, 'q20407': {'d20407': 1}, 'q20408': {'d20408': 1}, 'q20409': {'d20409': 1}, 'q20410': {'d20410': 1}, 'q20411': {'d20411': 1}, 'q20412': {'d20412': 1}, 'q20413': {'d20413': 1}, 'q20414': {'d20414': 1}, 'q20415': {'d20415': 1}, 'q20416': {'d20416': 1}, 'q20417': {'d20417': 1}, 'q20418': {'d20418': 1}, 'q20419': {'d20419': 1}, 'q20420': {'d20420': 1}, 'q20421': {'d20421': 1}, 'q20422': {'d20422': 1}, 'q20423': {'d20423': 1}, 'q20424': {'d20424': 1}, 'q20425': {'d20425': 1}, 'q20426': {'d20426': 1}, 'q20427': {'d20427': 1}, 'q20428': {'d20428': 1}, 'q20429': {'d20429': 1}, 'q20430': {'d20430': 1}, 'q20431': {'d20431': 1}, 'q20432': {'d20432': 1}, 'q20433': {'d20433': 1}, 'q20434': {'d20434': 1}, 'q20435': {'d20435': 1}, 'q20436': {'d20436': 1}, 'q20437': {'d20437': 1}, 'q20438': {'d20438': 1}, 'q20439': {'d20439': 1}, 'q20440': {'d20440': 1}, 'q20441': {'d20441': 1}, 'q20442': {'d20442': 1}, 'q20443': {'d20443': 1}, 'q20444': {'d20444': 1}, 'q20445': {'d20445': 1}, 'q20446': {'d20446': 1}, 'q20447': {'d20447': 1}, 'q20448': {'d20448': 1}, 'q20449': {'d20449': 1}, 'q20450': {'d20450': 1}, 'q20451': {'d20451': 1}, 'q20452': {'d20452': 1}, 'q20453': {'d20453': 1}, 'q20454': {'d20454': 1}, 'q20455': {'d20455': 1}, 'q20456': {'d20456': 1}, 'q20457': {'d20457': 1}, 'q20458': {'d20458': 1}, 'q20459': {'d20459': 1}, 'q20460': {'d20460': 1}, 'q20461': {'d20461': 1}, 'q20462': {'d20462': 1}, 'q20463': {'d20463': 1}, 'q20464': {'d20464': 1}, 'q20465': {'d20465': 1}, 'q20466': {'d20466': 1}, 'q20467': {'d20467': 1}, 'q20468': {'d20468': 1}, 'q20469': {'d20469': 1}, 'q20470': {'d20470': 1}, 'q20471': {'d20471': 1}, 'q20472': {'d20472': 1}, 'q20473': {'d20473': 1}, 'q20474': {'d20474': 1}, 'q20475': {'d20475': 1}, 'q20476': {'d20476': 1}, 'q20477': {'d20477': 1}, 'q20478': {'d20478': 1}, 'q20479': {'d20479': 1}, 'q20480': {'d20480': 1}, 'q20481': {'d20481': 1}, 'q20482': {'d20482': 1}, 'q20483': {'d20483': 1}, 'q20484': {'d20484': 1}, 'q20485': {'d20485': 1}, 'q20486': {'d20486': 1}, 'q20487': {'d20487': 1}, 'q20488': {'d20488': 1}, 'q20489': {'d20489': 1}, 'q20490': {'d20490': 1}, 'q20491': {'d20491': 1}, 'q20492': {'d20492': 1}, 'q20493': {'d20493': 1}, 'q20494': {'d20494': 1}, 'q20495': {'d20495': 1}, 'q20496': {'d20496': 1}, 'q20497': {'d20497': 1}, 'q20498': {'d20498': 1}, 'q20499': {'d20499': 1}, 'q20500': {'d20500': 1}, 'q20501': {'d20501': 1}, 'q20502': {'d20502': 1}, 'q20503': {'d20503': 1}, 'q20504': {'d20504': 1}, 'q20505': {'d20505': 1}, 'q20506': {'d20506': 1}, 'q20507': {'d20507': 1}, 'q20508': {'d20508': 1}, 'q20509': {'d20509': 1}, 'q20510': {'d20510': 1}, 'q20511': {'d20511': 1}, 'q20512': {'d20512': 1}, 'q20513': {'d20513': 1}, 'q20514': {'d20514': 1}, 'q20515': {'d20515': 1}, 'q20516': {'d20516': 1}, 'q20517': {'d20517': 1}, 'q20518': {'d20518': 1}, 'q20519': {'d20519': 1}, 'q20520': {'d20520': 1}, 'q20521': {'d20521': 1}, 'q20522': {'d20522': 1}, 'q20523': {'d20523': 1}, 'q20524': {'d20524': 1}, 'q20525': {'d20525': 1}, 'q20526': {'d20526': 1}, 'q20527': {'d20527': 1}, 'q20528': {'d20528': 1}, 'q20529': {'d20529': 1}, 'q20530': {'d20530': 1}, 'q20531': {'d20531': 1}, 'q20532': {'d20532': 1}, 'q20533': {'d20533': 1}, 'q20534': {'d20534': 1}, 'q20535': {'d20535': 1}, 'q20536': {'d20536': 1}, 'q20537': {'d20537': 1}, 'q20538': {'d20538': 1}, 'q20539': {'d20539': 1}, 'q20540': {'d20540': 1}, 'q20541': {'d20541': 1}, 'q20542': {'d20542': 1}, 'q20543': {'d20543': 1}, 'q20544': {'d20544': 1}, 'q20545': {'d20545': 1}, 'q20546': {'d20546': 1}, 'q20547': {'d20547': 1}, 'q20548': {'d20548': 1}, 'q20549': {'d20549': 1}, 'q20550': {'d20550': 1}, 'q20551': {'d20551': 1}, 'q20552': {'d20552': 1}, 'q20553': {'d20553': 1}, 'q20554': {'d20554': 1}, 'q20555': {'d20555': 1}, 'q20556': {'d20556': 1}, 'q20557': {'d20557': 1}, 'q20558': {'d20558': 1}, 'q20559': {'d20559': 1}, 'q20560': {'d20560': 1}, 'q20561': {'d20561': 1}, 'q20562': {'d20562': 1}, 'q20563': {'d20563': 1}, 'q20564': {'d20564': 1}, 'q20565': {'d20565': 1}, 'q20566': {'d20566': 1}, 'q20567': {'d20567': 1}, 'q20568': {'d20568': 1}, 'q20569': {'d20569': 1}, 'q20570': {'d20570': 1}, 'q20571': {'d20571': 1}, 'q20572': {'d20572': 1}, 'q20573': {'d20573': 1}, 'q20574': {'d20574': 1}, 'q20575': {'d20575': 1}, 'q20576': {'d20576': 1}, 'q20577': {'d20577': 1}, 'q20578': {'d20578': 1}, 'q20579': {'d20579': 1}, 'q20580': {'d20580': 1}, 'q20581': {'d20581': 1}, 'q20582': {'d20582': 1}, 'q20583': {'d20583': 1}, 'q20584': {'d20584': 1}, 'q20585': {'d20585': 1}, 'q20586': {'d20586': 1}, 'q20587': {'d20587': 1}, 'q20588': {'d20588': 1}, 'q20589': {'d20589': 1}, 'q20590': {'d20590': 1}, 'q20591': {'d20591': 1}, 'q20592': {'d20592': 1}, 'q20593': {'d20593': 1}, 'q20594': {'d20594': 1}, 'q20595': {'d20595': 1}, 'q20596': {'d20596': 1}, 'q20597': {'d20597': 1}, 'q20598': {'d20598': 1}, 'q20599': {'d20599': 1}, 'q20600': {'d20600': 1}, 'q20601': {'d20601': 1}, 'q20602': {'d20602': 1}, 'q20603': {'d20603': 1}, 'q20604': {'d20604': 1}}\n"
     ]
    }
   ],
   "source": [
    "print(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Queries Where Explanations w/ Colbert Help Most:\n",
      "         baseline_mrr  baseline_ndcg  expl_mrr  expl_ndcg  delta_mrr  \\\n",
      "q20462      0.015152       0.164851  1.000000    1.00000   0.984848   \n",
      "q20269      0.090909       0.278943  1.000000    1.00000   0.909091   \n",
      "q20136      0.142857       0.333333  1.000000    1.00000   0.857143   \n",
      "q20160      0.200000       0.386853  1.000000    1.00000   0.800000   \n",
      "q20468      0.333333       0.500000  1.000000    1.00000   0.666667   \n",
      "q20549      0.111111       0.301030  0.500000    0.63093   0.388889   \n",
      "q20358      0.200000       0.386853  0.500000    0.63093   0.300000   \n",
      "q20499      0.052632       0.231378  0.333333    0.50000   0.280702   \n",
      "q20420      0.058824       0.239812  0.333333    0.50000   0.274510   \n",
      "q20293      0.083333       0.270238  0.333333    0.50000   0.250000   \n",
      "\n",
      "        delta_ndcg  \n",
      "q20462    0.835149  \n",
      "q20269    0.721057  \n",
      "q20136    0.666667  \n",
      "q20160    0.613147  \n",
      "q20468    0.500000  \n",
      "q20549    0.329900  \n",
      "q20358    0.244077  \n",
      "q20499    0.268622  \n",
      "q20420    0.260188  \n",
      "q20293    0.229762  \n",
      "\n",
      "Top 10 Queries Where Baseline Beats Explanations w/ Colbert:\n",
      "         baseline_mrr  baseline_ndcg  expl_mrr  expl_ndcg  delta_mrr  \\\n",
      "q20106           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20127           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20137           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20146           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20153           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20179           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20214           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20218           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20228           1.0            1.0       0.0        0.0       -1.0   \n",
      "q20232           1.0            1.0       0.0        0.0       -1.0   \n",
      "\n",
      "        delta_ndcg  \n",
      "q20106        -1.0  \n",
      "q20127        -1.0  \n",
      "q20137        -1.0  \n",
      "q20146        -1.0  \n",
      "q20153        -1.0  \n",
      "q20179        -1.0  \n",
      "q20214        -1.0  \n",
      "q20218        -1.0  \n",
      "q20228        -1.0  \n",
      "q20232        -1.0  \n"
     ]
    }
   ],
   "source": [
    "def mrr_at_k(ranked_docs, relevant, k=1000):\n",
    "    for i, doc in enumerate(ranked_docs[:k], start=1):\n",
    "        if doc in relevant:\n",
    "            return 1.0 / i\n",
    "    return 0.0\n",
    "\n",
    "def ndcg_at_k(ranked_docs, relevant, k=1000):\n",
    "    dcg = 0.0\n",
    "    for i, doc in enumerate(ranked_docs[:k], start=1):\n",
    "        if doc in relevant:\n",
    "            dcg += 1.0 / math.log2(i + 1)\n",
    "    ideal_hits = min(len(relevant), k)\n",
    "    idcg = sum(1.0 / math.log2(i + 1) for i in range(1, ideal_hits + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def compute_per_query_metrics(results_dict, qrels):\n",
    "    metrics = {}\n",
    "    for qid, doc_scores in results_dict.items():\n",
    "        ranked = [doc for doc, _ in sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)]\n",
    "        relevant = qrels.get(qid, set())\n",
    "        metrics[qid] = {\n",
    "            \"mrr\":  mrr_at_k(ranked, relevant),\n",
    "            \"ndcg\": ndcg_at_k(ranked, relevant)\n",
    "        }\n",
    "    return pd.DataFrame.from_dict(metrics, orient=\"index\")\n",
    "\n",
    "# Compute metrics\n",
    "df_base = compute_per_query_metrics(baseline_results, qrels).rename(columns=lambda c: f\"baseline_{c}\")\n",
    "df_expl = compute_per_query_metrics(expl_results,     qrels).rename(columns=lambda c: f\"expl_{c}\")\n",
    "\n",
    "# Merge and compute deltas\n",
    "metrics_df = df_base.join(df_expl)\n",
    "metrics_df[\"delta_mrr\"]  = metrics_df[\"expl_mrr\"] - metrics_df[\"baseline_mrr\"]\n",
    "metrics_df[\"delta_ndcg\"] = metrics_df[\"expl_ndcg\"] - metrics_df[\"baseline_ndcg\"]\n",
    "\n",
    "# Show top‑10 winners and losers by ΔMRR\n",
    "top_winners = metrics_df.nlargest(10, \"delta_mrr\")\n",
    "top_losers  = metrics_df.nsmallest(10, \"delta_mrr\")\n",
    "\n",
    "print(\"Top 10 Queries Where Explanations w/ Colbert Help Most:\\n\", top_winners)\n",
    "print(\"\\nTop 10 Queries Where Baseline Beats Explanations w/ Colbert:\\n\", top_losers)\n",
    "\n",
    "# (Optional) Save full breakdown for further analysis\n",
    "metrics_df.to_csv(\"per_query_comparison.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/pi_wenlongzhao_umass_edu/27/vaishnavisha/envs/virtual-env/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster PCA plot saved to: plots/clusters/clusters_pca.png\n",
      "Average delta MRR per cluster:\n",
      "   cluster  delta_mrr\n",
      "0        0  -0.155488\n",
      "1        1  -0.198535\n",
      "2        2  -0.135041\n",
      "3        3  -0.236893\n",
      "4        4  -0.119186\n",
      "Performance bar plot saved to: plots/performance/avg_delta_mrr_bar.png\n",
      "Word cloud for cluster 0 saved to: plots/wordcloud/wordcloud_cluster_0.png\n",
      "Word cloud for cluster 1 saved to: plots/wordcloud/wordcloud_cluster_1.png\n",
      "Word cloud for cluster 2 saved to: plots/wordcloud/wordcloud_cluster_2.png\n",
      "Word cloud for cluster 3 saved to: plots/wordcloud/wordcloud_cluster_3.png\n",
      "Word cloud for cluster 4 saved to: plots/wordcloud/wordcloud_cluster_4.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ---------------------------\n",
    "# Directory Setup for Saving Plots\n",
    "# ---------------------------\n",
    "base_dir = \"plots\"\n",
    "subdirs = {\n",
    "    \"clusters\": os.path.join(base_dir, \"clusters\"),\n",
    "    \"performance\": os.path.join(base_dir, \"performance\"),\n",
    "    \"wordcloud\": os.path.join(base_dir, \"wordcloud\")\n",
    "}\n",
    "for subdir in subdirs.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Sample Data Setup (Replace with your actual data)\n",
    "# ---------------------------\n",
    "# Assume 'metrics_df' is your DataFrame of per-query metrics.\n",
    "# Assume 'queries' is a dict mapping query_id to query text.\n",
    "df_metrics = metrics_df.copy()\n",
    "\n",
    "# If the query IDs are in the index, reset the index to create a 'query_id' column.\n",
    "if 'query_id' not in df_metrics.columns:\n",
    "    df_metrics = df_metrics.reset_index().rename(columns={'index': 'query_id'})\n",
    "\n",
    "# Merge query texts into the DataFrame.\n",
    "df_metrics[\"query_text\"] = df_metrics[\"query_id\"].apply(lambda q: queries.get(q, \"\"))\n",
    "\n",
    "# ---------------------------\n",
    "# 1. TF-IDF Feature Extraction\n",
    "# ---------------------------\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=500)\n",
    "tfidf_matrix = vectorizer.fit_transform(df_metrics[\"query_text\"])\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Clustering Queries with K-Means\n",
    "# ---------------------------\n",
    "num_clusters = 5  # Adjust as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "df_metrics[\"cluster\"] = kmeans.fit_predict(tfidf_matrix)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Visualizing Clusters using PCA\n",
    "# ---------------------------\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "reduced_features = pca.fit_transform(tfidf_matrix.toarray())\n",
    "df_metrics[\"pca1\"] = reduced_features[:, 0]\n",
    "df_metrics[\"pca2\"] = reduced_features[:, 1]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(df_metrics[\"pca1\"], df_metrics[\"pca2\"], \n",
    "                      c=df_metrics[\"cluster\"], cmap=\"viridis\", alpha=0.7)\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.title(\"Query Clusters (based on TF-IDF)\")\n",
    "plt.legend(*scatter.legend_elements(), title=\"Cluster\")\n",
    "plt.grid(True)\n",
    "cluster_plot_path = os.path.join(subdirs[\"clusters\"], \"clusters_pca.png\")\n",
    "plt.savefig(cluster_plot_path)\n",
    "plt.close()\n",
    "print(f\"Cluster PCA plot saved to: {cluster_plot_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Analyzing Cluster Performance\n",
    "# ---------------------------\n",
    "# Compute average delta MRR per cluster.\n",
    "cluster_perf = df_metrics.groupby(\"cluster\")[\"delta_mrr\"].mean().reset_index()\n",
    "print(\"Average delta MRR per cluster:\")\n",
    "print(cluster_perf)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(cluster_perf[\"cluster\"].astype(str), cluster_perf[\"delta_mrr\"])\n",
    "plt.xlabel(\"Cluster\")\n",
    "plt.ylabel(\"Average Delta MRR (expl - baseline)\")\n",
    "plt.title(\"Average Delta MRR by Query Cluster\")\n",
    "plt.grid(True)\n",
    "performance_bar_path = os.path.join(subdirs[\"performance\"], \"avg_delta_mrr_bar.png\")\n",
    "plt.savefig(performance_bar_path)\n",
    "plt.close()\n",
    "print(f\"Performance bar plot saved to: {performance_bar_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Generate Word Clouds for Each Cluster\n",
    "# ---------------------------\n",
    "for cluster in sorted(df_metrics[\"cluster\"].unique()):\n",
    "    cluster_text = \" \".join(df_metrics[df_metrics[\"cluster\"] == cluster][\"query_text\"])\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(cluster_text)\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for Cluster {cluster}\")\n",
    "    wordcloud_path = os.path.join(subdirs[\"wordcloud\"], f\"wordcloud_cluster_{cluster}.png\")\n",
    "    plt.savefig(wordcloud_path)\n",
    "    plt.close()\n",
    "    print(f\"Word cloud for cluster {cluster} saved to: {wordcloud_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 analysis data saved to: plots/topk_analysis/top100_analysis.csv\n",
      "Query Group Counts based on Top-K presence:\n",
      "group\n",
      "Both             263\n",
      "Baseline Only    179\n",
      "Neither           52\n",
      "Colbert Only       6\n",
      "Name: count, dtype: int64\n",
      "Query group bar chart saved to: plots/topk_analysis/query_group_bar_top100.png\n",
      "\n",
      "=== Sample queries from group 'Both' ===\n",
      "  query_id  baseline_rank  expl_rank                             query_text\n",
      "0   q20105            1.0        2.0       sort by a token in string python\n",
      "3   q20108            9.0       41.0  test for iterable is string in python\n",
      "4   q20109            1.0        6.0     python print results of query loop\n",
      "\n",
      "=== Sample queries from group 'Baseline Only' ===\n",
      "  query_id  baseline_rank  expl_rank                             query_text\n",
      "1   q20106            1.0        inf          python check file is readonly\n",
      "2   q20107            6.0        inf  declaring empty numpy array in python\n",
      "7   q20112           24.0        inf            python numpy array as float\n",
      "\n",
      "=== Sample queries from group 'Neither' ===\n",
      "   query_id  baseline_rank  expl_rank  \\\n",
      "11   q20116          154.0        inf   \n",
      "13   q20118          118.0        inf   \n",
      "17   q20122          428.0        inf   \n",
      "\n",
      "                                   query_text  \n",
      "11  how to skip an index in a for loop python  \n",
      "13           python raise without parentheses  \n",
      "17       python cast true or false as numbers  \n",
      "\n",
      "=== Sample queries from group 'Colbert Only' ===\n",
      "    query_id  baseline_rank  expl_rank                       query_text\n",
      "62    q20167          162.0       19.0    python ctypes array of arrays\n",
      "191   q20296          210.0       57.0       python dict keys lowercase\n",
      "269   q20374            inf       83.0  how to write a parser on python\n",
      "Top-100 analysis samples saved to: plots/topk_analysis/top100_analysis_samples.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameter: Top-K cutoff\n",
    "# ---------------------------\n",
    "top_k = 100  # You can adjust this (e.g., 5, 10, 20)\n",
    "\n",
    "# ---------------------------\n",
    "# Directory Setup for Saving Plots and CSVs\n",
    "# ---------------------------\n",
    "base_dir = \"plots\"\n",
    "subdirs = {\n",
    "    \"clusters\": os.path.join(base_dir, \"clusters\"),\n",
    "    \"performance\": os.path.join(base_dir, \"performance\"),\n",
    "    \"wordcloud\": os.path.join(base_dir, \"wordcloud\"),\n",
    "    \"topk_analysis\": os.path.join(base_dir, \"topk_analysis\")\n",
    "}\n",
    "for subdir in subdirs.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Function: Get Rank of Correct Document\n",
    "# ---------------------------\n",
    "def get_rank(query_id, results, qrels):\n",
    "    \"\"\"\n",
    "    Returns the rank (1-indexed) of the correct document for a given query.\n",
    "    Assumes that qrels is a nested dict, e.g., {query_id: {correct_doc: 1}}.\n",
    "    If the correct document is not found in the results, returns None.\n",
    "    \"\"\"\n",
    "    # Extract the correct document ID from the nested qrels structure.\n",
    "    correct_doc = list(qrels.get(query_id, {}).keys())[0] if query_id in qrels else None\n",
    "    if correct_doc is None:\n",
    "        return None\n",
    "    \n",
    "    # Sort documents by descending score\n",
    "    sorted_docs = sorted(results.get(query_id, {}).items(), key=lambda x: x[1], reverse=True)\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        if doc_id == correct_doc:\n",
    "            return rank\n",
    "    return None\n",
    "\n",
    "# ---------------------------\n",
    "# Create a DataFrame with Top-K Analysis per Query\n",
    "# ---------------------------\n",
    "# Assumptions:\n",
    "# - baseline_results: dict mapping query_id -> {doc_id: score, ...}\n",
    "# - expl_results: dict mapping query_id -> {doc_id: score, ...}\n",
    "# - qrels: dict mapping query_id -> {correct_doc: 1}\n",
    "# - queries: dict mapping query_id -> query text (for further analysis)\n",
    "\n",
    "data = []\n",
    "for qid in qrels:\n",
    "    baseline_rank = get_rank(qid, baseline_results, qrels)\n",
    "    expl_rank = get_rank(qid, expl_results, qrels)\n",
    "    \n",
    "    # Determine if correct doc is in top_k (if rank exists and <= top_k)\n",
    "    baseline_in_topk = baseline_rank is not None and baseline_rank <= top_k\n",
    "    expl_in_topk = expl_rank is not None and expl_rank <= top_k\n",
    "    \n",
    "    data.append({\n",
    "        \"query_id\": qid,\n",
    "        \"baseline_rank\": baseline_rank if baseline_rank is not None else float('inf'),\n",
    "        \"expl_rank\": expl_rank if expl_rank is not None else float('inf'),\n",
    "        \"baseline_in_topk\": baseline_in_topk,\n",
    "        \"expl_in_topk\": expl_in_topk\n",
    "    })\n",
    "\n",
    "df_topk = pd.DataFrame(data)\n",
    "\n",
    "# Save the raw data for further inspection\n",
    "csv_path = os.path.join(subdirs[\"topk_analysis\"], f\"top{top_k}_analysis.csv\")\n",
    "df_topk.to_csv(csv_path, index=False)\n",
    "print(f\"Top-{top_k} analysis data saved to: {csv_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Analyze Group Overlap\n",
    "# ---------------------------\n",
    "# Define groups based on whether the correct doc is found in top_k:\n",
    "# \"Both\": both methods retrieve the correct doc within top_k.\n",
    "# \"Baseline Only\": only baseline does.\n",
    "# \"Explanation Only\": only explanation does.\n",
    "# \"Neither\": neither method does.\n",
    "def label_group(row):\n",
    "    if row[\"baseline_in_topk\"] and row[\"expl_in_topk\"]:\n",
    "        return \"Both\"\n",
    "    elif row[\"baseline_in_topk\"] and not row[\"expl_in_topk\"]:\n",
    "        return \"Baseline Only\"\n",
    "    elif not row[\"baseline_in_topk\"] and row[\"expl_in_topk\"]:\n",
    "        return \"Colbert Only\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "df_topk[\"group\"] = df_topk.apply(label_group, axis=1)\n",
    "group_counts = df_topk[\"group\"].value_counts()\n",
    "print(\"Query Group Counts based on Top-K presence:\")\n",
    "print(group_counts)\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization: Bar Chart for Query Group Counts\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "group_counts.plot(kind=\"bar\", color=\"skyblue\")\n",
    "plt.xlabel(\"Query Group\")\n",
    "plt.ylabel(\"Number of Queries\")\n",
    "plt.title(f\"Queries Grouped by Presence of Correct Doc in Top-{top_k}\")\n",
    "plt.grid(True, axis='y')\n",
    "bar_chart_path = os.path.join(subdirs[\"topk_analysis\"], f\"query_group_bar_top{top_k}.png\")\n",
    "plt.savefig(bar_chart_path)\n",
    "plt.close()\n",
    "print(f\"Query group bar chart saved to: {bar_chart_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Optional: Merge Query Text for Further Analysis\n",
    "# ---------------------------\n",
    "# Merge query text if available (assuming 'queries' is a dict mapping query_id to text)\n",
    "df_topk[\"query_text\"] = df_topk[\"query_id\"].apply(lambda q: queries.get(q, \"\"))\n",
    "\n",
    "# Print out a few examples from each group for manual inspection\n",
    "for group in df_topk[\"group\"].unique():\n",
    "    print(f\"\\n=== Sample queries from group '{group}' ===\")\n",
    "    print(df_topk[df_topk[\"group\"] == group][[\"query_id\", \"baseline_rank\", \"expl_rank\", \"query_text\"]].head(3))\n",
    "\n",
    "# Optionally save the samples to a CSV file for further manual analysis.\n",
    "sample_csv_path = os.path.join(subdirs[\"topk_analysis\"], f\"top{top_k}_analysis_samples.csv\")\n",
    "df_topk.to_csv(sample_csv_path, index=False)\n",
    "print(f\"Top-{top_k} analysis samples saved to: {sample_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 analysis data saved to: plots/topk_analysis/top100_analysis.csv\n",
      "Query Group Counts based on Top-K presence:\n",
      "group\n",
      "Both             263\n",
      "Baseline Only    179\n",
      "Neither           52\n",
      "Colbert Only       6\n",
      "Name: count, dtype: int64\n",
      "Query group bar chart saved to: plots/topk_analysis/query_group_bar_top100.png\n",
      "Word cloud saved to: plots/topk_analysis/group_analysis/wordcloud_Both.png\n",
      "\n",
      "Group: Both\n",
      "Top words:\n",
      "python: 261\n",
      "file: 26\n",
      "list: 23\n",
      "string: 19\n",
      "check: 12\n",
      "read: 11\n",
      "remove: 10\n",
      "line: 10\n",
      "object: 9\n",
      "array: 9\n",
      "Word cloud saved to: plots/topk_analysis/group_analysis/wordcloud_Baseline_Only.png\n",
      "\n",
      "Group: Baseline Only\n",
      "Top words:\n",
      "python: 178\n",
      "list: 19\n",
      "check: 18\n",
      "string: 18\n",
      "file: 16\n",
      "object: 14\n",
      "numpy: 10\n",
      "array: 10\n",
      "value: 10\n",
      "function: 9\n",
      "Word cloud saved to: plots/topk_analysis/group_analysis/wordcloud_Neither.png\n",
      "\n",
      "Group: Neither\n",
      "Top words:\n",
      "python: 51\n",
      "list: 7\n",
      "index: 5\n",
      "string: 5\n",
      "array: 4\n",
      "type: 4\n",
      "object: 4\n",
      "element: 3\n",
      "remove: 3\n",
      "cast: 2\n",
      "Word cloud saved to: plots/topk_analysis/group_analysis/wordcloud_Colbert_Only.png\n",
      "\n",
      "Group: Colbert Only\n",
      "Top words:\n",
      "python: 6\n",
      "ctypes: 1\n",
      "array: 1\n",
      "arrays: 1\n",
      "dict: 1\n",
      "keys: 1\n",
      "lowercase: 1\n",
      "write: 1\n",
      "parser: 1\n",
      "create: 1\n",
      "\n",
      "Group top words summary saved to: plots/topk_analysis/group_analysis/group_top_words_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameter: Top-K cutoff\n",
    "# ---------------------------\n",
    "top_k = 100  # You can adjust this (e.g., 5, 10, 20)\n",
    "\n",
    "# ---------------------------\n",
    "# Directory Setup for Saving Plots and CSVs\n",
    "# ---------------------------\n",
    "base_dir = \"plots\"\n",
    "subdirs = {\n",
    "    \"clusters\": os.path.join(base_dir, \"clusters\"),\n",
    "    \"performance\": os.path.join(base_dir, \"performance\"),\n",
    "    \"wordcloud\": os.path.join(base_dir, \"wordcloud\"),\n",
    "    \"topk_analysis\": os.path.join(base_dir, \"topk_analysis\")\n",
    "}\n",
    "for subdir in subdirs.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Function: Get Rank of Correct Document\n",
    "# ---------------------------\n",
    "def get_rank(query_id, results, qrels):\n",
    "    \"\"\"\n",
    "    Returns the rank (1-indexed) of the correct document for a given query.\n",
    "    Assumes that qrels is a nested dict, e.g., {query_id: {correct_doc: 1}}.\n",
    "    If the correct document is not found in the results, returns None.\n",
    "    \"\"\"\n",
    "    # Extract the correct document ID from the nested qrels structure.\n",
    "    correct_doc = list(qrels.get(query_id, {}).keys())[0] if query_id in qrels else None\n",
    "    if correct_doc is None:\n",
    "        return None\n",
    "    \n",
    "    # Sort documents by descending score\n",
    "    sorted_docs = sorted(results.get(query_id, {}).items(), key=lambda x: x[1], reverse=True)\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        if doc_id == correct_doc:\n",
    "            return rank\n",
    "    return None\n",
    "\n",
    "# ---------------------------\n",
    "# Create a DataFrame with Top-K Analysis per Query\n",
    "# ---------------------------\n",
    "# Assumptions:\n",
    "# - baseline_results: dict mapping query_id -> {doc_id: score, ...}\n",
    "# - expl_results: dict mapping query_id -> {doc_id: score, ...}\n",
    "# - qrels: dict mapping query_id -> {correct_doc: 1}\n",
    "# - queries: dict mapping query_id -> query text\n",
    "\n",
    "data = []\n",
    "for qid in qrels:\n",
    "    baseline_rank = get_rank(qid, baseline_results, qrels)\n",
    "    expl_rank = get_rank(qid, expl_results, qrels)\n",
    "    \n",
    "    # Determine if correct doc is in top_k (if rank exists and <= top_k)\n",
    "    baseline_in_topk = baseline_rank is not None and baseline_rank <= top_k\n",
    "    expl_in_topk = expl_rank is not None and expl_rank <= top_k\n",
    "    \n",
    "    data.append({\n",
    "        \"query_id\": qid,\n",
    "        \"baseline_rank\": baseline_rank if baseline_rank is not None else float('inf'),\n",
    "        \"expl_rank\": expl_rank if expl_rank is not None else float('inf'),\n",
    "        \"baseline_in_topk\": baseline_in_topk,\n",
    "        \"expl_in_topk\": expl_in_topk\n",
    "    })\n",
    "\n",
    "df_topk = pd.DataFrame(data)\n",
    "\n",
    "# ---------------------------\n",
    "# Merge Query Text from the 'queries' Dictionary\n",
    "# ---------------------------\n",
    "# Since queries is a simple dict mapping query_id -> query text,\n",
    "# we convert it into a DataFrame.\n",
    "df_queries = pd.DataFrame(list(queries.items()), columns=[\"query_id\", \"query_text\"])\n",
    "# Merge with df_topk on 'query_id'\n",
    "df_topk = pd.merge(df_topk, df_queries, on=\"query_id\", how=\"left\")\n",
    "\n",
    "# Save the raw top-k analysis data for further inspection\n",
    "csv_path = os.path.join(subdirs[\"topk_analysis\"], f\"top{top_k}_analysis.csv\")\n",
    "df_topk.to_csv(csv_path, index=False)\n",
    "print(f\"Top-{top_k} analysis data saved to: {csv_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Analyze Group Overlap\n",
    "# ---------------------------\n",
    "# Define groups based on whether the correct doc is found in top_k:\n",
    "# \"Both\": both methods retrieve the correct doc within top_k.\n",
    "# \"Baseline Only\": only baseline does.\n",
    "# \"Colbert Only\": only explanation does.\n",
    "# \"Neither\": neither method does.\n",
    "def label_group(row):\n",
    "    if row[\"baseline_in_topk\"] and row[\"expl_in_topk\"]:\n",
    "        return \"Both\"\n",
    "    elif row[\"baseline_in_topk\"] and not row[\"expl_in_topk\"]:\n",
    "        return \"Baseline Only\"\n",
    "    elif not row[\"baseline_in_topk\"] and row[\"expl_in_topk\"]:\n",
    "        return \"Colbert Only\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "df_topk[\"group\"] = df_topk.apply(label_group, axis=1)\n",
    "group_counts = df_topk[\"group\"].value_counts()\n",
    "print(\"Query Group Counts based on Top-K presence:\")\n",
    "print(group_counts)\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization: Bar Chart for Query Group Counts\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "group_counts.plot(kind=\"bar\", color=\"skyblue\")\n",
    "plt.xlabel(\"Query Group\")\n",
    "plt.ylabel(\"Number of Queries\")\n",
    "plt.title(f\"Queries Grouped by Presence of Correct Doc in Top-{top_k}\")\n",
    "plt.grid(True, axis='y')\n",
    "bar_chart_path = os.path.join(subdirs[\"topk_analysis\"], f\"query_group_bar_top{top_k}.png\")\n",
    "plt.savefig(bar_chart_path)\n",
    "plt.close()\n",
    "print(f\"Query group bar chart saved to: {bar_chart_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Group-Level Text Analysis: Word Clouds & Top Terms\n",
    "# ---------------------------\n",
    "group_analysis_dir = os.path.join(subdirs[\"topk_analysis\"], \"group_analysis\")\n",
    "os.makedirs(group_analysis_dir, exist_ok=True)\n",
    "\n",
    "# Function to generate a word cloud for a given text\n",
    "def generate_wordcloud(text, title, save_path):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Word cloud saved to: {save_path}\")\n",
    "\n",
    "# Function to compute and return the top N words by frequency using CountVectorizer\n",
    "def top_terms(texts, top_n=10):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    sum_words = np.array(X.sum(axis=0)).flatten()\n",
    "    words_freq = [(word, sum_words[idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "    return words_freq[:top_n]\n",
    "\n",
    "group_summary = {}\n",
    "for group in df_topk[\"group\"].unique():\n",
    "    group_df = df_topk[df_topk[\"group\"] == group]\n",
    "    all_text = \" \".join(group_df[\"query_text\"].tolist())\n",
    "    \n",
    "    # Save word cloud for this group\n",
    "    wc_path = os.path.join(group_analysis_dir, f\"wordcloud_{group.replace(' ', '_')}.png\")\n",
    "    generate_wordcloud(all_text, f\"Word Cloud for Group: {group}\", wc_path)\n",
    "    \n",
    "    # Compute top terms\n",
    "    top_words = top_terms(group_df[\"query_text\"].tolist(), top_n=10)\n",
    "    group_summary[group] = top_words\n",
    "    print(f\"\\nGroup: {group}\")\n",
    "    print(\"Top words:\")\n",
    "    for word, freq in top_words:\n",
    "        print(f\"{word}: {freq}\")\n",
    "\n",
    "# Save group summary to CSV\n",
    "summary_rows = []\n",
    "for group, words in group_summary.items():\n",
    "    for word, freq in words:\n",
    "        summary_rows.append({\"group\": group, \"word\": word, \"frequency\": freq})\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv_path = os.path.join(group_analysis_dir, \"group_top_words_summary.csv\")\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "print(f\"\\nGroup top words summary saved to: {summary_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-100 analysis data saved to: plots/topk_analysis/top100_analysis.csv\n",
      "Query Group Counts based on Top-K presence:\n",
      "group\n",
      "Both             263\n",
      "Baseline Only    179\n",
      "Neither           52\n",
      "Colbert Only       6\n",
      "Name: count, dtype: int64\n",
      "Query group bar chart saved to: plots/topk_analysis/query_group_bar_top100.png\n",
      "Group top 10 keywords saved to: plots/topk_analysis/group_top10_keywords.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------\n",
    "# Hyperparameter: Top-K cutoff\n",
    "# ---------------------------\n",
    "top_k = 100  # Adjust as needed (e.g., 5, 10, 20)\n",
    "\n",
    "# ---------------------------\n",
    "# Directory Setup for Saving Plots and CSVs\n",
    "# ---------------------------\n",
    "base_dir = \"plots\"\n",
    "subdirs = {\n",
    "    \"clusters\": os.path.join(base_dir, \"clusters\"),\n",
    "    \"performance\": os.path.join(base_dir, \"performance\"),\n",
    "    \"wordcloud\": os.path.join(base_dir, \"wordcloud\"),\n",
    "    \"topk_analysis\": os.path.join(base_dir, \"topk_analysis\")\n",
    "}\n",
    "for subdir in subdirs.values():\n",
    "    os.makedirs(subdir, exist_ok=True)\n",
    "\n",
    "# ---------------------------\n",
    "# Helper Function: Get Rank of Correct Document\n",
    "# ---------------------------\n",
    "def get_rank(query_id, results, qrels):\n",
    "    \"\"\"\n",
    "    Returns the rank (1-indexed) of the correct document for a given query.\n",
    "    Assumes that qrels is a nested dict, e.g., {query_id: {correct_doc: 1}}.\n",
    "    If the correct document is not found in the results, returns None.\n",
    "    \"\"\"\n",
    "    # Extract the correct document ID from the nested qrels structure.\n",
    "    correct_doc = list(qrels.get(query_id, {}).keys())[0] if query_id in qrels else None\n",
    "    if correct_doc is None:\n",
    "        return None\n",
    "    \n",
    "    # Sort documents by descending score.\n",
    "    sorted_docs = sorted(results.get(query_id, {}).items(), key=lambda x: x[1], reverse=True)\n",
    "    for rank, (doc_id, score) in enumerate(sorted_docs, start=1):\n",
    "        if doc_id == correct_doc:\n",
    "            return rank\n",
    "    return None\n",
    "\n",
    "# ---------------------------\n",
    "# Create a DataFrame with Top-K Analysis per Query\n",
    "# ---------------------------\n",
    "# Assumptions:\n",
    "# - baseline_results: dict mapping query_id -> {doc_id: score, ...}\n",
    "# - expl_results: dict mapping query_id -> {doc_id: score, ...}\n",
    "# - qrels: dict mapping query_id -> {correct_doc: 1}\n",
    "# - queries: nested dict mapping query_id -> {'text': query text, ...}\n",
    "data = []\n",
    "for qid in qrels:\n",
    "    baseline_rank = get_rank(qid, baseline_results, qrels)\n",
    "    expl_rank = get_rank(qid, expl_results, qrels)\n",
    "    \n",
    "    # Determine if correct doc is in top_k (if rank exists and <= top_k)\n",
    "    baseline_in_topk = baseline_rank is not None and baseline_rank <= top_k\n",
    "    expl_in_topk = expl_rank is not None and expl_rank <= top_k\n",
    "    \n",
    "    data.append({\n",
    "        \"query_id\": qid,\n",
    "        \"baseline_rank\": baseline_rank if baseline_rank is not None else float('inf'),\n",
    "        \"expl_rank\": expl_rank if expl_rank is not None else float('inf'),\n",
    "        \"baseline_in_topk\": baseline_in_topk,\n",
    "        \"expl_in_topk\": expl_in_topk\n",
    "    })\n",
    "\n",
    "df_topk = pd.DataFrame(data)\n",
    "\n",
    "# ---------------------------\n",
    "# Merge Query Text from the Nested 'queries' Dictionary\n",
    "# ---------------------------\n",
    "# Convert the nested queries dict into a DataFrame.\n",
    "# We assume the nested dict maps query_id -> {'text': query text, ...}\n",
    "df_queries = pd.DataFrame.from_dict(queries, orient='index').reset_index().rename(columns={'index': 'query_id'})\n",
    "if \"text\" in df_queries.columns:\n",
    "    df_queries = df_queries.rename(columns={\"text\": \"query_text\"})\n",
    "else:\n",
    "    df_queries[\"query_text\"] = df_queries.iloc[:,1]  # Fallback if needed\n",
    "\n",
    "# Merge with df_topk on 'query_id'\n",
    "df_topk = pd.merge(df_topk, df_queries[[\"query_id\", \"query_text\"]], on=\"query_id\", how=\"left\")\n",
    "\n",
    "# Save the raw top-k analysis data for further inspection\n",
    "csv_path = os.path.join(subdirs[\"topk_analysis\"], f\"top{top_k}_analysis.csv\")\n",
    "df_topk.to_csv(csv_path, index=False)\n",
    "print(f\"Top-{top_k} analysis data saved to: {csv_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Analyze Group Overlap\n",
    "# ---------------------------\n",
    "# Define groups based on whether the correct doc is found in top_k:\n",
    "# \"Both\": both methods retrieve the correct doc within top_k.\n",
    "# \"Baseline Only\": only baseline does.\n",
    "# \"Colbert Only\": only explanation (e.g., ColBERT) does.\n",
    "# \"Neither\": neither method does.\n",
    "def label_group(row):\n",
    "    if row[\"baseline_in_topk\"] and row[\"expl_in_topk\"]:\n",
    "        return \"Both\"\n",
    "    elif row[\"baseline_in_topk\"] and not row[\"expl_in_topk\"]:\n",
    "        return \"Baseline Only\"\n",
    "    elif not row[\"baseline_in_topk\"] and row[\"expl_in_topk\"]:\n",
    "        return \"Colbert Only\"\n",
    "    else:\n",
    "        return \"Neither\"\n",
    "\n",
    "df_topk[\"group\"] = df_topk.apply(label_group, axis=1)\n",
    "group_counts = df_topk[\"group\"].value_counts()\n",
    "print(\"Query Group Counts based on Top-K presence:\")\n",
    "print(group_counts)\n",
    "\n",
    "# ---------------------------\n",
    "# Visualization: Bar Chart for Query Group Counts\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "group_counts.plot(kind=\"bar\", color=\"skyblue\")\n",
    "plt.xlabel(\"Query Group\")\n",
    "plt.ylabel(\"Number of Queries\")\n",
    "plt.title(f\"Queries Grouped by Presence of Correct Doc in Top-{top_k}\")\n",
    "plt.grid(True, axis='y')\n",
    "bar_chart_path = os.path.join(subdirs[\"topk_analysis\"], f\"query_group_bar_top{top_k}.png\")\n",
    "plt.savefig(bar_chart_path)\n",
    "plt.close()\n",
    "print(f\"Query group bar chart saved to: {bar_chart_path}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Extract Top 10 Prominent Keywords for Each Group\n",
    "# ---------------------------\n",
    "group_keywords = []\n",
    "# For each unique group, extract keywords using CountVectorizer.\n",
    "for group in df_topk[\"group\"].unique():\n",
    "    group_df = df_topk[df_topk[\"group\"] == group]\n",
    "    # Combine all query texts in the group into one string.\n",
    "    text = \" \".join(group_df[\"query_text\"].dropna().tolist())\n",
    "    # Initialize CountVectorizer with English stopwords.\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform([text])\n",
    "    # Sum frequencies of each term.\n",
    "    term_freq = np.array(X.sum(axis=0)).flatten()\n",
    "    # Retrieve term names.\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    # Create a DataFrame of terms and their frequencies.\n",
    "    freq_df = pd.DataFrame({\"term\": terms, \"frequency\": term_freq})\n",
    "    # Sort terms by frequency (descending) and select the top 10.\n",
    "    freq_df = freq_df.sort_values(\"frequency\", ascending=False).head(10)\n",
    "    freq_df[\"group\"] = group\n",
    "    group_keywords.append(freq_df)\n",
    "\n",
    "# Concatenate the results for all groups.\n",
    "group_keywords_df = pd.concat(group_keywords).reset_index(drop=True)\n",
    "keywords_csv_path = os.path.join(subdirs[\"topk_analysis\"], \"group_top10_keywords.csv\")\n",
    "group_keywords_df.to_csv(keywords_csv_path, index=False)\n",
    "print(f\"Group top 10 keywords saved to: {keywords_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
